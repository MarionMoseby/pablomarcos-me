[{"categories":null,"contents":"Ya va sonando el carrillón. En esos breves segundos, en esos instantes en los que tiemblan mis manos hambrientas de sueños, el año nuevo que se vislumbra, lejano aún en el horizonte, parece abalanzarse poco a poco sobre nosotros, como la pesada página de un libro incunable que ve por fin la luz del día.\nY así, una a una, voy engullendo las uvas, como doce musas consejeras traídas del más largo año, los doce últimos (¿o eran los primeros?) compases de un vals que se pierde en la lejanía. Como el año, empieza fácil la tarea, y jugueteo con la primera en la lengua como quien baila un tango: intenso pero relajado. Para la quinta, el baile se ha convertido en una jota, podeída mi boca por la voz del chicote, que ordena comer. Nueve, diez, once\u0026hellip; cuando llego ya a la última, parecen mis mandíbulas una clase de zumba, y mis dientes, los exhaustos alumnos de un año tedioso pero lleno de dulzor.\nY, entonces, ya está hecho. Con el estruendo de cien mil fuegos artificiales, ha pasado por fin la página, y comienza un nuevo capítulo, una temporada completa de ese libro en blanco que es el futuro. Un capítulo lleno de páginas por descubrir, de bailarinas de papel que algunos llenaremos de historias y que otros preferirán convertir en flores, en gaviotas o en barquitos con los que surcar la tempestad que les nubla. Un año, dicho sea lo evidente, lleno de esperanza, de propósitos que se olvidan porque también evolucionan, de mil luces y cien sombras que llenen de sal nuestra vida. Un año lleno de sueños, de deseos para lo propio y para lo ajeno, una nueva oportunidad de brillar con luz propia.\nY, mientras todos se emperifollan para celebrar y yo me preparo para dormir tranquilo, lo tengo claro: hoy empieza mi nueva vida, hoy no recordaré más el pasado.\n📸 : Kateryna Hliznitsova on Unsplash\n#poesia #poetry #añonuevo #newyear #poesiaenespañol\nInstancia: Compartir Deja tu comentario en ","date":"January 1, 2023","hero":"/es/posts/poesia/a%C3%B1o-nuevo/hero.jpg","permalink":"https://www.pablomarcos.me/es/posts/poesia/a%C3%B1o-nuevo/","summary":"Ya va sonando el carrillón. En esos breves segundos, en esos instantes en los que tiemblan mis manos hambrientas de sueños, el año nuevo que se vislumbra, lejano aún en el horizonte, parece abalanzarse poco a poco sobre nosotros, como la pesada página de un libro incunable que ve por fin la luz del día.\nY así, una a una, voy engullendo las uvas, como doce musas consejeras traídas del más largo año, los doce últimos (¿o eran los primeros?","tags":null,"title":"Año Nuevo"},{"categories":null,"contents":"For a brief update on my work, and to showcase the usefulness of the software I am creating, the 15th of November, 2022, I gave a presentation based on the following slides during IARC\u0026rsquo;s OncoMetaBolomics Team\u0026rsquo;s Monthly Branch Meeting.\n","date":"November 14, 2022","hero":"/es/posts/master-en-biolog%C3%ADa-computacional/tfm/cangraph-update/hero.png","permalink":"https://www.pablomarcos.me/es/posts/master-en-biolog%C3%ADa-computacional/tfm/cangraph-update/","summary":"For a brief update on my work, and to showcase the usefulness of the software I am creating, the 15th of November, 2022, I gave a presentation based on the following slides during IARC\u0026rsquo;s OncoMetaBolomics Team\u0026rsquo;s Monthly Branch Meeting.","tags":null,"title":"CanGraph Update"},{"categories":null,"contents":"On the 21st of January 2022, the Scientific Day of the Early Career and Visiting Scientists\u0026rsquo; Association of IARC was held. During this day, I delivered a 3 minute presentation showing the updates of CanGraph development, including the Poster shown below. It can be downloaded in PDF format here.\nPrevious Next \u0026nbsp; \u0026nbsp; Page: / With regards to this presentation, the judges had the following comments:\n","date":"September 21, 2022","hero":"/es/posts/master-en-biolog%C3%ADa-computacional/tfm/ecsa-day-2022/hero.png","permalink":"https://www.pablomarcos.me/es/posts/master-en-biolog%C3%ADa-computacional/tfm/ecsa-day-2022/","summary":"On the 21st of January 2022, the Scientific Day of the Early Career and Visiting Scientists\u0026rsquo; Association of IARC was held. During this day, I delivered a 3 minute presentation showing the updates of CanGraph development, including the Poster shown below. It can be downloaded in PDF format here.\nPrevious Next \u0026nbsp; \u0026nbsp; Page: / With regards to this presentation, the judges had the following comments:","tags":null,"title":"ECSA Day 2022"},{"categories":null,"contents":"As part of the development of the CanGraph package, I created a dedicated documentation site using Sphinx. It features detailed function descriptions, useful guidelines and, soon, some tutorials!\n","date":"August 27, 2022","hero":"/es/posts/master-en-biolog%C3%ADa-computacional/tfm/cangraph-documentation/hero.png","permalink":"https://www.pablomarcos.me/es/posts/master-en-biolog%C3%ADa-computacional/tfm/cangraph-documentation/","summary":"As part of the development of the CanGraph package, I created a dedicated documentation site using Sphinx. It features detailed function descriptions, useful guidelines and, soon, some tutorials!","tags":null,"title":"CanGraph Documentation"},{"categories":null,"contents":"The following presentation was used during the defense of my Master\u0026rsquo;s Thesis on July 11, 2022.\n","date":"July 11, 2022","hero":"/es/posts/master-en-biolog%C3%ADa-computacional/tfm/presentation/hero.png","permalink":"https://www.pablomarcos.me/es/posts/master-en-biolog%C3%ADa-computacional/tfm/presentation/","summary":"The following presentation was used during the defense of my Master\u0026rsquo;s Thesis on July 11, 2022.","tags":null,"title":"Master's Thesis Presentation"},{"categories":null,"contents":"Abstract Abstract Research on cancer, one of the most lethal diseases in the world today, is an expensive, complex process, usually carried out manually in laboratories. In this publication, we present CanGraph, a software solution that allows its users to annotate and interpret unknown metabolites by making use of five pre-existing databases (HMDB, SMPDB, DrugBank, ExposomeExplorer and Wikidata) and five search criteria (InChI, InChIKey, Structural Similarity, HMDB_ID, Name and ChEBI ID), resulting in an output database in GraphML format containing the associations to the different metabolic pathways, tissues and organisms to which these molecules may belong. Although it still presents problems, such as the long processing time, we hope that this program will be useful in automating the search for potential relationships between compounds and various diseases (specially cancer, as is the mission of International Agency for Research on Cancer (IARC), the Institution where this project has been carried out), with a view towards generating a web service that will make this program, and all its knowledge, available to the scientific community at large.\nKeywords: metabolomics, cancer, python, bioinformatics\nResumen La investigación sobre el cáncer, una de las enfermedades más letales del mundo en la actualidad, es un proceso caro y complejo, que suele llevarse a cabo manualmente en laboratorios. En este trabajo, presentamos CanGraph, una utilidad de software que permite a sus usuarios anotar e interpretar metabolitos desconocidos haciendo uso de cinco bases de datos preexistentes (HMDB, SMPDB, DrugBank, ExposomeExplorer y Wikidata) y cinco criterios de búsqueda (InChI, InChIKey, Similaridad Estructural, HMDB_ID, Nombre y ChEBI ID), dando como resultado una base de datos en formato GraphML que contiene las asociaciones a las diferentes rutas metabólicas, tejidos y organismos a los que estas moléculas puedan pertenecer. Aunque todavía presenta problemas, como el largo tiempo de procesamiento, esperamos que este programa sea útil para automatizar la búsqueda de posibles relaciones entre compuestos y varias enfermedades (especialmente el cáncer, como figura en la misión de la Agencia Internacional de Investigación sobre el Cáncer (IARC), institución donde se ha llevado a cabo este proyecto), con vistas a generar un servicio web que ponga este programa, y todo su conocimiento, a disposición de la comunidad científica en su conjunto.\nPalabras clave: metabolómica, cancer, python, bioinformática\nIntroduction Currently, cancer is one of the most devastating diseases in existence. Worldwide, more than 19 million diagnoses and almost 10 million deaths occur each year from this family of diseases,1 accounting for 20% of deaths (and therefore being the most prevalent cause of it) in developed countries, and 13% of deaths in the developing world.2 Because of this, billions of euros are spent annually on cancer research,3 trying to find potential associations between cancer-causing compounds and drugs that may be able to treat said diseases.\nA modest part of this work is carried out at the International Agency for Research on Cancer, the World Health Organisation\u0026rsquo;s cancer research agency, which regularly publishes Monographs on some substances and their whether they might be classified or not as carcinogenic. As part of this work, and as in the rest of the industry, one of the activities that consumes most resources and effort is precisely the identification of new metabolites as potentially carcinogenic and their annotation and interpretation through the different pathways, tissues and organisms to which these substances may belong. This work is mainly done by hand, employing enormous amounts of human and personal resources which, if automated, could be freed up, allowing us to further expand the Agency\u0026rsquo;s research efforts.\nTo solve this problem, metabolomics is being increasingly employed in cancer research.4 This discipline, consisting on the global analysis of small molecule metabolites, can provide critical information about the cancer state that are otherwise not apparent, and makes automatizing the discovery of this new information easier and simpler.\nObjectives Given the need to automate, as far as possible, the search for new, potentially cancer-causing substances, and suitable drugs to treat cancer, and given the ease with which machines can find associations based on complex patterns (such as structural similarity) that can be difficult for humans to discover, the IARC Metabolism and Nutrition team has decided to create the CanGraph project. This consists of a series of software utilities that use metabolomics data to automatically annotate and interpret metabolites found in cancer research, discovering potential associations with cancer and associating them with potential membership in known metabolomic pathways in humans. The objectives of this project are several:\nTo begin with, we want to create a Python program for internal use in the Agency, which allows the scientists who collaborate with it to obtain a series of Knowledge Graphs (that is, some graph-structured data models) that present a clear visualization of the function of the subject metabolite inside the human metabolome as a whole. Then, we would like to automate the analysis of these resulting networks. For instance, we would like to develop a way to find associations with cancer or other diseases, potential membership to known regulatory mechanisms and pathways on the metabolome, or interactions with other metabolites. Finally, we would like to, eventually, offer the program as Software as a Service (SaaS), inside a web utility that allows interested researchers to make use of our work. Matherials and Methods As explained, the intention of the solution herein described is to annotate and interpret a series of metabolites recently discovered inside a laboratory, trying to associate them to those tissues, metabolic pathways and/or organisms to which these metabolites might belong. For this, we have used a series of pre-existing resources, which we have fined-tuned to better suit our needs:\nThe Databases In order to automate the annotation and interpretation of metabolites, the first thing we will need is a list of pre-existing, high-quality databases in which to search for compounds similar to those that may be identified in a laboratory. Thus, we have chosen the following five databases, which we hope will provide a comprehensive overview of the human metabolome and the interactions that the molecules in it have with various types of cancer:\nThe Human Metabolome Database (HMDB) is an open-access database containing detailed information on small molecule metabolites found in the human body, intended for applications in metabolomics, clinical chemistry, biomarker discovery and general education.5 The database includes chemical as well as molecular and biochemical data, including over 41,000 metabolite entries and approximately 7200 protein and DNA sequences, and is provided by the Wishart Research Group, a laboratory led by Dr. David Wishart in the Departments of Biological Sciences and Computer Science at the University of Alberta, in Edmonton, Canada.\nDrugBank is a bioinformatics and chemoinformatics resource that combines detailed drug data with comprehensive drug target information, containing over 7,800 drug entries and nearly 2,200 FDA-approved small molecule drugs, 340 FDA-approved biotech drugs, 93 nutraceuticals and \u0026gt;5,000 experimental drugs, which can be linked to protein sequences (drug targets) and product data.6 The database is provided by OMx Personal Health Analytics Inc, a spin-off of the Wishart Research Group founded at the University of Alberta.\nThe Small Molecule Pathway Database (SMPDB) is an interactive database containing over 618 small molecule pathways found in humans, over 70% of which are not found in any other pathway database.7 It is designed to support pathway elucidation and discovery in metabolomics, transcriptomics, proteomics and systems biology by providing detailed representations of the pathways, metabolites and proteins it contains. It has also been developed by the Wishart Research Group.\nExposome Explorer is the first database dedicated to biomarkers of exposure to environmental risk factors for disease, with a particular focus on cancers. It aims to provide comprehensive data on all known biomarkers of exposure to dietary factors, pollutants and pollution measured in population studies by collecting information on more than 800 peer-reviewed publications containing more than 10,000 measurements of different metabolites to test whether they can be used as biomarkers for a given disease.8 This database is an internal development of IARC (where this work is carried out), but is publicly accessible on the Internet.\nWikiData is a free and open source knowledge base that can be read and edited by both humans and machines.9 As a central repository for the structured data of the Wikimedia projects (including Wikipedia, Wikivoyage, Wiktionary and Wikisource) it is one of the world\u0026rsquo;s largest collaboratively generated Open Data collections, which, although probably of lower quality due to being freely editable and not produced solely by experts, hopefully will include a large amount of generalist data, such as relationships between metabolites and diseases or a basic ontology of the different types of cancer.\nEach of these databases has its pros and cons, which, as may be visualised in Table 1, and complement each other: for example, the SMPDB contains high quality data for pathways, but less information for metabolites than, for example, the Human Metabolome Database, which in turn is complemented by DrugBank, which has more information on drugs than on metabolites. On the other hand, Exposome Explorer allows us to find associations between all these metabolites, dietary intakes (the exposome, a novel concept defined by the CDC as \u0026ldquo;the measure of all exposures of an individual over a lifetime and their relationship to health\u0026rdquo;10) and various diseases. Finally, WikiData allows us to add both general information (relationship of certain drugs and/or metabolites with diseases) and detailed information (for example, identifiers in external databases such as UniProt or Pubmed IDs for references, which are usually better detailed in this database than in, for instance, SMPDB, which however provides lots of information that is only present there) in a massive way, although with the drawback of being probably less accurate than other databases.\nHMDB SMPDB DrugBank Exposome Explorer WikiData Data Quality High Huge High High Low Data Quantity High Medium High Medium Huge External IDs Medium Low Medium High High Info on Metabolites Huge Low Medium High Medium Info on Associated Nodes Medium Low Medium High High \\centerline{Table 1: Pros and Cons of each of the five databases being used.}\nThe Database Management System (DBMS) In order to define, create, maintain and control access to these databases, a Database Management System that can work with various types of input is needed. For this, we have chosen Neo4J, one of the most widely used DBMS in the world of computing, and which, unlike other more common ones such as MySQL or MariaDB, works in a \u0026ldquo;non-relational\u0026rdquo; way, i.e. the data is not structured as a series of interconnected tables with primary and secondary keys and a list of related values, but is presented, in this case, in a graph format.\nIn mathematics and computer science, a graph is a structure consisting of a series of objects (called \u0026ldquo;vertices\u0026rdquo; or \u0026ldquo;nodes\u0026rdquo;) that may or may not be related by a series of \u0026ldquo;edges\u0026rdquo; or \u0026ldquo;arcs\u0026rdquo;, which allow binary relationships between the elements of the set to be represented.11 The advantage of this type of databases, which have re-emerged since the 1960s, is that they allow for more efficient processing on those areas of knowledge that can be represented as \u0026ldquo;networks\u0026rdquo; (e.g. a person\u0026rsquo;s list of friends on Facebook, or, in the case in point, the human metabolism, which is made up of a series of metabolites (e.g. a the nodes), that are interrelated by enzymes and other non-enzymatic reactions (the edges)), as well as its greater efficiency in finding elements (because, instead of needing to find the index of each element in each table, the element itself directly points to related nodes in the database).11 Because of these advantages, the Neo4J data model is simpler and more expressive than other RDMS such as MySQL, allowing us to query the graph almost in natural language.\nThe Common Schema In order to obtain a coherent, useful and reliable result, we have designed the software in question so that it is capable of presenting its results according to a schema common to the 5 databases, which has been designed to simplify the result as much as possible while minimising the loss of information. This schema, which can be consulted at large in Annex I, has been designed taking into account the particularities of each of the 5 databases, their strengths and weaknesses, and unifying the types of nodes created and their properties in order to maximise the value of each data field. In Figure 1 you can see a preview of it, as well as its differences and similarities with the original schema.\nAs can be seen, this scheme has been acquired by progressively merging different types of nodes; for example, all nodes dealing with diseases, whether cancers or not, have been renamed Disease. The MicrobialMetabolite and Component nodes, which come from Exposome Explorer, have been merged with the rest of Metabolites, eliminating a number of unhelpful properties present in the original database (those ending in * _count * ). For the drugs present in DrugBank, the patent holders have not been added, but only the Company that is responsible for the manufacturing and packaging of the drugs, in order to filter them if necessary; prices have also been excluded, since we did not consider them relevant. There are also nodes that have been designed from the beginning to have common keys: for example, the ExternalEquivalents, which appeared in WikiData, have been uniformly designed to represent exact equivalents of a given node.\n{ height=55%}\nIn general, a thorough analysis of the different fields present in the five databases has been carried out, considering those equivalent to merge them and those different to keep them separate. For example, while the Kegg_Pathway_ID and Kegg_Component_ID can be merged into a single KEGG_ID (as they are prefixed and values cannot collide), the FooDB_IDs necessarily need a separate field for the FooDB_Compound_ID and the FooDB_Food_ID.\nThe Software This software has been designed as a Python12 script originally tested on a computer running Python 3.8.10 inside KDE Neon 5.25, a GNU/Linux distribution based on Ubuntu 20.04 LTS and using Linux Kernel version 5.13.0-51; although, in general it should be compatible with all systems capable of running Python. It works by processing the five databases mentioned above to produce, for each metabolite for which information is requested, a file in GraphML format (an XML-based file format for graphs)13 containing all the nodes associated with it. This requires the user to provide a CSV with a number of optional metabolite identifiers. Although more detailed instructions can be found in the README of the project itself on Git,14 the program supports the following data as input identifiers:\nInChI: The International Chemical Identifier, an identifier designed by IUPAC and NIST to provide a standard, readable way of encoding molecular information and to facilitate searching for information in databases and on the web.15 This identifier can be generated autonomously for any small metabolite using openly licensed software, and its known structure and formula. It is highly recommended that this is calculated and provided to the program (in case of hitherto unknown metabolites, this might be the only identifier availaible on the list)\nInChIKey: This is a hashed version of the InChI Identifier, i.e., a mathematically condensed version of it shaped as a 27-character string that is simpler, and thus easier to use, than the original.16 Providing it is opcional.\nIdentifier: The identifier in the Human Metabolome Database, if available.\nName: A commonly accepted name for the metabolite. It may be more imprecise that the other identifiers and even lead to false positives, so it is recommended that you enter it only if it is standardised according to IUPAC nomenclature.17\nChEBI: The ChEBI database identifier, a resource for small chemical entities of biological interest.18 It is also optional.\nOther identifiers, such as SMILES or MonoisotopicMass, were considered as potential for inclusion, but were discarded, the former because it provides less information than InChI, which can unerringly (99.5% accuracy)15 identify a metabolite, and the latter because it could lead to many false positives (as we theorise that there will be many metabolites with a similar mass).\nOnce these identifiers are received in the appropriate format, the program tries to find exact matches for Name, Identifier, InChI, InChIKey or ChEBI; and, if it does not find them, it searches for metabolites with a structural similarity of at least 95% with the original metabolite about which information is sought. To calculate this structural similarity, we have used rdkit, a Python module focused on chemoinformatics and open-source machine learning that allows us to calculate the structure of a molecule from its InChI (if it is present among the parameters presented to the program).\nOnce the relevant structure has been calculated, we obtain the MACCS fingerprint of both the \u0026ldquo;Query\u0026rdquo; molecule and all the \u0026ldquo;Subject\u0026rdquo; molecules present in our five databases using rdkit. These fingerprints, whose acronym stands for \u0026ldquo;Molecular ACCess System\u0026rdquo;, are 166-bit 2D structure fingerprints that are commonly used to measure molecular similarity. Since each bit is either on or off, MACCS keys can represent more than 9.3x10^49^ different molecules,19 which should give us enough confidence that they correctly represent the molecules we are working with. Next, for each pair of query and subject molecules, we calculate their Sørensen-Dice similarity index; if the match is greater than 95%, we import both the subject and the query into our knowledge graph. This Sørensen-Dice similarity index is a measure of similarity between two vectors widely used in computer science and machine learning, and takes values from 0 to 1 where 1 is most similar and 0 is least similar.20 It was developed and published independently by Thorvald Sørensen and Lee Raymond Dice, who published it in 1948 and 1945, respectively.\nThe original metabolite will be imported and marked as OriginalMetabolite, and all those related to it will be marked by the relationship -[r:ORIGINALLY_IDENTIFIED_AS]-\u0026gt;, with the properties of the relationship explaining the basis for this identification.\nSample Workflow A potential workflow, as depicted in Figure 2, would be as follows: a researcher, in their laboratory, finds a new metabolite in the course of their research. Intrigued about it, they calculate its InChI, and provide it to our program, along with any other identifiers they may have collected from it. After letting the program run, the researcher will obtain a GraphML file, which they can open in Neo4J or any other data management program (e.g. CytoScape) allowing them to investigate the associations between the provided metabolites and those that have been identified as the same or with a high degree of similarity. This exploration is currently left to the researcher, and can be done manually; however, due to the large number of nodes present in the exports, we are considering developing a complementary program for this.\n{ width=90%}\nUnderstanding the Output To understand and illustrate the kind of results that can be obtained with this program, as well as to demonstrate its correct functioning, we have performed a test run with a series of metabolites discovered by the IARC Nutrition and Metabolism team, to which the author of this paper belongs. Also, to better understand the output of the program, we have created a series of graphs that count the number of times data is retrieved from each individual database, as well as the reasons for choosing to import a particular field (as defined in this same section).\nResults Sample Outputs For the time being, the solution we have given to the problem explained above (the need to annotate and interpret newly discovered metabolites) takes the form of a Minimal Viable Product Software that does not yet enjoy all the functionalities we would like for it to have. However, from a list of metabolites provided by the IARC Nutrition and Metabolism Team, we have managed to generate a series of Knowledge Graphs, demonstrating the viability of the project and its usefulness. As can be seen in Figure 3, these graphs maintain the scheme designed in Figure 1 and explained in more detail in Annex I; we can see that, for a given OriginalMetabolite, almost 200,000 related nodes and 250,000 relationships appear, allowing us to navigate the graph and understand the different information sources.\n{ width=85%}\nThis provides a great variety of information, such as the publications in which it has been found (Figure 4a), the pathways it can take part in, (Figure 4b), the diseases it is related with (Figure 4c), and, in the case of proteins, their Genomic and Proteic sequences (Figure 4d).\n{ width=90%}\nProvenance of the Data In Figure 5a, we can see the provenance of the data present in CanGraph\u0026rsquo;s outputs, categorised according to the method that has been used to consider these data as \u0026ldquo;valid for import\u0026rdquo; from one of our five databases. This has been calculated by noting each time the program considered a record present in a database as a suitable match for import into the result, writing down both the reason why it was considered valid and the database from which it was taken.\nAll five input identifiers have been invoked a similar amount of times, which indicates that, most probably, they complement each other, so that, when one of them is missing, it is still possible to obtain information, as the others could make up for its absence. In this way, a researcher using our program would not have to indicate all the 5 proposed identifiers, but, by indicating just one of them (InChI) or even more if they are availaible, it is likely that she would find relevant matches in our 5 databases.\nOn the other hand, with respect to the distribution of the data, the \u0026ldquo;structural similarity\u0026rdquo; criterion clearly stands out for its breadth. This makes sense, as \u0026ldquo;structural similarity\u0026rdquo; is, next to \u0026ldquo;name\u0026rdquo;, the least strict field we have defined; that is, for a given metabolite, there is only one InChI, one ChEBI, one HMDB_ID, and, if well defined, one name; however, a single metabolite can be similar to many others, which explains the disproportionate impact of this feature on the selection of fields for import. Moreover, this is quite positive, as it implies that, for unknown metabolites (for which all other data sources are less likely to be present) there is a higher probability that we will find one with structural similarity in our database. Also worth mentioning is the ChEBI field, which has a much smaller mean and width than the other fields. This is because, in the file provided by the IARC Nutrition and Metabolism Team that was used to test the software, the ChEBI IDs were annotated in a non-standard way (prefixed), and so they could not find matches in all databases. This is a problem that should be tackled in new versions of the software, but it also demonstrates its versatility: despite input problems, the program managed to generate Knowledge Graphs successfully.\n{ width=110%}\nFinally, Figure 5b shows the origin of the data present in the outputs we have generated, this time categorised according to the database from which they have been extracted. Unlike in the previous figure, here there are two databases that stand out when it comes to providing information: WikiData and the HMDB. This makes sense: on the one hand, as we have explained, WikiData is the more general database, and the scripts we have designed to extract information from it are triggered whenever there are matches by name or other common identifiers such as UniProt ID, CAS Number and others, which are likely to be present. Regarding the HMDB, its disproportionate impact on providing information is also logical, since, in the input provided, almost all metabolites came with a non-null \u0026ldquo;Identifier\u0026rdquo; value (which, as explained, is the ID in the HMDB). This can also be seen in Figure 4a, where the average usage for the \u0026ldquo;HMDB_ID\u0026rdquo; criterion is also the highest (although just slightly) when importing data.\nPotential Improvements One of the main problems with CanGraph is its long running time: for the list of 25 metabolites on which the program was tested, the estimated running time is about 160 hours. While some of this slowness may be due to rdkit, which takes a long time to generate the MACCS fingerprints and the Sørensen-Dice index on which some of the database search criteria are based, most of it is due to the poor transaction times offered by the Neo4J Python driver, which is generally inefficient, and the Neo4J database itself, which is rather unstable, at least on GNU/Linux. Perhaps these problems can be solved by using the IARC High Performance Computing system, which has supercomputers for which available resources or driver instability may be less of a problem. Another problem that has required addressing is the instability of WikiData\u0026rsquo;s SPARQL point (from which we get the information for this database): as this cannot be available 24 hours a day, we have had to design the code to be able to retry those requests that have not been able to pass through on the fist attempt.\nAnother major detail that remains to be worked out is the post-processing of the generated networks. At present, this work is left to the researchers using the software, although there is the possibility at IARC of generating a program that allows this to be done in a comprehensive and automated way, simplifying the process even further.\nConclusions As set out on the beggining, and as part of the CanGraph project, we have been able to design a software capable of acting as a \u0026ldquo;search engine\u0026rdquo; in existing databases, achieving our goal of annotating and interpreting hitherto unknown metabolites, as well as their potential relationships with certain diseases, in a simple, fast and convenient way. Although there is still work to be done, such as generating a script to automate the post-processing of the generated networks, or fixing minor details in the integration of certain search criteria, this paper has definitively proven the usefulness and success of our project, which we hope will help IARC to continue its indispensable work against cancer and for the benefit of humanity.\nIn addition, the complex and time-consuming process of manually optimising the five schemas of five very different databases has allowed us to greatly improve our knowledge of the different identifiers in existing databases on the internet, enabling us to generate a schema that is both reliable for the future and adaptable to new databases that we could add to this \u0026ldquo;search engine\u0026rdquo; that we are creating.\nLooking into the future, we would also like to provide this program as Software as a Service (SaaS) in the cloud, available at least to all IARC scientists and potentially to external researchers who find the service useful. Using pre-existing software such as cytoscape.js, it might be possible to present the resulting Knowledge Graphs in an attractive, simple and informative way to potential users. We would also like to try running the service on the IARC High Performance Computing system, which could potentially help reduce the two major roadblocks encountered to date: the instability of the DataBase Management System and the slow processing times.\nAnnex I - Schema Definition From any software, a standardised, unified and reliable output is expected, so that it can be worked with in a simple and comfortable way. For this reason, and as explained in the rest of the paper, we have considered it necessary to define a common schema for the search results offered by this program, so that, when we extract information from each of the five different databases, we obtain a result with the minimum number of duplications and complexities possible, but which maintains as much of the information and usefulness present in each data field as possible.\nThe Relationships There are 35 different kind of nodes, which can be seen with their associated nodes here:\n{width=90% margin=auto}\nThe Nodes There are 29 different kind of nodes, each with their own properties. These are:\n{width=120%} {width=120%}\nFor the sake of simplicity, it has been decided against presenting here a detailed description of each feature: for example, some features are Boolean, some are Integer, and some are text strings; however, such a detailed analysis seemed too much for this article.\nAdditional Comments Please note that this schema can be interactively consulted in Neo4J by importing the corresponding GraphML file from the Git Repository. The command to use, after presenting new-schema.graphml in Neo4J\u0026rsquo;s import path (availaible in your instance\u0026rsquo;s config) is:\nCALL apoc.import.graphml(\u0026quot;new-schema.graphml\u0026quot;, {useTypes:true, storeNodeIds:false, readLabels:True})\nThis file can also be explored using other GraphML-compatible software, such as cytoscape.\nAnnex II - License Information This document, and the accompanying images, are availaible under the CC By SA 4.0 License. You are free to adapt and reuse the text as you like under the terms of the license, as long as you give appropriate credit and release any modifications made under the same license.\nThis PDF was generated using pandoc:\npandoc --pdf-engine=xelatex --highlight-style tango --biblio Bibliography.bib --toc \u0026#34;TFM Body.md\u0026#34; -o \u0026#34;TFM Body.pdf\u0026#34; The software presented on this paper is availaible under a MIT License, and can be accessed in IARC\u0026rsquo;S OMB Repository\nThe databases used in this paper are availaible under a series of different Licenses:\nHMDB and SMPDB are availaible under an undisclosed, non-commercial license. DrugBank is availaible under a CC-By-NC 4.0 International License as long as you ask them for the data. Exposome Explorer is an internal development at IARC, and its full database is not availaible for download; a reduced version can be consulted here. WikiData is released to the public as CC-0 - Public Domain. References Sung H, Ferlay J, Siegel RL, Laversanne M, Soerjomataram I, Jemal A, et al. Global cancer statistics 2020: GLOBOCAN estimates of incidence and mortality worldwide for 36 cancers in 185 countries. CA: A Cancer Journal for Clinicians [Internet] 2021 [cited 2022 Jun 21];71(3):209–49. Available from: https://onlinelibrary.wiley.com/doi/abs/10.3322/caac.21660\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nBoffetta P, Parkin DM. Cancer in developing countries. CA: A Cancer Journal for Clinicians [Internet] 1994 [cited 2022 Jun 24];44(2):81–90. Available from: https://onlinelibrary.wiley.com/doi/abs/10.3322/canjclin.44.2.81\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nEckhouse S, Lewison G, Sullivan R. Trends in the global funding and activity of cancer research. Mol Oncol [Internet] 2008 [cited 2022 Jun 21];2(1):20–32. Available from: https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5527789/\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nSchmidt DR, Patel R, Kirsch DG, Lewis CA, Vander Heiden MG, Locasale JW. Metabolomics in cancer research and emerging applications in clinical oncology. CA: A Cancer Journal for Clinicians [Internet] 2021 [cited 2022 Jun 24];71(4):333–58. Available from: https://onlinelibrary.wiley.com/doi/abs/10.3322/caac.21670\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nWishart DS, Guo A, Oler E, Wang F, Anjum A, Peters H, et al. HMDB 5.0: The human metabolome database for 2022. Nucleic Acids Research [Internet] 2022 [cited 2022 Jun 23];50(D1):D622–31. Available from: https://doi.org/10.1093/nar/gkab1062\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nWishart DS, Feunang YD, Guo AC, Lo EJ, Marcu A, Grant JR, et al. DrugBank 5.0: A major update to the DrugBank database for 2018. Nucleic Acids Research [Internet] 2018 [cited 2022 Jun 23];46(D1):D1074–82. Available from: https://doi.org/10.1093/nar/gkx1037\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nJewison T, Su Y, Disfany FM, Liang Y, Knox C, Maciejewski A, et al. SMPDB 2.0: Big improvements to the small molecule pathway database. Nucleic Acids Res 2014;42(Database issue):D478–484.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nNeveu V, Nicolas G, Salek RM, Wishart DS, Scalbert A. Exposome-explorer 2.0: An update incorporating candidate dietary biomarkers and dietary associations with cancer risk. Nucleic Acids Research [Internet] 2020 [cited 2022 Jun 23];48(D1):D908–12. Available from: https://doi.org/10.1093/nar/gkz1009\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nWikidata: A free and open knowledge base [Internet]. [cited 2022 Jun 23];Available from: https://www.wikidata.org/wiki/Wikidata:Main_Page\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nExposome and exposomics NIOSH CDC [Internet]. 2022 [cited 2022 Jun 21];Available from: https://www.cdc.gov/niosh/topics/exposome/default.html\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nRodriguez MA, Neubauer P. The graph traversal pattern [Internet]. 2010 [cited 2022 Jun 21];Available from: http://arxiv.org/abs/1004.1001\u0026#160;\u0026#x21a9;\u0026#xfe0e;\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nPython reference manual. Python.org [Internet]. [cited 2022 Jun 23];Available from: https: //www.python.org/doc/\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nGraphML specification [Internet]. [cited 2022 Jun 23];Available from: http://graphml.graphdrawing.org/specification.html\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nGithub Repository: https://github.com/OMB-IARC/CanGraph\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nHeller SR, McNaught A, Pletnev I, Stein S, Tchekhovskoi D. InChI, the IUPAC internationalchemical identifier. Journal of Cheminformatics [Internet] 2015 [cited 2022 Jun 24];7(1):23. Available from: https://doi.org/10.1186/s13321-015-0068-415.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nKarol PJ. The InChI code. J Chem Educ [Internet] 2018 [cited 2022 Jun 24];95(6):911–2. Available from: https://doi.org/10.1021/acs.jchemed.8b00090\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nIUPAC Nomenclature Guide: https://iupac.org/what-we-do/nomenclature/\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nHastings J, Owen G, Dekker A, Ennis M, Kale N, Muthukrishnan V, et al. ChEBI in 2016: Improved services and an expanding collection of metabolites. Nucleic Acids Res [Internet] 2016 [cited 2022 Jun 24];44(D1):D1214–9. Available from: https://europepmc.org/articles/PMC4702775\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nKuwahara H, Gao X. Analysis of the effects of related fingerprints on molecular similarity using an eigenvalue entropy approach. Journal of Cheminformatics [Internet] 2021 [cited 2022 Jun 24];13(1):27. Available from: https://doi.org/10.1186/s13321-021-00506-2\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nDice LR. Measures of the amount of ecologic association between species. Ecology [Internet] 1945 [cited 2022 Jun 24];26(3):297–302. Available from: https://onlinelibrary.wiley.com/doi/abs/10.2307/1932409\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","date":"June 24, 2022","hero":"/es/posts/master-en-biolog%C3%ADa-computacional/tfm/thesis/hero.png","permalink":"https://www.pablomarcos.me/es/posts/master-en-biolog%C3%ADa-computacional/tfm/thesis/","summary":"Abstract Abstract Research on cancer, one of the most lethal diseases in the world today, is an expensive, complex process, usually carried out manually in laboratories. In this publication, we present CanGraph, a software solution that allows its users to annotate and interpret unknown metabolites by making use of five pre-existing databases (HMDB, SMPDB, DrugBank, ExposomeExplorer and Wikidata) and five search criteria (InChI, InChIKey, Structural Similarity, HMDB_ID, Name and ChEBI ID), resulting in an output database in GraphML format containing the associations to the different metabolic pathways, tissues and organisms to which these molecules may belong.","tags":null,"title":"CanGraph"},{"categories":null,"contents":" An English version of this document can be found below Hola! Soy Pablo, ex-alumno del Máster en Biología Computacional de la UPM durante el Curso 2021-2022. Durante el Máster, una de las cosas que más me han agobiado ha sido el ver que tareas que, en un principio, parecían de enorme complejidad, en realidad resultaban sencillas una vez tenías alguna guía que seguir, alguna página web encontrada en internet que te ayudase un poco a orientarte y a saber qué, exactamente, qué quieren los profesores de mí. Así, con la idea de ayudar a futuros estudiantes de este Máster, he creado esta web, que recopila todos los ejercicios que realicé para las optativas que he elegido de una manera (espero) elegante, bonita y fácil de seguir.\nObviamente, la intención no es que la gente \u0026ldquo;copie\u0026rdquo; los resultados de mis ejercicios, sino que estos sirvan de inspiración, para que la gente no pase por el camino de lágrimas de ansiedad, desazón y agobio por el que ha tenido que pasar toda mi promoción. Todos los materiales aquí son por completo de mi creación (o de sus coautores, en el caso donde se indique) y están disponibles bajo la Licencia Creative Commons Attribution-ShareAlike 4.0 International, para que sean modificados y adaptados como creáis necesario, siempre que se mantenga una cita al autor y se libere la obra derivada bajo la misma licencia. De esta manera, si crees que mis trabajos se pueden mejorar, o si quieres hacer cualquier apunte, no dudes en utilizar los botones de \u0026ldquo;Mejorar esta Página\u0026rdquo; que encontrarás en la esquina inferior derecha de cada Post. También puedes abrir una Issue directamente en el Repositorio Git desde el que se genera esta página.. Si mi trabajo montando este sitio te ha parecido útil, puedes realizar una donación en mi Ko-Fi, que, aunque no me va a permitir ganar dinero pese a las horas y horas de trabajo dedicadas a generar esta página, al menos me motivará para seguir manteniéndola y me permitirá costear el dominio y el hosting, que, en total, sale a unos 20 euros al año.\nSi algún profesor ( vamos julito! ) considera que su copyright está siendo violando, tiene quejas con esta página, o, simplemente, quiere hablar conmigo, puede enviarme un email a profesores-masterbc@loreak.org; en general, mi posición es que las obras presentadas en este sitio web representan creaciones completamente mías, y que, por tanto, yo soy el único dueño detrás de la Propiedad Intelectual de las mismas, aún cuando estén basadas (muy ligeramente) en los enunciados definidos por los diferentes profesores; al fin y al cabo, no se puede patentar una idea\nPor último, adjunto una tabla con mis calificaciones para cada una de las tareas disponibles en la web, que espero pueda servir a aquellos que piensen hacer uso del material disponible en la misma para valorar si este es fiable o no:\nBig Data Engineering # Título Autor Nota Comentarios 1 Walmart Stock Exercise Pablo Marcos 7 He usado Pandas en lugar de SPARK, que es el objetivo de la asignatura jeje 2 Predicting Crew Members Pablo Marcos 10 - 3 Predicting Churn Risk Pablo Marcos 10 - 4 Hack Data Pablo Marcos 10 - 5 Predicting Dog Food Spoiling Pablo Marcos 10 - 6 Pneumonia identification from X-Ray Imagery Yaiza Arnáiz \u0026amp; Pablo Marcos 9 - * Extra Point: Auto Insurance Pablo Marcos 5 - Computational Tools in Evolutionary Biology # Título Autor Nota Comentarios 1 Exercise 1 (Parte Alejandro Couce) Pablo Marcos 8.5 Los gráficos lineales en amarillo deberían llevar escala logaritmica Bottleneck * Population_Size si se relaciona con tfix 2 Excercise 2 (Parte Israel Pagán) Pablo Marcos 8.5 El árbol filogenético no cuadra, la R2 es muy mala.\nSe puede hacer con: iqtree -s Rabies.fas -m MFP Biología programable No desglosó las notas, sólo me puso un 9.5 de nota global, sin comentarios adicionales\nStatistical Data Analysis and Visualization # Título Autor Nota 1 Notas de Clase Pablo Marcos 10 2 Análisis de 2 Figuras Yaiza Arnáiz, Lexane Louis, Alexandre Vergnaud \u0026amp; Pablo Marcos 10 3 Exploring a DB Yaiza Arnáiz, Lexane Louis, Alexandre Vergnaud \u0026amp; Pablo Marcos 10 4 PCA, FA \u0026amp; CA Yaiza Arnáiz, Lexane Louis, Alexandre Vergnaud \u0026amp;Pablo Marcos 8 Genomics Assisted Breeding Kind reminder: Please, DO NOT PICK THIS SUBJECT # Título Autor Nota Comentarios 1 Teoría \u0026amp; Heredabilidad Pablo Marcos 23/60 Me tira beef de que no atiendo en clase (lolz) La fórmula usada para calcular la R2 es incorrecto (no explica) 2 Breeding Values \u0026amp; Introduction to R Pablo Marcos 43/60 - 3 En Construcción 4 En Construcción 5 En Construcción 6 En Construcción 7 En Construcción Genomics Data Analysis and Visualization # Título Autor Nota Comentarios 1 Trabajo Final (Parte Jaime) Pablo Marcos 4.28/4.4 - 1 Trabajo Final (Parte Joaquin) Pablo Marcos 2.76/3.5 - 2 Single Cell - 1 Pablo Marcos 8 No se enfoca bien el análisis en la complementación de los FTs Se proporcionan más muestras de las necesarias Texto muy pequeño en los gráficos En los 1.2, 1.3 y 1.4 no se analizan las muestras problema 2 Single Cell - 2 Pablo Marcos 7 Texto muy pequeño en los gráficos En la 2.5 falta el scorede los biomarcadores de Celulas Madre en la PCA completa indicando si E30 o S18 son buenos biomarcadores 2 Single Cell - 3 Pablo Marcos 6.8 La 3.1 y 3.2 no argumentan el algoritmo usado para clustering Knowledge Representation # Título Autor Nota Comentarios 1 Practical Assignment Yaiza Arnáiz \u0026amp; Pablo Marcos 9.8 Todo el mundo sacó 10, no sé qué hicimos mal También es vd que hicimos una rayada en vez de lo de leer papers Lead Discovery Please, PICK THIS SUBJECT: La profesora es un amor # Título Autor Nota Comentarios 1 LBVS Pablo Marcos 3/3 Todo perfe 2 PBVS Pablo Marcos 2.5/3 Se me olvidó validar el farmacóforo 3 Paper Final Pablo Marcos 3.6/4 Validé el farmacóforo, pero olvidé escribirlo :/ Bioinformatics Programming Challenges # Título Autor Nota Comentarios 1 Creating Objects Pablo Marcos 20/20 Great work! 2 Intensive integration using Web APIs Pablo Marcos 16/20 El iterador no es correcto Las redes no están consolidadas Quiere una rayada tipo esto (💃 al autor) 3 GFF feature files and visualization Pablo Marcos 15/20 Finds all + features on the + strand and all - on the - strand but the not-overlaping features are missing The GFF files should not have duplicates, but, since they are marked as coming from different transcripts, its ok 4 Searching for Orthologues Pablo Marcos 20/20 Spectacular! Thank you! 5 SPARQL Queries Pablo Marcos 20/20 El profesor finge que no nos hemos copiado todos Modelización y simulación de biosistemas Mi asignatura favorita del Máster, aunque de las más duras. La recomiendo.\n# Título Autor Nota Comentarios 1 Lesson 1 Pablo Marcos 8.5 Buen trabajo! 2 Lesson 2 Pablo Marcos 9.5 Falta explicar efecto de beta y mu en las dinámicas temporales 3 Lesson 3 Pablo Marcos 9.5 Se debe estudiar la transición como fenómeno absoluto, no temporal 4 Lesson 4 Pablo Marcos 8.5 El GoL debería habérmelo inventado yo (imposible imho) * Optional Essay Pablo Marcos 1/1 Buena fumada bro 5 Lesson 6 Pablo Marcos 8 Should use larger network Debería haber simulado muuuchas networks diferentes Muy raro que los 3 gráficos del final salen iguales * Lesson 5 Pablo Marcos 1/1 Optional Exercise La imagen del header está disponible bajo la Unsplash License en el Unsplash de Joel Vodell\nEnglish Version Hi, I\u0026rsquo;m Pablo, a former student of the Master in Computational Biology at the UPM during the academic year 2021-2022. During the Master\u0026rsquo;s course, one of the things that has overwhelmed me the most has been to see that tasks that, at first, seemed extremely complex, were actually easy once you had a guide to follow, a web page found on the internet that helped you a little to orient yourself and to know what, exactly, the professors want from you. So, with the idea of helping future students of this Master, I have created this website, which compiles all the exercises I did for the electives I have chosen in a (hopefully) elegant, beautiful and easy-to-follow way.\nObviously, the intention is not for people to \u0026ldquo;copy\u0026rdquo; the results of my exercises, but for them to serve as inspiration, so that people do not go down the path of tears of anxiety, discomfort and overwhelm that my entire class has had to go through. All the materials here are entirely my own creation (or their co-authors, where indicated) and are available under the Creative Commons Attribution-ShareAlike 4.0 International License, to be modified and adapted as you see fit, as long as you keep a citation to the author and release the derivative work under the same license. So, if you think my works can be improved, or if you want to make any comments, feel free to use the \u0026ldquo;Improve this Page\u0026rdquo; buttons in the bottom right corner of each Post. You can also open an Issue directly in the Git Repository from which this page is generated. If you find my work on this site useful, you can make a donation in my Ko-Fi, which, although it won\u0026rsquo;t allow me to earn money despite the hours and hours of work dedicated to generate this page, at least it will motivate me to continue maintaining it and will allow me to pay for the domain and hosting, which, in total, costs about 20 euros per year.\nIf any teacher ( vamos julito! ) considers that their copyright is being violated, has complaints with this page, or, simply, wants to talk to me, he or she can send me an email to profesores-masterbc@loreak.org; in general, my position is that the works presented on this website represent entirely my own creations, and that, therefore, I am the sole owner behind the Intellectual Property of them, even if they are based (very slightly) on the statements defined by the different teachers; after all, [you can\u0026rsquo;t](https://www. copyright.gov/help/faq/faq-protect.html) patent an idea\nYou can find tables with my ratings out of 10 (with comments mostly in Spanish) in the top section, if you need to decide if the content is of good quality. The header image is available under the Unsplash License at Joel Vodell\u0026rsquo;s Unsplash\n","date":"March 20, 2022","hero":"/es/posts/master-en-biolog%C3%ADa-computacional/readme/hero.png","permalink":"https://www.pablomarcos.me/es/posts/master-en-biolog%C3%ADa-computacional/readme/","summary":"An English version of this document can be found below Hola! Soy Pablo, ex-alumno del Máster en Biología Computacional de la UPM durante el Curso 2021-2022. Durante el Máster, una de las cosas que más me han agobiado ha sido el ver que tareas que, en un principio, parecían de enorme complejidad, en realidad resultaban sencillas una vez tenías alguna guía que seguir, alguna página web encontrada en internet que te ayudase un poco a orientarte y a saber qué, exactamente, qué quieren los profesores de mí.","tags":null,"title":"README"},{"categories":null,"contents":"This jupyter notebook shall serve as accompanying material to this repositories\u0026rsquo; README, a report for the “Big Data Engineering” subject at UPM’s Master in Computational Biology. It is thus only intended as a recopilation of used code; for the full discussion, please refer to the README.\nDependencies Installation # First, we install jdk8 !apt-get install openjdk-8-jdk-headless -qq \u0026gt; /dev/null import os # And set the environment variable \u0026#39;JAVA_HOME\u0026#39;. os.environ[\u0026#34;JAVA_HOME\u0026#34;] = \u0026#34;/usr/lib/jvm/java-8-openjdk-amd64\u0026#34; !update-alternatives --set java /usr/lib/jvm/java-8-openjdk-amd64/jre/bin/java !java -version openjdk version \u0026quot;1.8.0_312\u0026quot;\rOpenJDK Runtime Environment (build 1.8.0_312-8u312-b07-0ubuntu1~18.04-b07)\rOpenJDK 64-Bit Server VM (build 25.312-b07, mixed mode)\r# We will also import sys (for conda) and set the python version to 3.7 (for orca) import sys; python_version = f\u0026#34;3.7.10\u0026#34; # We will install Miniconda to manage package instalations !wget https://repo.continuum.io/miniconda/Miniconda3-4.5.4-Linux-x86_64.sh !chmod +x Miniconda3-4.5.4-Linux-x86_64.sh !./Miniconda3-4.5.4-Linux-x86_64.sh -b -f -p /usr/local # Update it !conda install --channel defaults conda python=$python_version --yes !conda update --channel defaults --all --yes # And append it to the sys.path _ = (sys.path .append(f\u0026#34;/usr/local/lib/python3.7/site-packages\u0026#34;)) os.environ[\u0026#39;PYTHONHOME\u0026#39;]=\u0026#34;/usr/local\u0026#34; --2022-02-10 21:40:56-- https://repo.continuum.io/miniconda/Miniconda3-4.5.4-Linux-x86_64.sh\rResolving repo.continuum.io (repo.continuum.io)... 104.18.201.79, 104.18.200.79, 2606:4700::6812:c84f, ...\rConnecting to repo.continuum.io (repo.continuum.io)|104.18.201.79|:443... connected.\rHTTP request sent, awaiting response... 301 Moved Permanently\rLocation: https://repo.anaconda.com/miniconda/Miniconda3-4.5.4-Linux-x86_64.sh [following]\r--2022-02-10 21:40:56-- https://repo.anaconda.com/miniconda/Miniconda3-4.5.4-Linux-x86_64.sh\rResolving repo.anaconda.com (repo.anaconda.com)... 104.16.131.3, 104.16.130.3, 2606:4700::6810:8203, ...\rConnecting to repo.anaconda.com (repo.anaconda.com)|104.16.131.3|:443... connected.\rHTTP request sent, awaiting response... 200 OK\rLength: 58468498 (56M) [application/x-sh]\rSaving to: ‘Miniconda3-4.5.4-Linux-x86_64.sh.2’\rMiniconda3-4.5.4-Li 100%[===================\u0026gt;] 55.76M 234MB/s in 0.2s 2022-02-10 21:40:57 (234 MB/s) - ‘Miniconda3-4.5.4-Linux-x86_64.sh.2’ saved [58468498/58468498]\rPREFIX=/usr/local\rinstalling: python-3.6.5-hc3d631a_2 ...\rPython 3.6.5 :: Anaconda, Inc.\rinstalling: ca-certificates-2018.03.07-0 ...\rinstalling: conda-env-2.6.0-h36134e3_1 ...\rinstalling: libgcc-ng-7.2.0-hdf63c60_3 ...\rinstalling: libstdcxx-ng-7.2.0-hdf63c60_3 ...\rinstalling: libffi-3.2.1-hd88cf55_4 ...\rinstalling: ncurses-6.1-hf484d3e_0 ...\rinstalling: openssl-1.0.2o-h20670df_0 ...\rinstalling: tk-8.6.7-hc745277_3 ...\rinstalling: xz-5.2.4-h14c3975_4 ...\rinstalling: yaml-0.1.7-had09818_2 ...\rinstalling: zlib-1.2.11-ha838bed_2 ...\rinstalling: libedit-3.1.20170329-h6b74fdf_2 ...\rinstalling: readline-7.0-ha6073c6_4 ...\rinstalling: sqlite-3.23.1-he433501_0 ...\rinstalling: asn1crypto-0.24.0-py36_0 ...\rinstalling: certifi-2018.4.16-py36_0 ...\rinstalling: chardet-3.0.4-py36h0f667ec_1 ...\rinstalling: idna-2.6-py36h82fb2a8_1 ...\rinstalling: pycosat-0.6.3-py36h0a5515d_0 ...\rinstalling: pycparser-2.18-py36hf9f622e_1 ...\rinstalling: pysocks-1.6.8-py36_0 ...\rinstalling: ruamel_yaml-0.15.37-py36h14c3975_2 ...\rinstalling: six-1.11.0-py36h372c433_1 ...\rinstalling: cffi-1.11.5-py36h9745a5d_0 ...\rinstalling: setuptools-39.2.0-py36_0 ...\rinstalling: cryptography-2.2.2-py36h14c3975_0 ...\rinstalling: wheel-0.31.1-py36_0 ...\rinstalling: pip-10.0.1-py36_0 ...\rinstalling: pyopenssl-18.0.0-py36_0 ...\rinstalling: urllib3-1.22-py36hbe7ace6_0 ...\rinstalling: requests-2.18.4-py36he2e5f8d_1 ...\rinstalling: conda-4.5.4-py36_0 ...\runlinking: ca-certificates-2021.10.26-h06a4308_2\runlinking: certifi-2021.10.8-py37h06a4308_2\runlinking: cffi-1.15.0-py37hd667e15_1\runlinking: conda-4.11.0-py37h06a4308_0\runlinking: cryptography-36.0.0-py37h9ce1e76_0\runlinking: idna-3.3-pyhd3eb1b0_0\runlinking: libffi-3.3-he6710b0_2\runlinking: libgcc-ng-9.3.0-h5101ec6_17\runlinking: libstdcxx-ng-9.3.0-hd4cf53a_17\runlinking: ncurses-6.3-h7f8727e_2\runlinking: openssl-1.1.1m-h7f8727e_0\runlinking: pip-21.2.2-py37h06a4308_0\runlinking: pycosat-0.6.3-py37h27cfd23_0\runlinking: pycparser-2.21-pyhd3eb1b0_0\runlinking: pyopenssl-22.0.0-pyhd3eb1b0_0\runlinking: pysocks-1.7.1-py37_1\runlinking: python-3.7.11-h12debd9_0\runlinking: readline-8.1.2-h7f8727e_1\runlinking: requests-2.27.1-pyhd3eb1b0_0\runlinking: ruamel_yaml-0.15.100-py37h27cfd23_0\runlinking: setuptools-58.0.4-py37h06a4308_0\runlinking: six-1.16.0-pyhd3eb1b0_0\runlinking: sqlite-3.37.2-hc218d9a_0\runlinking: tk-8.6.11-h1ccaba5_0\runlinking: urllib3-1.26.8-pyhd3eb1b0_0\runlinking: wheel-0.37.1-pyhd3eb1b0_0\runlinking: xz-5.2.5-h7b6447c_0\runlinking: yaml-0.2.5-h7b6447c_0\runlinking: zlib-1.2.11-h7f8727e_4\rinstallation finished.\rWARNING:\rYou currently have a PYTHONPATH environment variable set. This may cause\runexpected behavior when running the Python interpreter in Miniconda3.\rFor best results, please verify that your PYTHONPATH only points to\rdirectories of packages that are compatible with the Python interpreter\rin Miniconda3: /usr/local\rSolving environment: - \u0008\u0008\\ \u0008\u0008| \u0008\u0008/ \u0008\u0008- \u0008\u0008\\ \u0008\u0008| \u0008\u0008/ \u0008\u0008- \u0008\u0008\\ \u0008\u0008| \u0008\u0008/ \u0008\u0008- \u0008\u0008\\ \u0008\u0008| \u0008\u0008/ \u0008\u0008- \u0008\u0008\\ \u0008\u0008| \u0008\u0008/ \u0008\u0008- \u0008\u0008\\ \u0008\u0008| \u0008\u0008/ \u0008\u0008- \u0008\u0008\\ \u0008\u0008| \u0008\u0008/ \u0008\u0008- \u0008\u0008\\ \u0008\u0008| \u0008\u0008/ \u0008\u0008- \u0008\u0008\\ \u0008\u0008| \u0008\u0008/ \u0008\u0008- \u0008\u0008\\ \u0008\u0008| \u0008\u0008/ \u0008\u0008- \u0008\u0008\\ \u0008\u0008| \u0008\u0008/ \u0008\u0008- \u0008\u0008\\ \u0008\u0008| \u0008\u0008/ \u0008\u0008- \u0008\u0008\\ \u0008\u0008| \u0008\u0008/ \u0008\u0008- \u0008\u0008\\ \u0008\u0008| \u0008\u0008/ \u0008\u0008- \u0008\u0008\\ \u0008\u0008| \u0008\u0008/ \u0008\u0008- \u0008\u0008\\ \u0008\u0008| \u0008\u0008/ \u0008\u0008- \u0008\u0008\\ \u0008\u0008| \u0008\u0008/ \u0008\u0008- \u0008\u0008\\ \u0008\u0008| \u0008\u0008/ \u0008\u0008- \u0008\u0008\\ \u0008\u0008| \u0008\u0008/ \u0008\u0008- \u0008\u0008\\ \u0008\u0008| \u0008\u0008/ \u0008\u0008- \u0008\u0008\\ \u0008\u0008| \u0008\u0008/ \u0008\u0008- \u0008\u0008\\ \u0008\u0008| \u0008\u0008/ \u0008\u0008- \u0008\u0008\\ \u0008\u0008| \u0008\u0008/ \u0008\u0008- \u0008\u0008\\ \u0008\u0008| \u0008\u0008/ \u0008\u0008- \u0008\u0008\\ \u0008\u0008| \u0008\u0008/ \u0008\u0008- \u0008\u0008\\ \u0008\u0008| \u0008\u0008/ \u0008\u0008- \u0008\u0008\\ \u0008\u0008| \u0008\u0008/ \u0008\u0008- \u0008\u0008\\ \u0008\u0008| \u0008\u0008/ \u0008\u0008- \u0008\u0008\\ \u0008\u0008| \u0008\u0008/ \u0008\u0008- \u0008\u0008\\ \u0008\u0008| \u0008\u0008/ \u0008\u0008- \u0008\u0008\\ \u0008\u0008| \u0008\u0008/ \u0008\u0008- \u0008\u0008\\ \u0008\u0008| \u0008\u0008/ \u0008\u0008- \u0008\u0008\\ \u0008\u0008| \u0008\u0008/ \u0008\u0008- \u0008\u0008\\ \u0008\u0008| \u0008\u0008/ \u0008\u0008- \u0008\u0008\\ \u0008\u0008| \u0008\u0008/ \u0008\u0008- \u0008\u0008\\ \u0008\u0008| \u0008\u0008/ \u0008\u0008- \u0008\u0008\\ \u0008\u0008| \u0008\u0008/ \u0008\u0008- \u0008\u0008\\ \u0008\u0008| \u0008\u0008/ \u0008\u0008- \u0008\u0008\\ \u0008\u0008| \u0008\u0008/ \u0008\u0008- \u0008\u0008\\ \u0008\u0008| \u0008\u0008/ \u0008\u0008- \u0008\u0008\\ \u0008\u0008| \u0008\u0008/ \u0008\u0008- \u0008\u0008\\ \u0008\u0008| \u0008\u0008/ \u0008\u0008- \u0008\u0008\\ \u0008\u0008| \u0008\u0008/ \u0008\u0008- \u0008\u0008\\ \u0008\u0008| \u0008\u0008/ \u0008\u0008- \u0008\u0008\\ \u0008\u0008| \u0008\u0008/ \u0008\u0008- \u0008\u0008\\ \u0008\u0008| \u0008\u0008/ \u0008\u0008- \u0008\u0008\\ \u0008\u0008| \u0008\u0008/ \u0008\u0008- \u0008\u0008\\ \u0008\u0008| \u0008\u0008/ \u0008\u0008- \u0008\u0008\\ \u0008\u0008| \u0008\u0008/ \u0008\u0008- \u0008\u0008\\ \u0008\u0008| \u0008\u0008/ \u0008\u0008- \u0008\u0008\\ \u0008\u0008| \u0008\u0008/ \u0008\u0008- \u0008\u0008\\ \u0008\u0008| \u0008\u0008/ \u0008\u0008- \u0008\u0008\\ \u0008\u0008| \u0008\u0008/ \u0008\u0008- \u0008\u0008\\ \u0008\u0008| \u0008\u0008/ \u0008\u0008- \u0008\u0008\\ \u0008\u0008| \u0008\u0008/ \u0008\u0008- \u0008\u0008\\ \u0008\u0008| \u0008\u0008/ \u0008\u0008- \u0008\u0008\\ \u0008\u0008| \u0008\u0008/ \u0008\u0008- \u0008\u0008\\ \u0008\u0008| \u0008\u0008/ \u0008\u0008- \u0008\u0008\\ \u0008\u0008| \u0008\u0008done\r## Package Plan ##\renvironment location: /usr/local\radded / updated specs: - conda\r- python=3.7.10\rThe following packages will be UPDATED:\rasn1crypto: 0.24.0-py36_0 --\u0026gt; 1.4.0-py_0 ca-certificates: 2018.03.07-0 --\u0026gt; 2021.10.26-h06a4308_2 certifi: 2018.4.16-py36_0 --\u0026gt; 2021.10.8-py37h06a4308_2\rcffi: 1.11.5-py36h9745a5d_0 --\u0026gt; 1.15.0-py37hd667e15_1 chardet: 3.0.4-py36h0f667ec_1 --\u0026gt; 4.0.0-py37h06a4308_1003 charset-normalizer: 2.0.4-pyhd3eb1b0_0 --\u0026gt; 2.0.4-pyhd3eb1b0_0 conda: 4.5.4-py36_0 --\u0026gt; 4.11.0-py37h06a4308_0 cryptography: 2.2.2-py36h14c3975_0 --\u0026gt; 36.0.0-py37h9ce1e76_0 idna: 2.6-py36h82fb2a8_1 --\u0026gt; 3.3-pyhd3eb1b0_0 libffi: 3.2.1-hd88cf55_4 --\u0026gt; 3.3-he6710b0_2 libgcc-ng: 7.2.0-hdf63c60_3 --\u0026gt; 9.1.0-hdf63c60_0 libstdcxx-ng: 7.2.0-hdf63c60_3 --\u0026gt; 9.1.0-hdf63c60_0 ncurses: 6.1-hf484d3e_0 --\u0026gt; 6.3-h7f8727e_2 openssl: 1.0.2o-h20670df_0 --\u0026gt; 1.1.1m-h7f8727e_0 pip: 10.0.1-py36_0 --\u0026gt; 21.2.2-py37h06a4308_0 pycosat: 0.6.3-py36h0a5515d_0 --\u0026gt; 0.6.3-py37h27cfd23_0 pycparser: 2.18-py36hf9f622e_1 --\u0026gt; 2.21-pyhd3eb1b0_0 pyopenssl: 18.0.0-py36_0 --\u0026gt; 22.0.0-pyhd3eb1b0_0 pysocks: 1.6.8-py36_0 --\u0026gt; 1.7.1-py37_1 python: 3.6.5-hc3d631a_2 --\u0026gt; 3.7.10-h12debd9_4 readline: 7.0-ha6073c6_4 --\u0026gt; 8.1.2-h7f8727e_1 requests: 2.18.4-py36he2e5f8d_1 --\u0026gt; 2.27.1-pyhd3eb1b0_0 ruamel_yaml: 0.15.37-py36h14c3975_2 --\u0026gt; 0.15.87-py37h7b6447c_0 setuptools: 39.2.0-py36_0 --\u0026gt; 58.0.4-py37h06a4308_0 six: 1.11.0-py36h372c433_1 --\u0026gt; 1.16.0-pyhd3eb1b0_0 sqlite: 3.23.1-he433501_0 --\u0026gt; 3.37.2-hc218d9a_0 tk: 8.6.7-hc745277_3 --\u0026gt; 8.6.11-h1ccaba5_0 tqdm: 4.62.3-pyhd3eb1b0_1 --\u0026gt; 4.62.3-pyhd3eb1b0_1 urllib3: 1.22-py36hbe7ace6_0 --\u0026gt; 1.26.8-pyhd3eb1b0_0 wheel: 0.31.1-py36_0 --\u0026gt; 0.37.1-pyhd3eb1b0_0 xz: 5.2.4-h14c3975_4 --\u0026gt; 5.2.5-h7b6447c_0 Preparing transaction: - \u0008\u0008\\ \u0008\u0008| \u0008\u0008/ \u0008\u0008- \u0008\u0008done\rVerifying transaction: | \u0008\u0008/ \u0008\u0008- \u0008\u0008\\ \u0008\u0008| \u0008\u0008/ \u0008\u0008- \u0008\u0008\\ \u0008\u0008| \u0008\u0008/ \u0008\u0008- \u0008\u0008\\ \u0008\u0008| \u0008\u0008/ \u0008\u0008- \u0008\u0008\\ \u0008\u0008| \u0008\u0008/ \u0008\u0008- \u0008\u0008\\ \u0008\u0008| \u0008\u0008/ \u0008\u0008- \u0008\u0008\\ \u0008\u0008| \u0008\u0008/ \u0008\u0008- \u0008\u0008\\ \u0008\u0008| \u0008\u0008/ \u0008\u0008- \u0008\u0008\\ \u0008\u0008| \u0008\u0008/ \u0008\u0008- \u0008\u0008\\ \u0008\u0008| \u0008\u0008/ \u0008\u0008- \u0008\u0008\\ \u0008\u0008| \u0008\u0008/ \u0008\u0008- \u0008\u0008\\ \u0008\u0008| \u0008\u0008/ \u0008\u0008- \u0008\u0008\\ \u0008\u0008| \u0008\u0008/ \u0008\u0008- \u0008\u0008\\ \u0008\u0008| \u0008\u0008done\rExecuting transaction: - \u0008\u0008\\ \u0008\u0008| \u0008\u0008/ \u0008\u0008- \u0008\u0008\\ \u0008\u0008| \u0008\u0008/ \u0008\u0008- \u0008\u0008\\ \u0008\u0008| \u0008\u0008/ \u0008\u0008- \u0008\u0008\\ \u0008\u0008| \u0008\u0008/ \u0008\u0008- \u0008\u0008\\ \u0008\u0008| \u0008\u0008/ \u0008\u0008- \u0008\u0008\\ \u0008\u0008| \u0008\u0008/ \u0008\u0008- \u0008\u0008\\ \u0008\u0008| \u0008\u0008/ \u0008\u0008- \u0008\u0008\\ \u0008\u0008| \u0008\u0008/ \u0008\u0008- \u0008\u0008\\ \u0008\u0008| \u0008\u0008/ \u0008\u0008- \u0008\u0008\\ \u0008\u0008| \u0008\u0008/ \u0008\u0008- \u0008\u0008\\ \u0008\u0008| \u0008\u0008/ \u0008\u0008- \u0008\u0008\\ \u0008\u0008| \u0008\u0008/ \u0008\u0008- \u0008\u0008\\ \u0008\u0008| \u0008\u0008/ \u0008\u0008- \u0008\u0008\\ \u0008\u0008| \u0008\u0008/ \u0008\u0008- \u0008\u0008\\ \u0008\u0008| \u0008\u0008/ \u0008\u0008- \u0008\u0008\\ \u0008\u0008| \u0008\u0008/ \u0008\u0008- \u0008\u0008\\ \u0008\u0008| \u0008\u0008/ \u0008\u0008- \u0008\u0008\\ \u0008\u0008| \u0008\u0008/ \u0008\u0008- \u0008\u0008\\ \u0008\u0008| \u0008\u0008/ \u0008\u0008- \u0008\u0008\\ \u0008\u0008done\rCollecting package metadata (current_repodata.json): - \u0008\u0008\\ \u0008\u0008| \u0008\u0008/ \u0008\u0008- \u0008\u0008\\ \u0008\u0008done\rSolving environment: / \u0008\u0008- \u0008\u0008\\ \u0008\u0008| \u0008\u0008done\r## Package Plan ##\renvironment location: /usr/local\rThe following packages will be REMOVED:\rasn1crypto-1.4.0-py_0\rchardet-4.0.0-py37h06a4308_1003\rconda-env-2.6.0-h36134e3_1\rlibedit-3.1.20170329-h6b74fdf_2\rThe following packages will be UPDATED:\rlibgcc-ng 9.1.0-hdf63c60_0 --\u0026gt; 9.3.0-h5101ec6_17\rlibstdcxx-ng 9.1.0-hdf63c60_0 --\u0026gt; 9.3.0-hd4cf53a_17\rpython 3.7.10-h12debd9_4 --\u0026gt; 3.7.11-h12debd9_0\rruamel_yaml 0.15.87-py37h7b6447c_0 --\u0026gt; 0.15.100-py37h27cfd23_0\ryaml 0.1.7-had09818_2 --\u0026gt; 0.2.5-h7b6447c_0\rzlib 1.2.11-ha838bed_2 --\u0026gt; 1.2.11-h7f8727e_4\rPreparing transaction: - \u0008\u0008done\rVerifying transaction: | \u0008\u0008/ \u0008\u0008- \u0008\u0008\\ \u0008\u0008done\rExecuting transaction: / \u0008\u0008- \u0008\u0008\\ \u0008\u0008done\r# For our ML processing, we will be using the latest pre-release version of BigDL Orca # Installing BigDL Orca from pip will automatically install pyspark, bigdl, and their dependencies. !pip install --pre --upgrade bigdl-orca Requirement already satisfied: bigdl-orca in /usr/local/lib/python3.7/site-packages (0.14.0b20220210)\rRequirement already satisfied: bigdl-dllib==0.14.0b20220210 in /usr/local/lib/python3.7/site-packages (from bigdl-orca) (0.14.0b20220210)\rRequirement already satisfied: packaging in /usr/local/lib/python3.7/site-packages (from bigdl-orca) (21.3)\rRequirement already satisfied: filelock in /usr/local/lib/python3.7/site-packages (from bigdl-orca) (3.4.2)\rRequirement already satisfied: bigdl-math==0.14.0.dev1 in /usr/local/lib/python3.7/site-packages (from bigdl-orca) (0.14.0.dev1)\rRequirement already satisfied: pyzmq in /usr/local/lib/python3.7/site-packages (from bigdl-orca) (22.3.0)\rRequirement already satisfied: conda-pack==0.3.1 in /usr/local/lib/python3.7/site-packages (from bigdl-orca) (0.3.1)\rRequirement already satisfied: bigdl-tf==0.14.0.dev1 in /usr/local/lib/python3.7/site-packages (from bigdl-orca) (0.14.0.dev1)\rRequirement already satisfied: pyspark==2.4.6 in /usr/local/lib/python3.7/site-packages (from bigdl-dllib==0.14.0b20220210-\u0026gt;bigdl-orca) (2.4.6)\rRequirement already satisfied: six\u0026gt;=1.10.0 in /usr/local/lib/python3.7/site-packages (from bigdl-dllib==0.14.0b20220210-\u0026gt;bigdl-orca) (1.16.0)\rRequirement already satisfied: numpy\u0026gt;=1.19.5 in /usr/local/lib/python3.7/site-packages (from bigdl-dllib==0.14.0b20220210-\u0026gt;bigdl-orca) (1.21.5)\rRequirement already satisfied: setuptools in /usr/local/lib/python3.7/site-packages (from conda-pack==0.3.1-\u0026gt;bigdl-orca) (58.0.4)\rRequirement already satisfied: py4j==0.10.7 in /usr/local/lib/python3.7/site-packages (from pyspark==2.4.6-\u0026gt;bigdl-dllib==0.14.0b20220210-\u0026gt;bigdl-orca) (0.10.7)\rRequirement already satisfied: pyparsing!=3.0.5,\u0026gt;=2.0.2 in /usr/local/lib/python3.7/site-packages (from packaging-\u0026gt;bigdl-orca) (3.0.7)\r\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\r# Sone other dependencies have to be installed manually !pip install torch==1.7.1 torchvision==0.8.2 !pip install six cloudpickle !pip install jep==3.9.0 Requirement already satisfied: torch==1.7.1 in /usr/local/lib/python3.7/site-packages (1.7.1)\rRequirement already satisfied: torchvision==0.8.2 in /usr/local/lib/python3.7/site-packages (0.8.2)\rRequirement already satisfied: typing-extensions in /usr/local/lib/python3.7/site-packages (from torch==1.7.1) (4.0.1)\rRequirement already satisfied: numpy in /usr/local/lib/python3.7/site-packages (from torch==1.7.1) (1.21.5)\rRequirement already satisfied: pillow\u0026gt;=4.1.1 in /usr/local/lib/python3.7/site-packages (from torchvision==0.8.2) (9.0.1)\r\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\rRequirement already satisfied: six in /usr/local/lib/python3.7/site-packages (1.16.0)\rRequirement already satisfied: cloudpickle in /usr/local/lib/python3.7/site-packages (2.0.0)\r\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\rRequirement already satisfied: jep==3.9.0 in /usr/local/lib/python3.7/site-packages (3.9.0)\r\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\r# Since we are working on colab, we will mount google drive into our system # in order to work with the necessary databases. from google.colab import drive drive.mount(\u0026#39;/content/drive\u0026#39;) Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\u0026quot;/content/drive\u0026quot;, force_remount=True).\rData Pre-processing # We will unzip the files (once only, thus commented) to our cloud storage and then #!unzip /content/drive/MyDrive/Colab_Notebooks/archive.zip -d /content/drive/MyDrive/Colab_Notebooks/ # define the path to the different folders: test_path = \u0026#39;/content/drive/MyDrive/Colab_Notebooks/chest_xray/test\u0026#39; train_path = \u0026#39;/content/drive/MyDrive/Colab_Notebooks/chest_xray/train\u0026#39; validation_path = \u0026#39;/content/drive/MyDrive/Colab_Notebooks/chest_xray/val\u0026#39; # For the processing of the images, we are going to be using pytorch # a ML Library originally developed by facebook import torch import torch.nn as nn import torch.optim as optim import torch.nn.functional as F import torchvision import torchvision.models as models import torchvision.transforms as T # First, we define a function that transforms (pre-processes) the images def custom_transform(sample): transformer = torchvision.transforms.Compose([T.CenterCrop(size=(299, 299)), T.ToTensor(), T.RandomHorizontalFlip(p=0.5), T.ColorJitter(brightness=0.5, hue=0), T.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),]) return transformer(sample[\u0026#34;image\u0026#34;]), sample[\u0026#34;label\u0026#34;] # The transformation consisted in resize the image to 224 x 224, flips 50% the image horizontallu. adjust the brigthness of the image and normalize. # Now, we will use some torch built-in functions to create a custom dataset # that we will feed to orca in order to create the model from torch.utils.data import Dataset, DataLoader import numpy as np from os import listdir from os.path import isfile, join from PIL import Image class CustomDataset(Dataset): \u0026#34;\u0026#34;\u0026#34;Face Landmarks dataset.\u0026#34;\u0026#34;\u0026#34; def __init__(self, root_dir, transform=None): \u0026#34;\u0026#34;\u0026#34; Args: root_dir (string): Directory with all the images. transform (callable, optional): Optional transform to be applied on a sample. \u0026#34;\u0026#34;\u0026#34; self.root_dir = root_dir self.transform = transform normal_names = [\u0026#34;NORMAL/\u0026#34; + f for f in listdir(join(root_dir, \u0026#34;NORMAL\u0026#34;)) if isfile(join(root_dir, \u0026#34;NORMAL\u0026#34;, f))] labels_normal = [0]*len(normal_names) pneumonia_names = [\u0026#34;PNEUMONIA/\u0026#34; + f for f in listdir(join(root_dir, \u0026#34;PNEUMONIA\u0026#34;)) if isfile(join(root_dir, \u0026#34;PNEUMONIA\u0026#34;, f))] labels_pneumonia = [1]*len(pneumonia_names) self.labels = labels_normal self.labels.extend(labels_pneumonia) self.labels = np.asarray(self.labels, dtype=np.float32) # labelling done self.filenames = normal_names self.filenames.extend(pneumonia_names) def __len__(self): return len(self.filenames) def __getitem__(self, idx): if torch.is_tensor(idx): idx = idx.tolist() img_name = os.path.join(self.root_dir, self.filenames[idx]) image = Image.open(img_name).convert(\u0026#34;RGB\u0026#34;) #print(image.shape) label = torch.Tensor([self.labels[idx]]) sample = {\u0026#34;image\u0026#34;: image, \u0026#34;label\u0026#34;: label} if self.transform: sample = self.transform(sample) return sample # We can now re-define the data to be of our \u0026#39;CustomDataset\u0026#39; class train_data = CustomDataset(train_path, transform=custom_transform) val_data = CustomDataset(validation_path, transform=custom_transform) test_data = CustomDataset(test_path, transform=custom_transform) # We load the data into pytorch batch_size = 32 train_loader = torch.utils.data.DataLoader(train_data, batch_size=batch_size, shuffle=True, num_workers=2) val_loader = torch.utils.data.DataLoader(val_data, batch_size=batch_size, shuffle=True, num_workers=2) test_loader = torch.utils.data.DataLoader(test_data, batch_size=batch_size, shuffle=False, num_workers=2) print(\u0026#39;Number of images for training: \u0026#39;, len(train_data)) print(\u0026#39;Number of images for testing: \u0026#39;, len(test_data)) print(\u0026#39;Number of images for validation: \u0026#39;, len(val_data)) Number of images for training: 5216\rNumber of images for testing: 624\rNumber of images for validation: 16\rIntegrated Network Stacking # We first import yet some more modules # In this case, the ones we will use for ML from __future__ import print_function import os import argparse from bigdl.orca import init_orca_context, stop_orca_context from bigdl.orca import OrcaContext # This will display terminal\u0026#39;s stdout and stderr in the Jupyter notebook, # and is recommended when running BigDL in Jupyter, as it makes troubleshooting easier OrcaContext.log_output = True cluster_mode = \u0026#34;local\u0026#34; if cluster_mode == \u0026#34;local\u0026#34;: # run in local mode init_orca_context(cores=1, memory=\u0026#34;8g\u0026#34;) elif cluster_mode == \u0026#34;k8s\u0026#34;: # run on K8s cluster init_orca_context(cluster_mode=\u0026#34;k8s\u0026#34;, num_nodes=2, cores=4) elif cluster_mode == \u0026#34;yarn\u0026#34;: # run on Hadoop YARN cluster init_orca_context( cluster_mode=\u0026#34;yarn-client\u0026#34;, cores=4, num_nodes=2, memory=\u0026#34;2g\u0026#34;, driver_memory=\u0026#34;10g\u0026#34;, driver_cores=1, conf={\u0026#34;spark.rpc.message.maxSize\u0026#34;: \u0026#34;1024\u0026#34;, \u0026#34;spark.task.maxFailures\u0026#34;: \u0026#34;1\u0026#34;, \u0026#34;spark.driver.extraJavaOptions\u0026#34;: \u0026#34;-Dbigdl.failure.retryTimes=1\u0026#34;}) Initializing orca context\rCurrent pyspark location is : /usr/local/lib/python3.7/site-packages/pyspark/__init__.py\rStart to getOrCreate SparkContext\rpyspark_submit_args is: --driver-class-path /usr/local/lib/python3.7/site-packages/bigdl/share/dllib/lib/bigdl-dllib-spark_2.4.6-0.14.0-SNAPSHOT-jar-with-dependencies.jar:/usr/local/lib/python3.7/site-packages/bigdl/share/orca/lib/bigdl-orca-spark_2.4.6-0.14.0-SNAPSHOT-jar-with-dependencies.jar pyspark-shell 2022-02-10 21:42:16 WARN NativeCodeLoader:62 - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\rSetting default log level to \u0026quot;WARN\u0026quot;.\rTo adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\r2022-02-10 21:42:20,290 Thread-4 WARN The bufferSize is set to 4000 but bufferedIo is false: false\r2022-02-10 21:42:20,294 Thread-4 WARN The bufferSize is set to 4000 but bufferedIo is false: false\r2022-02-10 21:42:20,295 Thread-4 WARN The bufferSize is set to 4000 but bufferedIo is false: false\r2022-02-10 21:42:20,296 Thread-4 WARN The bufferSize is set to 4000 but bufferedIo is false: false\r22-02-10 21:42:20 [Thread-4] INFO Engine$:121 - Auto detect executor number and executor cores number\r22-02-10 21:42:20 [Thread-4] INFO Engine$:123 - Executor number is 1 and executor cores number is 1\r22-02-10 21:42:21 [Thread-4] INFO ThreadPool$:95 - Set mkl threads to 1 on thread 15\r2022-02-10 21:42:21 WARN SparkContext:66 - Using an existing SparkContext; some configuration may not take effect.\r22-02-10 21:42:21 [Thread-4] INFO Engine$:446 - Find existing spark context. Checking the spark conf...\rcls.getname: com.intel.analytics.bigdl.dllib.utils.python.api.Sample\rBigDLBasePickler registering: bigdl.dllib.utils.common Sample\rcls.getname: com.intel.analytics.bigdl.dllib.utils.python.api.EvaluatedResult\rBigDLBasePickler registering: bigdl.dllib.utils.common EvaluatedResult\rcls.getname: com.intel.analytics.bigdl.dllib.utils.python.api.JTensor\rBigDLBasePickler registering: bigdl.dllib.utils.common JTensor\rcls.getname: com.intel.analytics.bigdl.dllib.utils.python.api.JActivity\rBigDLBasePickler registering: bigdl.dllib.utils.common JActivity\rSuccessfully got a SparkContext\rUser settings:\rKMP_AFFINITY=granularity=fine,compact,1,0\rKMP_BLOCKTIME=0\rKMP_SETTINGS=1\rOMP_NUM_THREADS=1\rEffective settings:\rKMP_ABORT_DELAY=0\rKMP_ADAPTIVE_LOCK_PROPS='1,1024'\rKMP_ALIGN_ALLOC=64\rKMP_ALL_THREADPRIVATE=128\rKMP_ATOMIC_MODE=2\rKMP_BLOCKTIME=0\rKMP_CPUINFO_FILE: value is not defined\rKMP_DETERMINISTIC_REDUCTION=false\rKMP_DEVICE_THREAD_LIMIT=2147483647\rKMP_DISP_HAND_THREAD=false\rKMP_DISP_NUM_BUFFERS=7\rKMP_DUPLICATE_LIB_OK=false\rKMP_FORCE_REDUCTION: value is not defined\rKMP_FOREIGN_THREADS_THREADPRIVATE=true\rKMP_FORKJOIN_BARRIER='2,2'\rKMP_FORKJOIN_BARRIER_PATTERN='hyper,hyper'\rKMP_FORKJOIN_FRAMES=true\rKMP_FORKJOIN_FRAMES_MODE=3\rKMP_GTID_MODE=3\rKMP_HANDLE_SIGNALS=false\rKMP_HOT_TEAMS_MAX_LEVEL=1\rKMP_HOT_TEAMS_MODE=0\rKMP_INIT_AT_FORK=true\rKMP_ITT_PREPARE_DELAY=0\rKMP_LIBRARY=throughput\rKMP_LOCK_KIND=queuing\rKMP_MALLOC_POOL_INCR=1M\rKMP_MWAIT_HINTS=0\rKMP_NUM_LOCKS_IN_BLOCK=1\rKMP_PLAIN_BARRIER='2,2'\rKMP_PLAIN_BARRIER_PATTERN='hyper,hyper'\rKMP_REDUCTION_BARRIER='1,1'\rKMP_REDUCTION_BARRIER_PATTERN='hyper,hyper'\rKMP_SCHEDULE='static,balanced;guided,iterative'\rKMP_SETTINGS=true\rKMP_SPIN_BACKOFF_PARAMS='4096,100'\rKMP_STACKOFFSET=64\rKMP_STACKPAD=0\rKMP_STACKSIZE=8M\rKMP_STORAGE_MAP=false\rKMP_TASKING=2\rKMP_TASKLOOP_MIN_TASKS=0\rKMP_TASK_STEALING_CONSTRAINT=1\rKMP_TEAMS_THREAD_LIMIT=4\rKMP_TOPOLOGY_METHOD=all\rKMP_USER_LEVEL_MWAIT=false\rKMP_USE_YIELD=1\rKMP_VERSION=false\rKMP_WARNINGS=true\rOMP_AFFINITY_FORMAT='OMP: pid %P tid %i thread %n bound to OS proc set {%A}'\rOMP_ALLOCATOR=omp_default_mem_alloc\rOMP_CANCELLATION=false\rOMP_DEBUG=disabled\rOMP_DEFAULT_DEVICE=0\rOMP_DISPLAY_AFFINITY=false\rOMP_DISPLAY_ENV=false\rOMP_DYNAMIC=false\rOMP_MAX_ACTIVE_LEVELS=2147483647\rOMP_MAX_TASK_PRIORITY=0\rOMP_NESTED=false\rOMP_NUM_THREADS='1'\rOMP_PLACES: value is not defined\rOMP_PROC_BIND='intel'\rOMP_SCHEDULE='static'\rOMP_STACKSIZE=8M\rOMP_TARGET_OFFLOAD=DEFAULT\rOMP_THREAD_LIMIT=2147483647\rOMP_TOOL=enabled\rOMP_TOOL_LIBRARIES: value is not defined\rOMP_WAIT_POLICY=PASSIVE\rKMP_AFFINITY='noverbose,warnings,respect,granularity=fine,compact,1,0'\rPossible 5-network stacking # Define the network\rclass IntegratedNet(nn.Module):\rdef __init__(self):\rsuper(IntegratedNet, self).__init__()\rself.resnet18 = models.resnet18(pretrained=True)\rself.resnet18.fc = nn.Linear(512, 32)\rself.densenet = models.densenet161(pretrained=True)\rself.densenet.classifier = nn.Linear(2208, 32)\rself.inception = models.inception_v3(pretrained=True)\rself.inception.fc = nn.Linear(2048, 32)\rself.mnasnet = models.mnasnet1_0(pretrained=True)\rself.mnasnet.classifier = nn.Sequential(nn.Dropout(0.2, inplace=True),\rnn.Linear(1280, 32))\rself.mobilenet_v2 = models.mobilenet_v2(pretrained=True)\rself.mobilenet_v2.classifier = nn.Sequential(nn.Dropout(0.2), nn.Linear(1280, 32))\rself.fc_out = nn.Linear(2*32, 1) # for binary classification, use single output\rdef forward(self, x):\rx_res = self.resnet18(x)\rx_dense = self.densenet(x.detach())\rx_inception = self.inception(x)[0]\rx_mnas = self.mnasnet(x)\rx_mobilenet = self.mobilenet_v2(x)\r#Concatenate the outputs\rx = torch.cat((x_res, x_dense, x_inception, x_mnas, x_mobilenet), axis=-1)\rx = self.fc_out(x)\rreturn x # Check that stacking works as expected a = torch.rand(size=(8, 32)) b = torch.rand(size=(8, 32)) c = torch.rand(size=(8, 32)) print(a.shape) d = torch.cat((a, b, c), axis=-1) print(d.shape) torch.Size([8, 32])\rtorch.Size([8, 96])\rThe stacking works well and we can concatenate the models if we have the same shape. That means that we will need to look at every last layer of each network to be sure they match. The networks themselves can be improved even further using industry-standard best practices for transfer learning\n# Define the network class IntegratedNet(nn.Module): def __init__(self): super(IntegratedNet, self).__init__() self.resnet18 = models.resnet18(pretrained=True) for param in self.resnet18.parameters(): param.requires_grad = False self.resnet18.avgpool = nn.Sequential(nn.ReLU(), nn.MaxPool2d(2, 2), nn.Conv2d(in_channels=512, out_channels=64, kernel_size=3, stride=1, padding=1), nn.BatchNorm2d(num_features=64), nn.ReLU(), nn.Conv2d(in_channels=64, out_channels=32, kernel_size=3, stride=1, padding=1), nn.BatchNorm2d(num_features=32), nn.ReLU(), nn.AdaptiveAvgPool2d(output_size=1)) self.resnet18.fc = nn.Identity() # This code will help us to mantain the weights and extract the features according to the papers research to improve the # performance of the transfering leaning (https://arxiv.org/pdf/1911.02685.pdf) self.mnasnet = models.mnasnet1_0(pretrained=True) for param in self.mnasnet.parameters(): param.requires_grad = False self.mnasnet.classifier = nn.Sequential(nn.Dropout(p=0.2, inplace=True), nn.ReLU(), nn.Linear(in_features=1280, out_features=256), nn.Dropout(p=0.2, inplace=True), nn.ReLU(), nn.Linear(256, 32)) self.fc_out = nn.Sequential(nn.Linear(2*32, 1), nn.Sigmoid()) # For binary classification, use single output # which will return values between 0 and 1 def forward(self, x): x_res = self.resnet18(x) x_mnas = self.mnasnet(x) x = torch.cat((x_res, x_mnas), axis=-1) x = self.fc_out(x) return x net = IntegratedNet() optimizer = optim.Adam(net.parameters(), lr=0.0001) # We define the training loss vs. the epochs criterion = nn.BCELoss() # and the binary cross entropy batch_size = 32 epochs = 2 # We can (finally) proceed to import the Orca stimators and metrics. # Since our output is binary (1: pneumonia + ; 0: pneumonia -) we need # a binary accuracy metric from bigdl.orca.learn.pytorch import Estimator from bigdl.orca.learn.metrics import BinaryAccuracy est = Estimator.from_torch(model=net, optimizer=optimizer, loss=criterion, metrics=[BinaryAccuracy()]) creating: createTorchLoss\rcreating: createTorchOptim\rcreating: createZooKerasBinaryAccuracy\rcreating: createEstimator\rTraining the Machine # Each epoch, we will try to fit the data from bigdl.orca.learn.trigger import EveryEpoch est.fit(data=train_loader, epochs=2, validation_data=test_loader, checkpoint_trigger=EveryEpoch()) creating: createEveryEpoch\rcreating: createMaxEpoch\r22-02-10 21:42:56 [Thread-4] INFO InternalDistriOptimizer$:944 - TorchModel[f33d891a] isTorch is true\r22-02-10 21:42:56 [Thread-4] INFO InternalDistriOptimizer$:950 - torch model will use 1 OMP threads.\r22-02-10 21:42:56 [Thread-4] INFO DistriOptimizer$:826 - caching training rdd ...\r22-02-10 21:43:26 [Thread-4] INFO DistriOptimizer$:652 - Cache thread models...\r22-02-10 21:43:26 [Executor task launch worker for task 5] INFO ThreadPool$:95 - Set mkl threads to 1 on thread 51\r22-02-10 21:43:26 [Executor task launch worker for task 5] INFO ThreadPool$:95 - Set mkl threads to 1 on thread 51\r22-02-10 21:43:26 [Executor task launch worker for task 5] INFO DistriOptimizer$:635 - model thread pool size is 1\r2022-02-10 21:43:26 WARN BlockManager:66 - Asked to remove block test_0weights0, which does not exist\r2022-02-10 21:43:26 WARN BlockManager:66 - Asked to remove block test_0gradients0, which does not exist\r22-02-10 21:43:26 [Thread-4] INFO DistriOptimizer$:654 - Cache thread models... done\r22-02-10 21:43:26 [Thread-4] INFO DistriOptimizer$:164 - Count dataset\r22-02-10 21:44:35 [Thread-4] INFO DistriOptimizer$:168 - Count dataset complete. Time elapsed: 68.657940791s\r22-02-10 21:45:43 [Thread-4] WARN DistriOptimizer$:170 - If the dataset is built directly from RDD[Minibatch], the data in each minibatch is fixed, and a single minibatch is randomly selected in each partition. If the dataset is transformed from RDD[Sample], each minibatch will be constructed on the fly from random samples, which is better for convergence.\r22-02-10 21:45:43 [Thread-4] INFO DistriOptimizer$:176 - config {\rcomputeThresholdbatchSize: 100\rmaxDropPercentage: 0.0\rwarmupIterationNum: 200\risLayerwiseScaled: false\rdropPercentage: 0.0\r}\r22-02-10 21:45:43 [Thread-4] INFO DistriOptimizer$:180 - Shuffle data\r22-02-10 21:45:43 [Thread-4] INFO DistriOptimizer$:183 - Shuffle data complete. Takes 1.2321E-4s\r22-02-10 21:46:07 [Thread-4] INFO DistriOptimizer$:430 - [Epoch 1 32/5216][Iteration 1][Wall Clock 18.958772659s] Trained 32.0 records in 18.958772659 seconds. Throughput is 1.687873 records/second. Loss is 0.7409593. 22-02-10 21:46:14 [Thread-4] INFO DistriOptimizer$:430 - [Epoch 1 64/5216][Iteration 2][Wall Clock 25.856106064s] Trained 32.0 records in 6.897333405 seconds. Throughput is 4.639474 records/second. Loss is 0.6954171. 22-02-10 21:46:21 [Thread-4] INFO DistriOptimizer$:430 - [Epoch 1 96/5216][Iteration 3][Wall Clock 32.647618171s] Trained 32.0 records in 6.791512107 seconds. Throughput is 4.711764 records/second. Loss is 0.67712724. 22-02-10 21:46:28 [Thread-4] INFO DistriOptimizer$:430 - [Epoch 1 128/5216][Iteration 4][Wall Clock 39.440653429s] Trained 32.0 records in 6.793035258 seconds. Throughput is 4.710707 records/second. Loss is 0.631244. 22-02-10 21:46:35 [Thread-4] INFO DistriOptimizer$:430 - [Epoch 1 160/5216][Iteration 5][Wall Clock 46.321818466s] Trained 32.0 records in 6.881165037 seconds. Throughput is 4.6503754 records/second. Loss is 0.5949562. 22-02-10 21:46:42 [Thread-4] INFO DistriOptimizer$:430 - [Epoch 1 192/5216][Iteration 6][Wall Clock 53.122212812s] Trained 32.0 records in 6.800394346 seconds. Throughput is 4.7056093 records/second. Loss is 0.5836719. 22-02-10 21:46:48 [Thread-4] INFO DistriOptimizer$:430 - [Epoch 1 224/5216][Iteration 7][Wall Clock 59.933256448s] Trained 32.0 records in 6.811043636 seconds. Throughput is 4.698252 records/second. Loss is 0.5472795. 22-02-10 21:46:55 [Thread-4] INFO DistriOptimizer$:430 - [Epoch 1 256/5216][Iteration 8][Wall Clock 66.715599266s] Trained 32.0 records in 6.782342818 seconds. Throughput is 4.7181334 records/second. Loss is 0.5152236. 22-02-10 21:47:02 [Thread-4] INFO DistriOptimizer$:430 - [Epoch 1 288/5216][Iteration 9][Wall Clock 73.506168474s] Trained 32.0 records in 6.790569208 seconds. Throughput is 4.712418 records/second. Loss is 0.5098698. 22-02-10 21:47:09 [Thread-4] INFO DistriOptimizer$:430 - [Epoch 1 320/5216][Iteration 10][Wall Clock 80.263415276s] Trained 32.0 records in 6.757246802 seconds. Throughput is 4.7356563 records/second. Loss is 0.47625983. 22-02-10 21:47:15 [Thread-4] INFO DistriOptimizer$:430 - [Epoch 1 352/5216][Iteration 11][Wall Clock 87.033361606s] Trained 32.0 records in 6.76994633 seconds. Throughput is 4.7267733 records/second. Loss is 0.49023643. 22-02-10 21:47:22 [Thread-4] INFO DistriOptimizer$:430 - [Epoch 1 384/5216][Iteration 12][Wall Clock 93.7706748s] Trained 32.0 records in 6.737313194 seconds. Throughput is 4.7496676 records/second. Loss is 0.44272727. 22-02-10 21:47:29 [Thread-4] INFO DistriOptimizer$:430 - [Epoch 1 416/5216][Iteration 13][Wall Clock 101.058661889s] Trained 32.0 records in 7.287987089 seconds. Throughput is 4.390787 records/second. Loss is 0.41462737. 22-02-10 21:47:40 [Thread-4] INFO DistriOptimizer$:430 - [Epoch 1 448/5216][Iteration 14][Wall Clock 111.393746668s] Trained 32.0 records in 10.335084779 seconds. Throughput is 3.0962493 records/second. Loss is 0.39545882. 22-02-10 21:47:47 [Thread-4] INFO DistriOptimizer$:430 - [Epoch 1 480/5216][Iteration 15][Wall Clock 118.859456396s] Trained 32.0 records in 7.465709728 seconds. Throughput is 4.2862635 records/second. Loss is 0.41585344. 22-02-10 21:47:54 [Thread-4] INFO DistriOptimizer$:430 - [Epoch 1 512/5216][Iteration 16][Wall Clock 125.658540022s] Trained 32.0 records in 6.799083626 seconds. Throughput is 4.7065167 records/second. Loss is 0.37532234. 22-02-10 21:48:01 [Thread-4] INFO DistriOptimizer$:430 - [Epoch 1 544/5216][Iteration 17][Wall Clock 132.404444545s] Trained 32.0 records in 6.745904523 seconds. Throughput is 4.743619 records/second. Loss is 0.349044. 22-02-10 21:48:08 [Thread-4] INFO DistriOptimizer$:430 - [Epoch 1 576/5216][Iteration 18][Wall Clock 139.22069048s] Trained 32.0 records in 6.816245935 seconds. Throughput is 4.6946664 records/second. Loss is 0.35089. 22-02-10 21:48:14 [Thread-4] INFO DistriOptimizer$:430 - [Epoch 1 608/5216][Iteration 19][Wall Clock 146.018535506s] Trained 32.0 records in 6.797845026 seconds. Throughput is 4.707374 records/second. Loss is 0.38825566. 22-02-10 21:48:21 [Thread-4] INFO DistriOptimizer$:430 - [Epoch 1 640/5216][Iteration 20][Wall Clock 152.783803897s] Trained 32.0 records in 6.765268391 seconds. Throughput is 4.7300415 records/second. Loss is 0.32899144. 22-02-10 21:48:28 [Thread-4] INFO DistriOptimizer$:430 - [Epoch 1 672/5216][Iteration 21][Wall Clock 159.622185819s] Trained 32.0 records in 6.838381922 seconds. Throughput is 4.6794696 records/second. Loss is 0.3136489. 22-02-10 21:48:35 [Thread-4] INFO DistriOptimizer$:430 - [Epoch 1 704/5216][Iteration 22][Wall Clock 166.446241182s] Trained 32.0 records in 6.824055363 seconds. Throughput is 4.689294 records/second. Loss is 0.31508517. 22-02-10 21:48:42 [Thread-4] INFO DistriOptimizer$:430 - [Epoch 1 736/5216][Iteration 23][Wall Clock 173.222055251s] Trained 32.0 records in 6.775814069 seconds. Throughput is 4.7226796 records/second. Loss is 0.33683923. 22-02-10 21:48:48 [Thread-4] INFO DistriOptimizer$:430 - [Epoch 1 768/5216][Iteration 24][Wall Clock 179.9197875s] Trained 32.0 records in 6.697732249 seconds. Throughput is 4.777736 records/second. Loss is 0.28181. 22-02-10 21:48:55 [Thread-4] INFO DistriOptimizer$:430 - [Epoch 1 800/5216][Iteration 25][Wall Clock 186.727560375s] Trained 32.0 records in 6.807772875 seconds. Throughput is 4.700509 records/second. Loss is 0.3039229. 22-02-10 21:49:02 [Thread-4] INFO DistriOptimizer$:430 - [Epoch 1 832/5216][Iteration 26][Wall Clock 193.522956942s] Trained 32.0 records in 6.795396567 seconds. Throughput is 4.70907 records/second. Loss is 0.33218768. 22-02-10 21:49:09 [Thread-4] INFO DistriOptimizer$:430 - [Epoch 1 864/5216][Iteration 27][Wall Clock 200.3053435s] Trained 32.0 records in 6.782386558 seconds. Throughput is 4.718103 records/second. Loss is 0.25397834. 22-02-10 21:49:15 [Thread-4] INFO DistriOptimizer$:430 - [Epoch 1 896/5216][Iteration 28][Wall Clock 207.017343117s] Trained 32.0 records in 6.711999617 seconds. Throughput is 4.767581 records/second. Loss is 0.24323863. 22-02-10 21:49:22 [Thread-4] INFO DistriOptimizer$:430 - [Epoch 1 928/5216][Iteration 29][Wall Clock 213.807601555s] Trained 32.0 records in 6.790258438 seconds. Throughput is 4.7126336 records/second. Loss is 0.20609272. 22-02-10 21:49:29 [Thread-4] INFO DistriOptimizer$:430 - [Epoch 1 960/5216][Iteration 30][Wall Clock 220.638302568s] Trained 32.0 records in 6.830701013 seconds. Throughput is 4.6847315 records/second. Loss is 0.30081338. 22-02-10 21:49:36 [Thread-4] INFO DistriOptimizer$:430 - [Epoch 1 992/5216][Iteration 31][Wall Clock 227.505102886s] Trained 32.0 records in 6.866800318 seconds. Throughput is 4.6601033 records/second. Loss is 0.36461398. 22-02-10 21:49:43 [Thread-4] INFO DistriOptimizer$:430 - [Epoch 1 1024/5216][Iteration 32][Wall Clock 234.315430881s] Trained 32.0 records in 6.810327995 seconds. Throughput is 4.6987457 records/second. Loss is 0.59439397. 22-02-10 21:49:50 [Thread-4] INFO DistriOptimizer$:430 - [Epoch 1 1056/5216][Iteration 33][Wall Clock 241.129526766s] Trained 32.0 records in 6.814095885 seconds. Throughput is 4.6961474 records/second. Loss is 0.35055575. 22-02-10 21:49:56 [Thread-4] INFO DistriOptimizer$:430 - [Epoch 1 1088/5216][Iteration 34][Wall Clock 247.834827934s] Trained 32.0 records in 6.705301168 seconds. Throughput is 4.772343 records/second. Loss is 0.25412917. 22-02-10 21:50:03 [Thread-4] INFO DistriOptimizer$:430 - [Epoch 1 1120/5216][Iteration 35][Wall Clock 254.625150112s] Trained 32.0 records in 6.790322178 seconds. Throughput is 4.7125893 records/second. Loss is 0.1865121. 22-02-10 21:50:10 [Thread-4] INFO DistriOptimizer$:430 - [Epoch 1 1152/5216][Iteration 36][Wall Clock 261.364822455s] Trained 32.0 records in 6.739672343 seconds. Throughput is 4.7480054 records/second. Loss is 0.31071025. 22-02-10 21:50:17 [Thread-4] INFO DistriOptimizer$:430 - [Epoch 1 1184/5216][Iteration 37][Wall Clock 268.101945448s] Trained 32.0 records in 6.737122993 seconds. Throughput is 4.749802 records/second. Loss is 0.2971943. 22-02-10 21:50:23 [Thread-4] INFO DistriOptimizer$:430 - [Epoch 1 1216/5216][Iteration 38][Wall Clock 274.868748738s] Trained 32.0 records in 6.76680329 seconds. Throughput is 4.7289686 records/second. Loss is 0.1827384. 22-02-10 21:50:30 [Thread-4] INFO DistriOptimizer$:430 - [Epoch 1 1248/5216][Iteration 39][Wall Clock 281.6256142s] Trained 32.0 records in 6.756865462 seconds. Throughput is 4.735924 records/second. Loss is 0.1761009. 22-02-10 21:50:37 [Thread-4] INFO DistriOptimizer$:430 - [Epoch 1 1280/5216][Iteration 40][Wall Clock 288.39495119s] Trained 32.0 records in 6.76933699 seconds. Throughput is 4.7271986 records/second. Loss is 0.4032011. 22-02-10 21:50:44 [Thread-4] INFO DistriOptimizer$:430 - [Epoch 1 1312/5216][Iteration 41][Wall Clock 295.183220978s] Trained 32.0 records in 6.788269788 seconds. Throughput is 4.714014 records/second. Loss is 0.27252793. 22-02-10 21:50:50 [Thread-4] INFO DistriOptimizer$:430 - [Epoch 1 1344/5216][Iteration 42][Wall Clock 301.921077822s] Trained 32.0 records in 6.737856844 seconds. Throughput is 4.7492847 records/second. Loss is 0.31315118. 22-02-10 21:50:57 [Thread-4] INFO DistriOptimizer$:430 - [Epoch 1 1376/5216][Iteration 43][Wall Clock 308.770502863s] Trained 32.0 records in 6.849425041 seconds. Throughput is 4.671925 records/second. Loss is 0.26336163. 22-02-10 21:51:04 [Thread-4] INFO DistriOptimizer$:430 - [Epoch 1 1408/5216][Iteration 44][Wall Clock 315.538317493s] Trained 32.0 records in 6.76781463 seconds. Throughput is 4.728262 records/second. Loss is 0.17597584. 22-02-10 21:51:11 [Thread-4] INFO DistriOptimizer$:430 - [Epoch 1 1440/5216][Iteration 45][Wall Clock 322.298508645s] Trained 32.0 records in 6.760191152 seconds. Throughput is 4.733594 records/second. Loss is 0.2258178. 22-02-10 21:51:18 [Thread-4] INFO DistriOptimizer$:430 - [Epoch 1 1472/5216][Iteration 46][Wall Clock 329.084691931s] Trained 32.0 records in 6.786183286 seconds. Throughput is 4.7154636 records/second. Loss is 0.22913301. 22-02-10 21:51:24 [Thread-4] INFO DistriOptimizer$:430 - [Epoch 1 1504/5216][Iteration 47][Wall Clock 335.819242218s] Trained 32.0 records in 6.734550287 seconds. Throughput is 4.7516165 records/second. Loss is 0.22196561. 22-02-10 21:51:31 [Thread-4] INFO DistriOptimizer$:430 - [Epoch 1 1536/5216][Iteration 48][Wall Clock 342.628164485s] Trained 32.0 records in 6.808922267 seconds. Throughput is 4.6997156 records/second. Loss is 0.2210895. 22-02-10 21:51:38 [Thread-4] INFO DistriOptimizer$:430 - [Epoch 1 1568/5216][Iteration 49][Wall Clock 349.484031166s] Trained 32.0 records in 6.855866681 seconds. Throughput is 4.6675353 records/second. Loss is 0.33125776. 22-02-10 21:51:45 [Thread-4] INFO DistriOptimizer$:430 - [Epoch 1 1600/5216][Iteration 50][Wall Clock 356.317319069s] Trained 32.0 records in 6.833287903 seconds. Throughput is 4.682958 records/second. Loss is 0.36880094. 22-02-10 21:51:52 [Thread-4] INFO DistriOptimizer$:430 - [Epoch 1 1632/5216][Iteration 51][Wall Clock 363.094483681s] Trained 32.0 records in 6.777164612 seconds. Throughput is 4.721739 records/second. Loss is 0.27387702. 22-02-10 21:51:58 [Thread-4] INFO DistriOptimizer$:430 - [Epoch 1 1664/5216][Iteration 52][Wall Clock 369.813809611s] Trained 32.0 records in 6.71932593 seconds. Throughput is 4.7623825 records/second. Loss is 0.1542029. 22-02-10 21:52:05 [Thread-4] INFO DistriOptimizer$:430 - [Epoch 1 1696/5216][Iteration 53][Wall Clock 376.619378499s] Trained 32.0 records in 6.805568888 seconds. Throughput is 4.7020316 records/second. Loss is 0.17516123. 22-02-10 21:52:12 [Thread-4] INFO DistriOptimizer$:430 - [Epoch 1 1728/5216][Iteration 54][Wall Clock 383.337157787s] Trained 32.0 records in 6.717779288 seconds. Throughput is 4.763479 records/second. Loss is 0.16206574. 22-02-10 21:52:19 [Thread-4] INFO DistriOptimizer$:430 - [Epoch 1 1760/5216][Iteration 55][Wall Clock 390.075957474s] Trained 32.0 records in 6.738799687 seconds. Throughput is 4.74862 records/second. Loss is 0.24132498. 22-02-10 21:52:25 [Thread-4] INFO DistriOptimizer$:430 - [Epoch 1 1792/5216][Iteration 56][Wall Clock 396.749384099s] Trained 32.0 records in 6.673426625 seconds. Throughput is 4.795138 records/second. Loss is 0.1807705. 22-02-10 21:52:32 [Thread-4] INFO DistriOptimizer$:430 - [Epoch 1 1824/5216][Iteration 57][Wall Clock 403.570949314s] Trained 32.0 records in 6.821565215 seconds. Throughput is 4.6910057 records/second. Loss is 0.21054552. 22-02-10 21:52:39 [Thread-4] INFO DistriOptimizer$:430 - [Epoch 1 1856/5216][Iteration 58][Wall Clock 410.351934625s] Trained 32.0 records in 6.780985311 seconds. Throughput is 4.719078 records/second. Loss is 0.22094488. 22-02-10 21:52:46 [Thread-4] INFO DistriOptimizer$:430 - [Epoch 1 1888/5216][Iteration 59][Wall Clock 417.134873645s] Trained 32.0 records in 6.78293902 seconds. Throughput is 4.717719 records/second. Loss is 0.29512477. 22-02-10 21:52:52 [Thread-4] INFO DistriOptimizer$:430 - [Epoch 1 1920/5216][Iteration 60][Wall Clock 423.784983823s] Trained 32.0 records in 6.650110178 seconds. Throughput is 4.81195 records/second. Loss is 0.18638173. 22-02-10 21:52:59 [Thread-4] INFO DistriOptimizer$:430 - [Epoch 1 1952/5216][Iteration 61][Wall Clock 430.535333277s] Trained 32.0 records in 6.750349454 seconds. Throughput is 4.740495 records/second. Loss is 0.13764668. 22-02-10 21:53:06 [Thread-4] INFO DistriOptimizer$:430 - [Epoch 1 1984/5216][Iteration 62][Wall Clock 437.319049228s] Trained 32.0 records in 6.783715951 seconds. Throughput is 4.717179 records/second. Loss is 0.22197035. 22-02-10 21:53:13 [Thread-4] INFO DistriOptimizer$:430 - [Epoch 1 2016/5216][Iteration 63][Wall Clock 444.135539783s] Trained 32.0 records in 6.816490555 seconds. Throughput is 4.6944976 records/second. Loss is 0.14469346. 22-02-10 21:53:19 [Thread-4] INFO DistriOptimizer$:430 - [Epoch 1 2048/5216][Iteration 64][Wall Clock 450.968796607s] Trained 32.0 records in 6.833256824 seconds. Throughput is 4.682979 records/second. Loss is 0.187809. 22-02-10 21:53:26 [Thread-4] INFO DistriOptimizer$:430 - [Epoch 1 2080/5216][Iteration 65][Wall Clock 457.73510084s] Trained 32.0 records in 6.766304233 seconds. Throughput is 4.7293177 records/second. Loss is 0.14461237. 22-02-10 21:53:33 [Thread-4] INFO DistriOptimizer$:430 - [Epoch 1 2112/5216][Iteration 66][Wall Clock 464.505803692s] Trained 32.0 records in 6.770702852 seconds. Throughput is 4.726245 records/second. Loss is 0.16437449. 22-02-10 21:53:40 [Thread-4] INFO DistriOptimizer$:430 - [Epoch 1 2144/5216][Iteration 67][Wall Clock 471.370337762s] Trained 32.0 records in 6.86453407 seconds. Throughput is 4.661642 records/second. Loss is 0.33725885. 22-02-10 21:53:47 [Thread-4] INFO DistriOptimizer$:430 - [Epoch 1 2176/5216][Iteration 68][Wall Clock 478.165744871s] Trained 32.0 records in 6.795407109 seconds. Throughput is 4.709063 records/second. Loss is 0.26953664. 22-02-10 21:53:54 [Thread-4] INFO DistriOptimizer$:430 - [Epoch 1 2208/5216][Iteration 69][Wall Clock 485.002154595s] Trained 32.0 records in 6.836409724 seconds. Throughput is 4.6808195 records/second. Loss is 0.21637137. 22-02-10 21:54:00 [Thread-4] INFO DistriOptimizer$:430 - [Epoch 1 2240/5216][Iteration 70][Wall Clock 491.817387401s] Trained 32.0 records in 6.815232806 seconds. Throughput is 4.695364 records/second. Loss is 0.19883518. 22-02-10 21:54:07 [Thread-4] INFO DistriOptimizer$:430 - [Epoch 1 2272/5216][Iteration 71][Wall Clock 498.581032464s] Trained 32.0 records in 6.763645063 seconds. Throughput is 4.731177 records/second. Loss is 0.13596493. 22-02-10 21:54:14 [Thread-4] INFO DistriOptimizer$:430 - [Epoch 1 2304/5216][Iteration 72][Wall Clock 505.354967625s] Trained 32.0 records in 6.773935161 seconds. Throughput is 4.7239895 records/second. Loss is 0.13678177. 22-02-10 21:54:21 [Thread-4] INFO DistriOptimizer$:430 - [Epoch 1 2336/5216][Iteration 73][Wall Clock 512.138171545s] Trained 32.0 records in 6.78320392 seconds. Throughput is 4.7175345 records/second. Loss is 0.2678615. 22-02-10 21:54:27 [Thread-4] INFO DistriOptimizer$:430 - [Epoch 1 2368/5216][Iteration 74][Wall Clock 518.88708231s] Trained 32.0 records in 6.748910765 seconds. Throughput is 4.7415056 records/second. Loss is 0.19904917. 22-02-10 21:54:34 [Thread-4] INFO DistriOptimizer$:430 - [Epoch 1 2400/5216][Iteration 75][Wall Clock 525.661121382s] Trained 32.0 records in 6.774039072 seconds. Throughput is 4.723917 records/second. Loss is 0.2796418. 22-02-10 21:54:41 [Thread-4] INFO DistriOptimizer$:430 - [Epoch 1 2432/5216][Iteration 76][Wall Clock 532.341938596s] Trained 32.0 records in 6.680817214 seconds. Throughput is 4.789833 records/second. Loss is 0.14647543. 22-02-10 21:54:48 [Thread-4] INFO DistriOptimizer$:430 - [Epoch 1 2464/5216][Iteration 77][Wall Clock 539.16928228s] Trained 32.0 records in 6.827343684 seconds. Throughput is 4.6870356 records/second. Loss is 0.32743. 22-02-10 21:54:54 [Thread-4] INFO DistriOptimizer$:430 - [Epoch 1 2496/5216][Iteration 78][Wall Clock 545.914887856s] Trained 32.0 records in 6.745605576 seconds. Throughput is 4.7438293 records/second. Loss is 0.18688497. 22-02-10 21:55:01 [Thread-4] INFO DistriOptimizer$:430 - [Epoch 1 2528/5216][Iteration 79][Wall Clock 552.647337904s] Trained 32.0 records in 6.732450048 seconds. Throughput is 4.753099 records/second. Loss is 0.23845446. 22-02-10 21:55:08 [Thread-4] INFO DistriOptimizer$:430 - [Epoch 1 2560/5216][Iteration 80][Wall Clock 559.371336613s] Trained 32.0 records in 6.723998709 seconds. Throughput is 4.759073 records/second. Loss is 0.16988738. 22-02-10 21:55:15 [Thread-4] INFO DistriOptimizer$:430 - [Epoch 1 2592/5216][Iteration 81][Wall Clock 566.131565306s] Trained 32.0 records in 6.760228693 seconds. Throughput is 4.7335677 records/second. Loss is 0.13906235. 22-02-10 21:55:21 [Thread-4] INFO DistriOptimizer$:430 - [Epoch 1 2624/5216][Iteration 82][Wall Clock 572.843014815s] Trained 32.0 records in 6.711449509 seconds. Throughput is 4.7679715 records/second. Loss is 0.21579969. 22-02-10 21:55:28 [Thread-4] INFO DistriOptimizer$:430 - [Epoch 1 2656/5216][Iteration 83][Wall Clock 579.533760008s] Trained 32.0 records in 6.690745193 seconds. Throughput is 4.782726 records/second. Loss is 0.15427372. 22-02-10 21:55:35 [Thread-4] INFO DistriOptimizer$:430 - [Epoch 1 2688/5216][Iteration 84][Wall Clock 586.372081151s] Trained 32.0 records in 6.838321143 seconds. Throughput is 4.679511 records/second. Loss is 0.14539114. 22-02-10 21:55:42 [Thread-4] INFO DistriOptimizer$:430 - [Epoch 1 2720/5216][Iteration 85][Wall Clock 593.096883119s] Trained 32.0 records in 6.724801968 seconds. Throughput is 4.7585044 records/second. Loss is 0.15692309. 22-02-10 21:55:48 [Thread-4] INFO DistriOptimizer$:430 - [Epoch 1 2752/5216][Iteration 86][Wall Clock 599.79599855s] Trained 32.0 records in 6.699115431 seconds. Throughput is 4.77675 records/second. Loss is 0.15897769. 22-02-10 21:55:55 [Thread-4] INFO DistriOptimizer$:430 - [Epoch 1 2784/5216][Iteration 87][Wall Clock 606.502156831s] Trained 32.0 records in 6.706158281 seconds. Throughput is 4.7717338 records/second. Loss is 0.13225605. 22-02-10 21:56:02 [Thread-4] INFO DistriOptimizer$:430 - [Epoch 1 2816/5216][Iteration 88][Wall Clock 613.30298778s] Trained 32.0 records in 6.800830949 seconds. Throughput is 4.7053075 records/second. Loss is 0.17590977. 22-02-10 21:56:09 [Thread-4] INFO DistriOptimizer$:430 - [Epoch 1 2848/5216][Iteration 89][Wall Clock 620.063338714s] Trained 32.0 records in 6.760350934 seconds. Throughput is 4.7334824 records/second. Loss is 0.18737496. 22-02-10 21:56:15 [Thread-4] INFO DistriOptimizer$:430 - [Epoch 1 2880/5216][Iteration 90][Wall Clock 626.859529402s] Trained 32.0 records in 6.796190688 seconds. Throughput is 4.70852 records/second. Loss is 0.2648821. 22-02-10 21:56:22 [Thread-4] INFO DistriOptimizer$:430 - [Epoch 1 2912/5216][Iteration 91][Wall Clock 633.611201937s] Trained 32.0 records in 6.751672535 seconds. Throughput is 4.7395663 records/second. Loss is 0.11329633. 22-02-10 21:56:29 [Thread-4] INFO DistriOptimizer$:430 - [Epoch 1 2944/5216][Iteration 92][Wall Clock 640.301207569s] Trained 32.0 records in 6.690005632 seconds. Throughput is 4.7832546 records/second. Loss is 0.15274645. 22-02-10 21:56:36 [Thread-4] INFO DistriOptimizer$:430 - [Epoch 1 2976/5216][Iteration 93][Wall Clock 647.115259835s] Trained 32.0 records in 6.814052266 seconds. Throughput is 4.696178 records/second. Loss is 0.25433618. 22-02-10 21:56:42 [Thread-4] INFO DistriOptimizer$:430 - [Epoch 1 3008/5216][Iteration 94][Wall Clock 653.898993906s] Trained 32.0 records in 6.783734071 seconds. Throughput is 4.7171664 records/second. Loss is 0.15290111. 22-02-10 21:56:49 [Thread-4] INFO DistriOptimizer$:430 - [Epoch 1 3040/5216][Iteration 95][Wall Clock 660.724480991s] Trained 32.0 records in 6.825487085 seconds. Throughput is 4.68831 records/second. Loss is 0.2581003. 22-02-10 21:56:56 [Thread-4] INFO DistriOptimizer$:430 - [Epoch 1 3072/5216][Iteration 96][Wall Clock 667.430151362s] Trained 32.0 records in 6.705670371 seconds. Throughput is 4.772081 records/second. Loss is 0.15010372. 22-02-10 21:57:03 [Thread-4] INFO DistriOptimizer$:430 - [Epoch 1 3104/5216][Iteration 97][Wall Clock 674.235728649s] Trained 32.0 records in 6.805577287 seconds. Throughput is 4.702026 records/second. Loss is 0.13532987. 22-02-10 21:57:09 [Thread-4] INFO DistriOptimizer$:430 - [Epoch 1 3136/5216][Iteration 98][Wall Clock 680.949401619s] Trained 32.0 records in 6.71367297 seconds. Throughput is 4.766392 records/second. Loss is 0.11280563. 22-02-10 21:57:16 [Thread-4] INFO DistriOptimizer$:430 - [Epoch 1 3168/5216][Iteration 99][Wall Clock 687.781583763s] Trained 32.0 records in 6.832182144 seconds. Throughput is 4.683716 records/second. Loss is 0.1720517. 22-02-10 21:57:23 [Thread-4] INFO DistriOptimizer$:430 - [Epoch 1 3200/5216][Iteration 100][Wall Clock 694.571733863s] Trained 32.0 records in 6.7901501 seconds. Throughput is 4.712709 records/second. Loss is 0.106909506. 22-02-10 21:57:30 [Thread-4] INFO DistriOptimizer$:430 - [Epoch 1 3232/5216][Iteration 101][Wall Clock 701.360011623s] Trained 32.0 records in 6.78827776 seconds. Throughput is 4.714009 records/second. Loss is 0.09179832. 22-02-10 21:57:37 [Thread-4] INFO DistriOptimizer$:430 - [Epoch 1 3264/5216][Iteration 102][Wall Clock 708.220740323s] Trained 32.0 records in 6.8607287 seconds. Throughput is 4.6642275 records/second. Loss is 0.22691554. 22-02-10 21:57:44 [Thread-4] INFO DistriOptimizer$:430 - [Epoch 1 3296/5216][Iteration 103][Wall Clock 714.996709434s] Trained 32.0 records in 6.775969111 seconds. Throughput is 4.722572 records/second. Loss is 0.14895684. 22-02-10 21:57:50 [Thread-4] INFO DistriOptimizer$:430 - [Epoch 1 3328/5216][Iteration 104][Wall Clock 721.731046821s] Trained 32.0 records in 6.734337387 seconds. Throughput is 4.7517667 records/second. Loss is 0.19885175. 22-02-10 21:57:57 [Thread-4] INFO DistriOptimizer$:430 - [Epoch 1 3360/5216][Iteration 105][Wall Clock 728.510744022s] Trained 32.0 records in 6.779697201 seconds. Throughput is 4.7199745 records/second. Loss is 0.19454193. 22-02-10 21:58:04 [Thread-4] INFO DistriOptimizer$:430 - [Epoch 1 3392/5216][Iteration 106][Wall Clock 735.372224752s] Trained 32.0 records in 6.86148073 seconds. Throughput is 4.6637163 records/second. Loss is 0.12056172. 22-02-10 21:58:11 [Thread-4] INFO DistriOptimizer$:430 - [Epoch 1 3424/5216][Iteration 107][Wall Clock 742.131011516s] Trained 32.0 records in 6.758786764 seconds. Throughput is 4.7345777 records/second. Loss is 0.137075. 22-02-10 21:58:17 [Thread-4] INFO DistriOptimizer$:430 - [Epoch 1 3456/5216][Iteration 108][Wall Clock 748.80626275s] Trained 32.0 records in 6.675251234 seconds. Throughput is 4.793827 records/second. Loss is 0.107153356. 22-02-10 21:58:24 [Thread-4] INFO DistriOptimizer$:430 - [Epoch 1 3488/5216][Iteration 109][Wall Clock 755.668238141s] Trained 32.0 records in 6.861975391 seconds. Throughput is 4.66338 records/second. Loss is 0.19318223. 22-02-10 21:58:31 [Thread-4] INFO DistriOptimizer$:430 - [Epoch 1 3520/5216][Iteration 110][Wall Clock 762.429594185s] Trained 32.0 records in 6.761356044 seconds. Throughput is 4.7327785 records/second. Loss is 0.15277529. 22-02-10 21:58:38 [Thread-4] INFO DistriOptimizer$:430 - [Epoch 1 3552/5216][Iteration 111][Wall Clock 769.212623245s] Trained 32.0 records in 6.78302906 seconds. Throughput is 4.717656 records/second. Loss is 0.09626183. 22-02-10 21:58:45 [Thread-4] INFO DistriOptimizer$:430 - [Epoch 1 3584/5216][Iteration 112][Wall Clock 776.014308834s] Trained 32.0 records in 6.801685589 seconds. Throughput is 4.704716 records/second. Loss is 0.10834856. 22-02-10 21:58:51 [Thread-4] INFO DistriOptimizer$:430 - [Epoch 1 3616/5216][Iteration 113][Wall Clock 782.710978996s] Trained 32.0 records in 6.696670162 seconds. Throughput is 4.7784944 records/second. Loss is 0.1213431. 22-02-10 21:58:58 [Thread-4] INFO DistriOptimizer$:430 - [Epoch 1 3648/5216][Iteration 114][Wall Clock 789.491921627s] Trained 32.0 records in 6.780942631 seconds. Throughput is 4.719108 records/second. Loss is 0.11271076. 22-02-10 21:59:05 [Thread-4] INFO DistriOptimizer$:430 - [Epoch 1 3680/5216][Iteration 115][Wall Clock 796.222577094s] Trained 32.0 records in 6.730655467 seconds. Throughput is 4.754366 records/second. Loss is 0.24562243. 22-02-10 21:59:12 [Thread-4] INFO DistriOptimizer$:430 - [Epoch 1 3712/5216][Iteration 116][Wall Clock 803.013196193s] Trained 32.0 records in 6.790619099 seconds. Throughput is 4.7123833 records/second. Loss is 0.17568555. 22-02-10 21:59:18 [Thread-4] INFO DistriOptimizer$:430 - [Epoch 1 3744/5216][Iteration 117][Wall Clock 809.821113871s] Trained 32.0 records in 6.807917678 seconds. Throughput is 4.7004094 records/second. Loss is 0.14830168. 22-02-10 21:59:25 [Thread-4] INFO DistriOptimizer$:430 - [Epoch 1 3776/5216][Iteration 118][Wall Clock 816.53414131s] Trained 32.0 records in 6.713027439 seconds. Throughput is 4.766851 records/second. Loss is 0.121589705. 22-02-10 21:59:32 [Thread-4] INFO DistriOptimizer$:430 - [Epoch 1 3808/5216][Iteration 119][Wall Clock 823.276304715s] Trained 32.0 records in 6.742163405 seconds. Throughput is 4.746251 records/second. Loss is 0.15787977. 22-02-10 21:59:39 [Thread-4] INFO DistriOptimizer$:430 - [Epoch 1 3840/5216][Iteration 120][Wall Clock 830.116225628s] Trained 32.0 records in 6.839920913 seconds. Throughput is 4.6784167 records/second. Loss is 0.10928552. 22-02-10 21:59:45 [Thread-4] INFO DistriOptimizer$:430 - [Epoch 1 3872/5216][Iteration 121][Wall Clock 836.916152179s] Trained 32.0 records in 6.799926551 seconds. Throughput is 4.705933 records/second. Loss is 0.085648045. 22-02-10 21:59:52 [Thread-4] INFO DistriOptimizer$:430 - [Epoch 1 3904/5216][Iteration 122][Wall Clock 843.693556481s] Trained 32.0 records in 6.777404302 seconds. Throughput is 4.7215714 records/second. Loss is 0.21453269. 22-02-10 21:59:59 [Thread-4] INFO DistriOptimizer$:430 - [Epoch 1 3936/5216][Iteration 123][Wall Clock 850.444829906s] Trained 32.0 records in 6.751273425 seconds. Throughput is 4.739846 records/second. Loss is 0.0913267. 22-02-10 22:00:06 [Thread-4] INFO DistriOptimizer$:430 - [Epoch 1 3968/5216][Iteration 124][Wall Clock 857.187161634s] Trained 32.0 records in 6.742331728 seconds. Throughput is 4.746133 records/second. Loss is 0.07844189. 22-02-10 22:00:13 [Thread-4] INFO DistriOptimizer$:430 - [Epoch 1 4000/5216][Iteration 125][Wall Clock 864.020873871s] Trained 32.0 records in 6.833712237 seconds. Throughput is 4.6826673 records/second. Loss is 0.086605534. 22-02-10 22:00:19 [Thread-4] INFO DistriOptimizer$:430 - [Epoch 1 4032/5216][Iteration 126][Wall Clock 870.820868092s] Trained 32.0 records in 6.799994221 seconds. Throughput is 4.7058864 records/second. Loss is 0.079634. 22-02-10 22:00:26 [Thread-4] INFO DistriOptimizer$:430 - [Epoch 1 4064/5216][Iteration 127][Wall Clock 877.581527845s] Trained 32.0 records in 6.760659753 seconds. Throughput is 4.733266 records/second. Loss is 0.06550676. 22-02-10 22:00:33 [Thread-4] INFO DistriOptimizer$:430 - [Epoch 1 4096/5216][Iteration 128][Wall Clock 884.401044821s] Trained 32.0 records in 6.819516976 seconds. Throughput is 4.6924143 records/second. Loss is 0.09587483. 22-02-10 22:00:40 [Thread-4] INFO DistriOptimizer$:430 - [Epoch 1 4128/5216][Iteration 129][Wall Clock 891.209141618s] Trained 32.0 records in 6.808096797 seconds. Throughput is 4.7002854 records/second. Loss is 0.08211537. 22-02-10 22:00:47 [Thread-4] INFO DistriOptimizer$:430 - [Epoch 1 4160/5216][Iteration 130][Wall Clock 898.062641029s] Trained 32.0 records in 6.853499411 seconds. Throughput is 4.6691475 records/second. Loss is 0.089452386. 22-02-10 22:00:53 [Thread-4] INFO DistriOptimizer$:430 - [Epoch 1 4192/5216][Iteration 131][Wall Clock 904.776157688s] Trained 32.0 records in 6.713516659 seconds. Throughput is 4.7665033 records/second. Loss is 0.06624305. 22-02-10 22:01:00 [Thread-4] INFO DistriOptimizer$:430 - [Epoch 1 4224/5216][Iteration 132][Wall Clock 911.565186978s] Trained 32.0 records in 6.78902929 seconds. Throughput is 4.713487 records/second. Loss is 0.0826805. 22-02-10 22:01:07 [Thread-4] INFO DistriOptimizer$:430 - [Epoch 1 4256/5216][Iteration 133][Wall Clock 918.314392512s] Trained 32.0 records in 6.749205534 seconds. Throughput is 4.7412987 records/second. Loss is 0.14187321. 22-02-10 22:01:14 [Thread-4] INFO DistriOptimizer$:430 - [Epoch 1 4288/5216][Iteration 134][Wall Clock 925.072321315s] Trained 32.0 records in 6.757928803 seconds. Throughput is 4.7351785 records/second. Loss is 0.06650776. 22-02-10 22:01:20 [Thread-4] INFO DistriOptimizer$:430 - [Epoch 1 4320/5216][Iteration 135][Wall Clock 931.855824566s] Trained 32.0 records in 6.783503251 seconds. Throughput is 4.7173266 records/second. Loss is 0.15353185. 22-02-10 22:01:27 [Thread-4] INFO DistriOptimizer$:430 - [Epoch 1 4352/5216][Iteration 136][Wall Clock 938.720085626s] Trained 32.0 records in 6.86426106 seconds. Throughput is 4.661827 records/second. Loss is 0.23408332. 22-02-10 22:01:34 [Thread-4] INFO DistriOptimizer$:430 - [Epoch 1 4384/5216][Iteration 137][Wall Clock 945.664278366s] Trained 32.0 records in 6.94419274 seconds. Throughput is 4.6081667 records/second. Loss is 0.14926878. 22-02-10 22:01:41 [Thread-4] INFO DistriOptimizer$:430 - [Epoch 1 4416/5216][Iteration 138][Wall Clock 952.449862686s] Trained 32.0 records in 6.78558432 seconds. Throughput is 4.71588 records/second. Loss is 0.10525458. 22-02-10 22:01:48 [Thread-4] INFO DistriOptimizer$:430 - [Epoch 1 4448/5216][Iteration 139][Wall Clock 959.251484564s] Trained 32.0 records in 6.801621878 seconds. Throughput is 4.70476 records/second. Loss is 0.1341818. 22-02-10 22:01:55 [Thread-4] INFO DistriOptimizer$:430 - [Epoch 1 4480/5216][Iteration 140][Wall Clock 966.020428086s] Trained 32.0 records in 6.768943522 seconds. Throughput is 4.7274733 records/second. Loss is 0.06436069. 22-02-10 22:02:01 [Thread-4] INFO DistriOptimizer$:430 - [Epoch 1 4512/5216][Iteration 141][Wall Clock 972.786675079s] Trained 32.0 records in 6.766246993 seconds. Throughput is 4.7293577 records/second. Loss is 0.09290875. 22-02-10 22:02:08 [Thread-4] INFO DistriOptimizer$:430 - [Epoch 1 4544/5216][Iteration 142][Wall Clock 979.535595624s] Trained 32.0 records in 6.748920545 seconds. Throughput is 4.741499 records/second. Loss is 0.13062987. 22-02-10 22:02:15 [Thread-4] INFO DistriOptimizer$:430 - [Epoch 1 4576/5216][Iteration 143][Wall Clock 986.343917332s] Trained 32.0 records in 6.808321708 seconds. Throughput is 4.7001305 records/second. Loss is 0.15823394. 22-02-10 22:02:22 [Thread-4] INFO DistriOptimizer$:430 - [Epoch 1 4608/5216][Iteration 144][Wall Clock 993.15293503s] Trained 32.0 records in 6.809017698 seconds. Throughput is 4.69965 records/second. Loss is 0.10641916. 22-02-10 22:02:28 [Thread-4] INFO DistriOptimizer$:430 - [Epoch 1 4640/5216][Iteration 145][Wall Clock 999.918971653s] Trained 32.0 records in 6.766036623 seconds. Throughput is 4.7295046 records/second. Loss is 0.07621796. 22-02-10 22:02:35 [Thread-4] INFO DistriOptimizer$:430 - [Epoch 1 4672/5216][Iteration 146][Wall Clock 1006.65198961s] Trained 32.0 records in 6.733017957 seconds. Throughput is 4.752698 records/second. Loss is 0.061431445. 22-02-10 22:02:42 [Thread-4] INFO DistriOptimizer$:430 - [Epoch 1 4704/5216][Iteration 147][Wall Clock 1013.419359322s] Trained 32.0 records in 6.767369712 seconds. Throughput is 4.728573 records/second. Loss is 0.10362315. 22-02-10 22:02:49 [Thread-4] INFO DistriOptimizer$:430 - [Epoch 1 4736/5216][Iteration 148][Wall Clock 1020.154452648s] Trained 32.0 records in 6.735093326 seconds. Throughput is 4.7512336 records/second. Loss is 0.11575904. 22-02-10 22:02:56 [Thread-4] INFO DistriOptimizer$:430 - [Epoch 1 4768/5216][Iteration 149][Wall Clock 1026.927711169s] Trained 32.0 records in 6.773258521 seconds. Throughput is 4.7244616 records/second. Loss is 0.05681411. 22-02-10 22:03:02 [Thread-4] INFO DistriOptimizer$:430 - [Epoch 1 4800/5216][Iteration 150][Wall Clock 1033.694225571s] Trained 32.0 records in 6.766514402 seconds. Throughput is 4.7291703 records/second. Loss is 0.060877793. 22-02-10 22:03:09 [Thread-4] INFO DistriOptimizer$:430 - [Epoch 1 4832/5216][Iteration 151][Wall Clock 1040.516422206s] Trained 32.0 records in 6.822196635 seconds. Throughput is 4.6905713 records/second. Loss is 0.042783625. 22-02-10 22:03:16 [Thread-4] INFO DistriOptimizer$:430 - [Epoch 1 4864/5216][Iteration 152][Wall Clock 1047.326092743s] Trained 32.0 records in 6.809670537 seconds. Throughput is 4.6991997 records/second. Loss is 0.08216939. 22-02-10 22:03:23 [Thread-4] INFO DistriOptimizer$:430 - [Epoch 1 4896/5216][Iteration 153][Wall Clock 1054.120556031s] Trained 32.0 records in 6.794463288 seconds. Throughput is 4.7097173 records/second. Loss is 0.07741855. 22-02-10 22:03:29 [Thread-4] INFO DistriOptimizer$:430 - [Epoch 1 4928/5216][Iteration 154][Wall Clock 1060.852311018s] Trained 32.0 records in 6.731754987 seconds. Throughput is 4.7535896 records/second. Loss is 0.2626429. 22-02-10 22:03:36 [Thread-4] INFO DistriOptimizer$:430 - [Epoch 1 4960/5216][Iteration 155][Wall Clock 1067.612095532s] Trained 32.0 records in 6.759784514 seconds. Throughput is 4.7338786 records/second. Loss is 0.07869965. 22-02-10 22:03:43 [Thread-4] INFO DistriOptimizer$:430 - [Epoch 1 4992/5216][Iteration 156][Wall Clock 1074.320605472s] Trained 32.0 records in 6.70850994 seconds. Throughput is 4.7700605 records/second. Loss is 0.093527876. 22-02-10 22:03:50 [Thread-4] INFO DistriOptimizer$:430 - [Epoch 1 5024/5216][Iteration 157][Wall Clock 1081.079708425s] Trained 32.0 records in 6.759102953 seconds. Throughput is 4.734356 records/second. Loss is 0.15742125. 22-02-10 22:03:56 [Thread-4] INFO DistriOptimizer$:430 - [Epoch 1 5056/5216][Iteration 158][Wall Clock 1087.782810616s] Trained 32.0 records in 6.703102191 seconds. Throughput is 4.773909 records/second. Loss is 0.04349641. 22-02-10 22:04:03 [Thread-4] INFO DistriOptimizer$:430 - [Epoch 1 5088/5216][Iteration 159][Wall Clock 1094.577397916s] Trained 32.0 records in 6.7945873 seconds. Throughput is 4.7096314 records/second. Loss is 0.078870356. 22-02-10 22:04:10 [Thread-4] INFO DistriOptimizer$:430 - [Epoch 1 5120/5216][Iteration 160][Wall Clock 1101.370426085s] Trained 32.0 records in 6.793028169 seconds. Throughput is 4.710712 records/second. Loss is 0.04059176. 22-02-10 22:04:17 [Thread-4] INFO DistriOptimizer$:430 - [Epoch 1 5152/5216][Iteration 161][Wall Clock 1108.111915812s] Trained 32.0 records in 6.741489727 seconds. Throughput is 4.746725 records/second. Loss is 0.041039396. 22-02-10 22:04:23 [Thread-4] INFO DistriOptimizer$:430 - [Epoch 1 5184/5216][Iteration 162][Wall Clock 1114.791108766s] Trained 32.0 records in 6.679192954 seconds. Throughput is 4.790998 records/second. Loss is 0.05375761. 22-02-10 22:04:30 [Thread-4] INFO DistriOptimizer$:430 - [Epoch 1 5216/5216][Iteration 163][Wall Clock 1121.554214669s] Trained 32.0 records in 6.763105903 seconds. Throughput is 4.731554 records/second. Loss is 0.059842613. 22-02-10 22:04:30 [Thread-4] INFO DistriOptimizer$:475 - [Epoch 1 5216/5216][Iteration 163][Wall Clock 1121.554214669s] Epoch finished. Wall clock time is 1127524.834879 ms\r22-02-10 22:04:30 [Thread-4] INFO DistriOptimizer$:112 - [Epoch 1 5216/5216][Iteration 163][Wall Clock 1121.554214669s] Validate model...\r22-02-10 22:06:12 [Thread-4] INFO DistriOptimizer$:178 - [Epoch 1 5216/5216][Iteration 163][Wall Clock 1121.554214669s] validate model throughput is 6.1448555 records/second\r22-02-10 22:06:12 [Thread-4] INFO DistriOptimizer$:181 - [Epoch 1 5216/5216][Iteration 163][Wall Clock 1121.554214669s] Top1Accuracy is Accuracy(correct: 404, count: 624, accuracy: 0.6474358974358975)\r22-02-10 22:06:18 [Thread-4] INFO DistriOptimizer$:430 - [Epoch 2 32/5216][Iteration 164][Wall Clock 1134.273668244s] Trained 32.0 records in 6.748833365 seconds. Throughput is 4.7415605 records/second. Loss is 0.054734804. 22-02-10 22:06:25 [Thread-4] INFO DistriOptimizer$:430 - [Epoch 2 64/5216][Iteration 165][Wall Clock 1140.965908117s] Trained 32.0 records in 6.692239873 seconds. Throughput is 4.7816577 records/second. Loss is 0.10637416. 22-02-10 22:06:32 [Thread-4] INFO DistriOptimizer$:430 - [Epoch 2 96/5216][Iteration 166][Wall Clock 1147.765643725s] Trained 32.0 records in 6.799735608 seconds. Throughput is 4.706065 records/second. Loss is 0.06186908. 22-02-10 22:06:39 [Thread-4] INFO DistriOptimizer$:430 - [Epoch 2 128/5216][Iteration 167][Wall Clock 1154.540941597s] Trained 32.0 records in 6.775297872 seconds. Throughput is 4.7230396 records/second. Loss is 0.03976368. 22-02-10 22:06:45 [Thread-4] INFO DistriOptimizer$:430 - [Epoch 2 160/5216][Iteration 168][Wall Clock 1161.235367919s] Trained 32.0 records in 6.694426322 seconds. Throughput is 4.7800956 records/second. Loss is 0.048289426. 22-02-10 22:06:52 [Thread-4] INFO DistriOptimizer$:430 - [Epoch 2 192/5216][Iteration 169][Wall Clock 1167.997214223s] Trained 32.0 records in 6.761846304 seconds. Throughput is 4.732435 records/second. Loss is 0.05610858. 22-02-10 22:06:59 [Thread-4] INFO DistriOptimizer$:430 - [Epoch 2 224/5216][Iteration 170][Wall Clock 1174.688523385s] Trained 32.0 records in 6.691309162 seconds. Throughput is 4.782323 records/second. Loss is 0.3479316. 22-02-10 22:07:06 [Thread-4] INFO DistriOptimizer$:430 - [Epoch 2 256/5216][Iteration 171][Wall Clock 1181.413911663s] Trained 32.0 records in 6.725388278 seconds. Throughput is 4.7580895 records/second. Loss is 0.04353298. 22-02-10 22:07:12 [Thread-4] INFO DistriOptimizer$:430 - [Epoch 2 288/5216][Iteration 172][Wall Clock 1188.099443836s] Trained 32.0 records in 6.685532173 seconds. Throughput is 4.786455 records/second. Loss is 0.06224461. 22-02-10 22:07:19 [Thread-4] INFO DistriOptimizer$:430 - [Epoch 2 320/5216][Iteration 173][Wall Clock 1194.85678389s] Trained 32.0 records in 6.757340054 seconds. Throughput is 4.7355914 records/second. Loss is 0.0697163. 22-02-10 22:07:26 [Thread-4] INFO DistriOptimizer$:430 - [Epoch 2 352/5216][Iteration 174][Wall Clock 1201.603889785s] Trained 32.0 records in 6.747105895 seconds. Throughput is 4.742774 records/second. Loss is 0.057634998. 22-02-10 22:07:33 [Thread-4] INFO DistriOptimizer$:430 - [Epoch 2 384/5216][Iteration 175][Wall Clock 1208.333147672s] Trained 32.0 records in 6.729257887 seconds. Throughput is 4.7553535 records/second. Loss is 0.06809784. 22-02-10 22:07:39 [Thread-4] INFO DistriOptimizer$:430 - [Epoch 2 416/5216][Iteration 176][Wall Clock 1215.066628159s] Trained 32.0 records in 6.733480487 seconds. Throughput is 4.7523713 records/second. Loss is 0.11933737. 22-02-10 22:07:46 [Thread-4] INFO DistriOptimizer$:430 - [Epoch 2 448/5216][Iteration 177][Wall Clock 1221.758365541s] Trained 32.0 records in 6.691737382 seconds. Throughput is 4.7820168 records/second. Loss is 0.031336. 22-02-10 22:07:53 [Thread-4] INFO DistriOptimizer$:430 - [Epoch 2 480/5216][Iteration 178][Wall Clock 1228.463282302s] Trained 32.0 records in 6.704916761 seconds. Throughput is 4.772617 records/second. Loss is 0.1301704. 22-02-10 22:07:59 [Thread-4] INFO DistriOptimizer$:430 - [Epoch 2 512/5216][Iteration 179][Wall Clock 1235.242561932s] Trained 32.0 records in 6.77927963 seconds. Throughput is 4.7202654 records/second. Loss is 0.051530637. 22-02-10 22:08:06 [Thread-4] INFO DistriOptimizer$:430 - [Epoch 2 544/5216][Iteration 180][Wall Clock 1241.965783131s] Trained 32.0 records in 6.723221199 seconds. Throughput is 4.759623 records/second. Loss is 0.04075704. 22-02-10 22:08:13 [Thread-4] INFO DistriOptimizer$:430 - [Epoch 2 576/5216][Iteration 181][Wall Clock 1248.669990151s] Trained 32.0 records in 6.70420702 seconds. Throughput is 4.7731223 records/second. Loss is 0.03940622. 22-02-10 22:08:20 [Thread-4] INFO DistriOptimizer$:430 - [Epoch 2 608/5216][Iteration 182][Wall Clock 1255.372045812s] Trained 32.0 records in 6.702055661 seconds. Throughput is 4.7746544 records/second. Loss is 0.091302186. 22-02-10 22:08:26 [Thread-4] INFO DistriOptimizer$:430 - [Epoch 2 640/5216][Iteration 183][Wall Clock 1262.007832872s] Trained 32.0 records in 6.63578706 seconds. Throughput is 4.8223367 records/second. Loss is 0.06382078. 22-02-10 22:08:33 [Thread-4] INFO DistriOptimizer$:430 - [Epoch 2 672/5216][Iteration 184][Wall Clock 1268.645879471s] Trained 32.0 records in 6.638046599 seconds. Throughput is 4.8206954 records/second. Loss is 0.03389951. 22-02-10 22:08:40 [Thread-4] INFO DistriOptimizer$:430 - [Epoch 2 704/5216][Iteration 185][Wall Clock 1275.32009067s] Trained 32.0 records in 6.674211199 seconds. Throughput is 4.7945743 records/second. Loss is 0.16956674. 22-02-10 22:08:46 [Thread-4] INFO DistriOptimizer$:430 - [Epoch 2 736/5216][Iteration 186][Wall Clock 1282.052184831s] Trained 32.0 records in 6.732094161 seconds. Throughput is 4.75335 records/second. Loss is 0.07228024. 22-02-10 22:08:53 [Thread-4] INFO DistriOptimizer$:430 - [Epoch 2 768/5216][Iteration 187][Wall Clock 1288.712290281s] Trained 32.0 records in 6.66010545 seconds. Throughput is 4.804729 records/second. Loss is 0.055083573. 22-02-10 22:09:00 [Thread-4] INFO DistriOptimizer$:430 - [Epoch 2 800/5216][Iteration 188][Wall Clock 1295.493118066s] Trained 32.0 records in 6.780827785 seconds. Throughput is 4.7191877 records/second. Loss is 0.029722702. 22-02-10 22:09:06 [Thread-4] INFO DistriOptimizer$:430 - [Epoch 2 832/5216][Iteration 189][Wall Clock 1302.252409233s] Trained 32.0 records in 6.759291167 seconds. Throughput is 4.7342243 records/second. Loss is 0.03207023. 22-02-10 22:09:13 [Thread-4] INFO DistriOptimizer$:430 - [Epoch 2 864/5216][Iteration 190][Wall Clock 1309.01838384s] Trained 32.0 records in 6.765974607 seconds. Throughput is 4.729548 records/second. Loss is 0.039159685. 22-02-10 22:09:20 [Thread-4] INFO DistriOptimizer$:430 - [Epoch 2 896/5216][Iteration 191][Wall Clock 1315.75699324s] Trained 32.0 records in 6.7386094 seconds. Throughput is 4.7487545 records/second. Loss is 0.067178294. 22-02-10 22:09:27 [Thread-4] INFO DistriOptimizer$:430 - [Epoch 2 928/5216][Iteration 192][Wall Clock 1322.537886275s] Trained 32.0 records in 6.780893035 seconds. Throughput is 4.7191424 records/second. Loss is 0.032825556. 22-02-10 22:09:34 [Thread-4] INFO DistriOptimizer$:430 - [Epoch 2 960/5216][Iteration 193][Wall Clock 1329.310162991s] Trained 32.0 records in 6.772276716 seconds. Throughput is 4.7251463 records/second. Loss is 0.15694778. 22-02-10 22:09:40 [Thread-4] INFO DistriOptimizer$:430 - [Epoch 2 992/5216][Iteration 194][Wall Clock 1335.966928802s] Trained 32.0 records in 6.656765811 seconds. Throughput is 4.807139 records/second. Loss is 0.055101987. 22-02-10 22:09:47 [Thread-4] INFO DistriOptimizer$:430 - [Epoch 2 1024/5216][Iteration 195][Wall Clock 1342.712421841s] Trained 32.0 records in 6.745493039 seconds. Throughput is 4.7439084 records/second. Loss is 0.13609813. 22-02-10 22:09:54 [Thread-4] INFO DistriOptimizer$:430 - [Epoch 2 1056/5216][Iteration 196][Wall Clock 1349.500486955s] Trained 32.0 records in 6.788065114 seconds. Throughput is 4.7141566 records/second. Loss is 0.0583543. 22-02-10 22:10:00 [Thread-4] INFO DistriOptimizer$:430 - [Epoch 2 1088/5216][Iteration 197][Wall Clock 1356.238547285s] Trained 32.0 records in 6.73806033 seconds. Throughput is 4.749141 records/second. Loss is 0.029106714. 22-02-10 22:10:07 [Thread-4] INFO DistriOptimizer$:430 - [Epoch 2 1120/5216][Iteration 198][Wall Clock 1362.892775037s] Trained 32.0 records in 6.654227752 seconds. Throughput is 4.808973 records/second. Loss is 0.035754457. 22-02-10 22:10:14 [Thread-4] INFO DistriOptimizer$:430 - [Epoch 2 1152/5216][Iteration 199][Wall Clock 1369.656477513s] Trained 32.0 records in 6.763702476 seconds. Throughput is 4.731137 records/second. Loss is 0.046034694. 22-02-10 22:10:21 [Thread-4] INFO DistriOptimizer$:430 - [Epoch 2 1184/5216][Iteration 200][Wall Clock 1376.377429796s] Trained 32.0 records in 6.720952283 seconds. Throughput is 4.76123 records/second. Loss is 0.03228614. 22-02-10 22:10:27 [Thread-4] INFO DistriOptimizer$:430 - [Epoch 2 1216/5216][Iteration 201][Wall Clock 1383.150826202s] Trained 32.0 records in 6.773396406 seconds. Throughput is 4.724365 records/second. Loss is 0.043486226. 22-02-10 22:10:34 [Thread-4] INFO DistriOptimizer$:430 - [Epoch 2 1248/5216][Iteration 202][Wall Clock 1389.824220951s] Trained 32.0 records in 6.673394749 seconds. Throughput is 4.795161 records/second. Loss is 0.049988642. 22-02-10 22:10:41 [Thread-4] INFO DistriOptimizer$:430 - [Epoch 2 1280/5216][Iteration 203][Wall Clock 1396.582125549s] Trained 32.0 records in 6.757904598 seconds. Throughput is 4.7351956 records/second. Loss is 0.05529841. 22-02-10 22:10:48 [Thread-4] INFO DistriOptimizer$:430 - [Epoch 2 1312/5216][Iteration 204][Wall Clock 1403.31175064s] Trained 32.0 records in 6.729625091 seconds. Throughput is 4.755094 records/second. Loss is 0.09950498. 22-02-10 22:10:54 [Thread-4] INFO DistriOptimizer$:430 - [Epoch 2 1344/5216][Iteration 205][Wall Clock 1410.081862016s] Trained 32.0 records in 6.770111376 seconds. Throughput is 4.726658 records/second. Loss is 0.048566222. 22-02-10 22:11:01 [Thread-4] INFO DistriOptimizer$:430 - [Epoch 2 1376/5216][Iteration 206][Wall Clock 1416.79208471s] Trained 32.0 records in 6.710222694 seconds. Throughput is 4.768843 records/second. Loss is 0.1152859. 22-02-10 22:11:08 [Thread-4] INFO DistriOptimizer$:430 - [Epoch 2 1408/5216][Iteration 207][Wall Clock 1423.502216374s] Trained 32.0 records in 6.710131664 seconds. Throughput is 4.768908 records/second. Loss is 0.2416677. 22-02-10 22:11:14 [Thread-4] INFO DistriOptimizer$:430 - [Epoch 2 1440/5216][Iteration 208][Wall Clock 1430.213018097s] Trained 32.0 records in 6.710801723 seconds. Throughput is 4.7684317 records/second. Loss is 0.06569219. 22-02-10 22:11:21 [Thread-4] INFO DistriOptimizer$:430 - [Epoch 2 1472/5216][Iteration 209][Wall Clock 1436.888735726s] Trained 32.0 records in 6.675717629 seconds. Throughput is 4.793492 records/second. Loss is 0.055820663. 22-02-10 22:11:28 [Thread-4] INFO DistriOptimizer$:430 - [Epoch 2 1504/5216][Iteration 210][Wall Clock 1443.623574487s] Trained 32.0 records in 6.734838761 seconds. Throughput is 4.751413 records/second. Loss is 0.03864559. 22-02-10 22:11:35 [Thread-4] INFO DistriOptimizer$:430 - [Epoch 2 1536/5216][Iteration 211][Wall Clock 1450.404357542s] Trained 32.0 records in 6.780783055 seconds. Throughput is 4.7192187 records/second. Loss is 0.060453054. 22-02-10 22:11:41 [Thread-4] INFO DistriOptimizer$:430 - [Epoch 2 1568/5216][Iteration 212][Wall Clock 1457.051590934s] Trained 32.0 records in 6.647233392 seconds. Throughput is 4.814033 records/second. Loss is 0.05805398. 22-02-10 22:11:48 [Thread-4] INFO DistriOptimizer$:430 - [Epoch 2 1600/5216][Iteration 213][Wall Clock 1463.773017707s] Trained 32.0 records in 6.721426773 seconds. Throughput is 4.760894 records/second. Loss is 0.055497084. 22-02-10 22:11:55 [Thread-4] INFO DistriOptimizer$:430 - [Epoch 2 1632/5216][Iteration 214][Wall Clock 1470.516832106s] Trained 32.0 records in 6.743814399 seconds. Throughput is 4.745089 records/second. Loss is 0.038889542. 22-02-10 22:12:02 [Thread-4] INFO DistriOptimizer$:430 - [Epoch 2 1664/5216][Iteration 215][Wall Clock 1477.38424505s] Trained 32.0 records in 6.867412944 seconds. Throughput is 4.6596875 records/second. Loss is 0.03774552. 22-02-10 22:12:08 [Thread-4] INFO DistriOptimizer$:430 - [Epoch 2 1696/5216][Iteration 216][Wall Clock 1484.150667477s] Trained 32.0 records in 6.766422427 seconds. Throughput is 4.7292347 records/second. Loss is 0.046205066. 22-02-10 22:12:15 [Thread-4] INFO DistriOptimizer$:430 - [Epoch 2 1728/5216][Iteration 217][Wall Clock 1490.811895297s] Trained 32.0 records in 6.66122782 seconds. Throughput is 4.8039193 records/second. Loss is 0.0330094. 22-02-10 22:12:22 [Thread-4] INFO DistriOptimizer$:430 - [Epoch 2 1760/5216][Iteration 218][Wall Clock 1497.579142284s] Trained 32.0 records in 6.767246987 seconds. Throughput is 4.728658 records/second. Loss is 0.061001375. 22-02-10 22:12:29 [Thread-4] INFO DistriOptimizer$:430 - [Epoch 2 1792/5216][Iteration 219][Wall Clock 1504.303488896s] Trained 32.0 records in 6.724346612 seconds. Throughput is 4.7588267 records/second. Loss is 0.027015787. 22-02-10 22:12:35 [Thread-4] INFO DistriOptimizer$:430 - [Epoch 2 1824/5216][Iteration 220][Wall Clock 1510.978417974s] Trained 32.0 records in 6.674929078 seconds. Throughput is 4.7940583 records/second. Loss is 0.12283087. 22-02-10 22:12:42 [Thread-4] INFO DistriOptimizer$:430 - [Epoch 2 1856/5216][Iteration 221][Wall Clock 1517.677838539s] Trained 32.0 records in 6.699420565 seconds. Throughput is 4.7765326 records/second. Loss is 0.09080452. 22-02-10 22:12:49 [Thread-4] INFO DistriOptimizer$:430 - [Epoch 2 1888/5216][Iteration 222][Wall Clock 1524.283433417s] Trained 32.0 records in 6.605594878 seconds. Throughput is 4.844378 records/second. Loss is 0.02359111. 22-02-10 22:12:55 [Thread-4] INFO DistriOptimizer$:430 - [Epoch 2 1920/5216][Iteration 223][Wall Clock 1530.965340204s] Trained 32.0 records in 6.681906787 seconds. Throughput is 4.789052 records/second. Loss is 0.028524905. 22-02-10 22:13:02 [Thread-4] INFO DistriOptimizer$:430 - [Epoch 2 1952/5216][Iteration 224][Wall Clock 1537.708045814s] Trained 32.0 records in 6.74270561 seconds. Throughput is 4.745869 records/second. Loss is 0.17805855. 22-02-10 22:13:09 [Thread-4] INFO DistriOptimizer$:430 - [Epoch 2 1984/5216][Iteration 225][Wall Clock 1544.454680503s] Trained 32.0 records in 6.746634689 seconds. Throughput is 4.7431054 records/second. Loss is 0.24332428. 22-02-10 22:13:15 [Thread-4] INFO DistriOptimizer$:430 - [Epoch 2 2016/5216][Iteration 226][Wall Clock 1551.126635792s] Trained 32.0 records in 6.671955289 seconds. Throughput is 4.7961955 records/second. Loss is 0.039940715. 22-02-10 22:13:22 [Thread-4] INFO DistriOptimizer$:430 - [Epoch 2 2048/5216][Iteration 227][Wall Clock 1557.847741925s] Trained 32.0 records in 6.721106133 seconds. Throughput is 4.7611213 records/second. Loss is 0.019942332. 22-02-10 22:13:29 [Thread-4] INFO DistriOptimizer$:430 - [Epoch 2 2080/5216][Iteration 228][Wall Clock 1564.520986414s] Trained 32.0 records in 6.673244489 seconds. Throughput is 4.7952685 records/second. Loss is 0.026350595. 22-02-10 22:13:36 [Thread-4] INFO DistriOptimizer$:430 - [Epoch 2 2112/5216][Iteration 229][Wall Clock 1571.379855649s] Trained 32.0 records in 6.858869235 seconds. Throughput is 4.665492 records/second. Loss is 0.0607169. 22-02-10 22:13:42 [Thread-4] INFO DistriOptimizer$:430 - [Epoch 2 2144/5216][Iteration 230][Wall Clock 1578.193939739s] Trained 32.0 records in 6.81408409 seconds. Throughput is 4.6961555 records/second. Loss is 0.18790375. 22-02-10 22:13:49 [Thread-4] INFO DistriOptimizer$:430 - [Epoch 2 2176/5216][Iteration 231][Wall Clock 1584.888451215s] Trained 32.0 records in 6.694511476 seconds. Throughput is 4.780035 records/second. Loss is 0.06759616. 22-02-10 22:13:56 [Thread-4] INFO DistriOptimizer$:430 - [Epoch 2 2208/5216][Iteration 232][Wall Clock 1591.600821358s] Trained 32.0 records in 6.712370143 seconds. Throughput is 4.767318 records/second. Loss is 0.052260593. 22-02-10 22:14:03 [Thread-4] INFO DistriOptimizer$:430 - [Epoch 2 2240/5216][Iteration 233][Wall Clock 1598.346263438s] Trained 32.0 records in 6.74544208 seconds. Throughput is 4.743944 records/second. Loss is 0.052328777. 22-02-10 22:14:09 [Thread-4] INFO DistriOptimizer$:430 - [Epoch 2 2272/5216][Iteration 234][Wall Clock 1605.06733383s] Trained 32.0 records in 6.721070392 seconds. Throughput is 4.7611465 records/second. Loss is 0.028220266. 22-02-10 22:14:16 [Thread-4] INFO DistriOptimizer$:430 - [Epoch 2 2304/5216][Iteration 235][Wall Clock 1611.788666362s] Trained 32.0 records in 6.721332532 seconds. Throughput is 4.7609606 records/second. Loss is 0.060379624. 22-02-10 22:14:23 [Thread-4] INFO DistriOptimizer$:430 - [Epoch 2 2336/5216][Iteration 236][Wall Clock 1618.415896087s] Trained 32.0 records in 6.627229725 seconds. Throughput is 4.8285637 records/second. Loss is 0.24005209. 22-02-10 22:14:29 [Thread-4] INFO DistriOptimizer$:430 - [Epoch 2 2368/5216][Iteration 237][Wall Clock 1625.153758887s] Trained 32.0 records in 6.7378628 seconds. Throughput is 4.7492805 records/second. Loss is 0.13914071. 22-02-10 22:14:36 [Thread-4] INFO DistriOptimizer$:430 - [Epoch 2 2400/5216][Iteration 238][Wall Clock 1631.829330105s] Trained 32.0 records in 6.675571218 seconds. Throughput is 4.793597 records/second. Loss is 0.04872594. 22-02-10 22:14:43 [Thread-4] INFO DistriOptimizer$:430 - [Epoch 2 2432/5216][Iteration 239][Wall Clock 1638.569506515s] Trained 32.0 records in 6.74017641 seconds. Throughput is 4.74765 records/second. Loss is 0.09358146. 22-02-10 22:14:49 [Thread-4] INFO DistriOptimizer$:430 - [Epoch 2 2464/5216][Iteration 240][Wall Clock 1645.246258753s] Trained 32.0 records in 6.676752238 seconds. Throughput is 4.7927494 records/second. Loss is 0.03792737. 22-02-10 22:14:56 [Thread-4] INFO DistriOptimizer$:430 - [Epoch 2 2496/5216][Iteration 241][Wall Clock 1651.987828703s] Trained 32.0 records in 6.74156995 seconds. Throughput is 4.746669 records/second. Loss is 0.039696902. 22-02-10 22:15:03 [Thread-4] INFO DistriOptimizer$:430 - [Epoch 2 2528/5216][Iteration 242][Wall Clock 1658.684211539s] Trained 32.0 records in 6.696382836 seconds. Throughput is 4.7786994 records/second. Loss is 0.028509516. 22-02-10 22:15:10 [Thread-4] INFO DistriOptimizer$:430 - [Epoch 2 2560/5216][Iteration 243][Wall Clock 1665.350753539s] Trained 32.0 records in 6.666542 seconds. Throughput is 4.80009 records/second. Loss is 0.054182403. 22-02-10 22:15:16 [Thread-4] INFO DistriOptimizer$:430 - [Epoch 2 2592/5216][Iteration 244][Wall Clock 1672.098503858s] Trained 32.0 records in 6.747750319 seconds. Throughput is 4.7423215 records/second. Loss is 0.030029643. 22-02-10 22:15:23 [Thread-4] INFO DistriOptimizer$:430 - [Epoch 2 2624/5216][Iteration 245][Wall Clock 1678.801408993s] Trained 32.0 records in 6.702905135 seconds. Throughput is 4.7740493 records/second. Loss is 0.04768407. 22-02-10 22:15:30 [Thread-4] INFO DistriOptimizer$:430 - [Epoch 2 2656/5216][Iteration 246][Wall Clock 1685.552404891s] Trained 32.0 records in 6.750995898 seconds. Throughput is 4.7400413 records/second. Loss is 0.03326167. 22-02-10 22:15:37 [Thread-4] INFO DistriOptimizer$:430 - [Epoch 2 2688/5216][Iteration 247][Wall Clock 1692.287650001s] Trained 32.0 records in 6.73524511 seconds. Throughput is 4.7511263 records/second. Loss is 0.035007305. 22-02-10 22:15:43 [Thread-4] INFO DistriOptimizer$:430 - [Epoch 2 2720/5216][Iteration 248][Wall Clock 1698.974355358s] Trained 32.0 records in 6.686705357 seconds. Throughput is 4.7856154 records/second. Loss is 0.08013898. 22-02-10 22:15:50 [Thread-4] INFO DistriOptimizer$:430 - [Epoch 2 2752/5216][Iteration 249][Wall Clock 1705.738966956s] Trained 32.0 records in 6.764611598 seconds. Throughput is 4.7305007 records/second. Loss is 0.13905539. 22-02-10 22:15:57 [Thread-4] INFO DistriOptimizer$:430 - [Epoch 2 2784/5216][Iteration 250][Wall Clock 1712.434349602s] Trained 32.0 records in 6.695382646 seconds. Throughput is 4.779413 records/second. Loss is 0.026021753. 22-02-10 22:16:03 [Thread-4] INFO DistriOptimizer$:430 - [Epoch 2 2816/5216][Iteration 251][Wall Clock 1719.180325062s] Trained 32.0 records in 6.74597546 seconds. Throughput is 4.743569 records/second. Loss is 0.1869105. 22-02-10 22:16:10 [Thread-4] INFO DistriOptimizer$:430 - [Epoch 2 2848/5216][Iteration 252][Wall Clock 1725.854752332s] Trained 32.0 records in 6.67442727 seconds. Throughput is 4.794419 records/second. Loss is 0.024402078. 22-02-10 22:16:17 [Thread-4] INFO DistriOptimizer$:430 - [Epoch 2 2880/5216][Iteration 253][Wall Clock 1732.560972607s] Trained 32.0 records in 6.706220275 seconds. Throughput is 4.7716894 records/second. Loss is 0.034902662. 22-02-10 22:16:24 [Thread-4] INFO DistriOptimizer$:430 - [Epoch 2 2912/5216][Iteration 254][Wall Clock 1739.2724565s] Trained 32.0 records in 6.711483893 seconds. Throughput is 4.767947 records/second. Loss is 0.06654404. 22-02-10 22:16:30 [Thread-4] INFO DistriOptimizer$:430 - [Epoch 2 2944/5216][Iteration 255][Wall Clock 1745.940964461s] Trained 32.0 records in 6.668507961 seconds. Throughput is 4.7986746 records/second. Loss is 0.14967342. 22-02-10 22:16:37 [Thread-4] INFO DistriOptimizer$:430 - [Epoch 2 2976/5216][Iteration 256][Wall Clock 1752.581416815s] Trained 32.0 records in 6.640452354 seconds. Throughput is 4.8189487 records/second. Loss is 0.083600916. 22-02-10 22:16:44 [Thread-4] INFO DistriOptimizer$:430 - [Epoch 2 3008/5216][Iteration 257][Wall Clock 1759.306806007s] Trained 32.0 records in 6.725389192 seconds. Throughput is 4.758089 records/second. Loss is 0.079755664. 22-02-10 22:16:50 [Thread-4] INFO DistriOptimizer$:430 - [Epoch 2 3040/5216][Iteration 258][Wall Clock 1765.974021237s] Trained 32.0 records in 6.66721523 seconds. Throughput is 4.799605 records/second. Loss is 0.06145911. 22-02-10 22:16:57 [Thread-4] INFO DistriOptimizer$:430 - [Epoch 2 3072/5216][Iteration 259][Wall Clock 1772.658226354s] Trained 32.0 records in 6.684205117 seconds. Throughput is 4.7874055 records/second. Loss is 0.014912148. 22-02-10 22:17:04 [Thread-4] INFO DistriOptimizer$:430 - [Epoch 2 3104/5216][Iteration 260][Wall Clock 1779.508033579s] Trained 32.0 records in 6.849807225 seconds. Throughput is 4.671664 records/second. Loss is 0.03870481. 22-02-10 22:17:10 [Thread-4] INFO DistriOptimizer$:430 - [Epoch 2 3136/5216][Iteration 261][Wall Clock 1786.182107258s] Trained 32.0 records in 6.674073679 seconds. Throughput is 4.794673 records/second. Loss is 0.03249783. 22-02-10 22:17:17 [Thread-4] INFO DistriOptimizer$:430 - [Epoch 2 3168/5216][Iteration 262][Wall Clock 1793.046367602s] Trained 32.0 records in 6.864260344 seconds. Throughput is 4.661828 records/second. Loss is 0.23055264. 22-02-10 22:17:24 [Thread-4] INFO DistriOptimizer$:430 - [Epoch 2 3200/5216][Iteration 263][Wall Clock 1799.843235054s] Trained 32.0 records in 6.796867452 seconds. Throughput is 4.708051 records/second. Loss is 0.032320842. 22-02-10 22:17:31 [Thread-4] INFO DistriOptimizer$:430 - [Epoch 2 3232/5216][Iteration 264][Wall Clock 1806.503550395s] Trained 32.0 records in 6.660315341 seconds. Throughput is 4.8045774 records/second. Loss is 0.08232775. 22-02-10 22:17:38 [Thread-4] INFO DistriOptimizer$:430 - [Epoch 2 3264/5216][Iteration 265][Wall Clock 1813.264386102s] Trained 32.0 records in 6.760835707 seconds. Throughput is 4.733143 records/second. Loss is 0.024289446. 22-02-10 22:17:44 [Thread-4] INFO DistriOptimizer$:430 - [Epoch 2 3296/5216][Iteration 266][Wall Clock 1820.02098112s] Trained 32.0 records in 6.756595018 seconds. Throughput is 4.736113 records/second. Loss is 0.053901657. 22-02-10 22:17:51 [Thread-4] INFO DistriOptimizer$:430 - [Epoch 2 3328/5216][Iteration 267][Wall Clock 1826.729380544s] Trained 32.0 records in 6.708399424 seconds. Throughput is 4.770139 records/second. Loss is 0.05734954. 22-02-10 22:17:58 [Thread-4] INFO DistriOptimizer$:430 - [Epoch 2 3360/5216][Iteration 268][Wall Clock 1833.451298696s] Trained 32.0 records in 6.721918152 seconds. Throughput is 4.7605457 records/second. Loss is 0.026738394. 22-02-10 22:18:05 [Thread-4] INFO DistriOptimizer$:430 - [Epoch 2 3392/5216][Iteration 269][Wall Clock 1840.247661149s] Trained 32.0 records in 6.796362453 seconds. Throughput is 4.708401 records/second. Loss is 0.04109376. 22-02-10 22:18:11 [Thread-4] INFO DistriOptimizer$:430 - [Epoch 2 3424/5216][Iteration 270][Wall Clock 1846.889504972s] Trained 32.0 records in 6.641843823 seconds. Throughput is 4.8179393 records/second. Loss is 0.045827955. 22-02-10 22:18:18 [Thread-4] INFO DistriOptimizer$:430 - [Epoch 2 3456/5216][Iteration 271][Wall Clock 1853.618465633s] Trained 32.0 records in 6.728960661 seconds. Throughput is 4.7555637 records/second. Loss is 0.09872858. 22-02-10 22:18:25 [Thread-4] INFO DistriOptimizer$:430 - [Epoch 2 3488/5216][Iteration 272][Wall Clock 1860.30499014s] Trained 32.0 records in 6.686524507 seconds. Throughput is 4.7857447 records/second. Loss is 0.0303055. 22-02-10 22:18:31 [Thread-4] INFO DistriOptimizer$:430 - [Epoch 2 3520/5216][Iteration 273][Wall Clock 1867.093860784s] Trained 32.0 records in 6.788870644 seconds. Throughput is 4.713597 records/second. Loss is 0.016503073. 22-02-10 22:18:38 [Thread-4] INFO DistriOptimizer$:430 - [Epoch 2 3552/5216][Iteration 274][Wall Clock 1873.866666529s] Trained 32.0 records in 6.772805745 seconds. Throughput is 4.7247777 records/second. Loss is 0.019285396. 22-02-10 22:18:45 [Thread-4] INFO DistriOptimizer$:430 - [Epoch 2 3584/5216][Iteration 275][Wall Clock 1880.620021597s] Trained 32.0 records in 6.753355068 seconds. Throughput is 4.7383857 records/second. Loss is 0.034037814. 22-02-10 22:18:52 [Thread-4] INFO DistriOptimizer$:430 - [Epoch 2 3616/5216][Iteration 276][Wall Clock 1887.268651739s] Trained 32.0 records in 6.648630142 seconds. Throughput is 4.8130217 records/second. Loss is 0.026287517. 22-02-10 22:18:58 [Thread-4] INFO DistriOptimizer$:430 - [Epoch 2 3648/5216][Iteration 277][Wall Clock 1894.016612318s] Trained 32.0 records in 6.747960579 seconds. Throughput is 4.7421737 records/second. Loss is 0.028653044. 22-02-10 22:19:05 [Thread-4] INFO DistriOptimizer$:430 - [Epoch 2 3680/5216][Iteration 278][Wall Clock 1900.791093414s] Trained 32.0 records in 6.774481096 seconds. Throughput is 4.723609 records/second. Loss is 0.061908815. 22-02-10 22:19:12 [Thread-4] INFO DistriOptimizer$:430 - [Epoch 2 3712/5216][Iteration 279][Wall Clock 1907.442058806s] Trained 32.0 records in 6.650965392 seconds. Throughput is 4.8113317 records/second. Loss is 0.16787758. 22-02-10 22:19:18 [Thread-4] INFO DistriOptimizer$:430 - [Epoch 2 3744/5216][Iteration 280][Wall Clock 1914.165572288s] Trained 32.0 records in 6.723513482 seconds. Throughput is 4.759416 records/second. Loss is 0.019417007. 22-02-10 22:19:25 [Thread-4] INFO DistriOptimizer$:430 - [Epoch 2 3776/5216][Iteration 281][Wall Clock 1920.843786956s] Trained 32.0 records in 6.678214668 seconds. Throughput is 4.7917 records/second. Loss is 0.025936026. 22-02-10 22:19:32 [Thread-4] INFO DistriOptimizer$:430 - [Epoch 2 3808/5216][Iteration 282][Wall Clock 1927.517299105s] Trained 32.0 records in 6.673512149 seconds. Throughput is 4.7950764 records/second. Loss is 0.023232222. 22-02-10 22:19:38 [Thread-4] INFO DistriOptimizer$:430 - [Epoch 2 3840/5216][Iteration 283][Wall Clock 1934.194099863s] Trained 32.0 records in 6.676800758 seconds. Throughput is 4.7927146 records/second. Loss is 0.0252783. 22-02-10 22:19:45 [Thread-4] INFO DistriOptimizer$:430 - [Epoch 2 3872/5216][Iteration 284][Wall Clock 1940.886500089s] Trained 32.0 records in 6.692400226 seconds. Throughput is 4.781543 records/second. Loss is 0.044139814. 22-02-10 22:19:52 [Thread-4] INFO DistriOptimizer$:430 - [Epoch 2 3904/5216][Iteration 285][Wall Clock 1947.580761725s] Trained 32.0 records in 6.694261636 seconds. Throughput is 4.7802134 records/second. Loss is 0.018998697. 22-02-10 22:19:59 [Thread-4] INFO DistriOptimizer$:430 - [Epoch 2 3936/5216][Iteration 286][Wall Clock 1954.328206394s] Trained 32.0 records in 6.747444669 seconds. Throughput is 4.742536 records/second. Loss is 0.013950298. 22-02-10 22:20:05 [Thread-4] INFO DistriOptimizer$:430 - [Epoch 2 3968/5216][Iteration 287][Wall Clock 1961.031168589s] Trained 32.0 records in 6.702962195 seconds. Throughput is 4.7740088 records/second. Loss is 0.0382402. 22-02-10 22:20:12 [Thread-4] INFO DistriOptimizer$:430 - [Epoch 2 4000/5216][Iteration 288][Wall Clock 1967.754647461s] Trained 32.0 records in 6.723478872 seconds. Throughput is 4.759441 records/second. Loss is 0.029997641. 22-02-10 22:20:19 [Thread-4] INFO DistriOptimizer$:430 - [Epoch 2 4032/5216][Iteration 289][Wall Clock 1974.355268539s] Trained 32.0 records in 6.600621078 seconds. Throughput is 4.8480287 records/second. Loss is 0.020224579. 22-02-10 22:20:25 [Thread-4] INFO DistriOptimizer$:430 - [Epoch 2 4064/5216][Iteration 290][Wall Clock 1981.075092512s] Trained 32.0 records in 6.719823973 seconds. Throughput is 4.7620296 records/second. Loss is 0.022528406. 22-02-10 22:20:32 [Thread-4] INFO DistriOptimizer$:430 - [Epoch 2 4096/5216][Iteration 291][Wall Clock 1987.790228325s] Trained 32.0 records in 6.715135813 seconds. Throughput is 4.765354 records/second. Loss is 0.029973663. 22-02-10 22:20:39 [Thread-4] INFO DistriOptimizer$:430 - [Epoch 2 4128/5216][Iteration 292][Wall Clock 1994.477276032s] Trained 32.0 records in 6.687047707 seconds. Throughput is 4.7853703 records/second. Loss is 0.020508267. 22-02-10 22:20:45 [Thread-4] INFO DistriOptimizer$:430 - [Epoch 2 4160/5216][Iteration 293][Wall Clock 2001.184920536s] Trained 32.0 records in 6.707644504 seconds. Throughput is 4.770676 records/second. Loss is 0.018784666. 22-02-10 22:20:52 [Thread-4] INFO DistriOptimizer$:430 - [Epoch 2 4192/5216][Iteration 294][Wall Clock 2007.89370223s] Trained 32.0 records in 6.708781694 seconds. Throughput is 4.7698674 records/second. Loss is 0.025164114. 22-02-10 22:20:59 [Thread-4] INFO DistriOptimizer$:430 - [Epoch 2 4224/5216][Iteration 295][Wall Clock 2014.624621431s] Trained 32.0 records in 6.730919201 seconds. Throughput is 4.7541795 records/second. Loss is 0.019716263. 22-02-10 22:21:06 [Thread-4] INFO DistriOptimizer$:430 - [Epoch 2 4256/5216][Iteration 296][Wall Clock 2021.368797681s] Trained 32.0 records in 6.74417625 seconds. Throughput is 4.7448344 records/second. Loss is 0.023273723. 22-02-10 22:21:12 [Thread-4] INFO DistriOptimizer$:430 - [Epoch 2 4288/5216][Iteration 297][Wall Clock 2028.108449761s] Trained 32.0 records in 6.73965208 seconds. Throughput is 4.7480197 records/second. Loss is 0.044285476. 22-02-10 22:21:19 [Thread-4] INFO DistriOptimizer$:430 - [Epoch 2 4320/5216][Iteration 298][Wall Clock 2034.78523545s] Trained 32.0 records in 6.676785689 seconds. Throughput is 4.7927256 records/second. Loss is 0.015211448. 22-02-10 22:21:26 [Thread-4] INFO DistriOptimizer$:430 - [Epoch 2 4352/5216][Iteration 299][Wall Clock 2041.479516216s] Trained 32.0 records in 6.694280766 seconds. Throughput is 4.7802 records/second. Loss is 0.07407382. 22-02-10 22:21:33 [Thread-4] INFO DistriOptimizer$:430 - [Epoch 2 4384/5216][Iteration 300][Wall Clock 2048.211063737s] Trained 32.0 records in 6.731547521 seconds. Throughput is 4.753736 records/second. Loss is 0.071602456. 22-02-10 22:21:39 [Thread-4] INFO DistriOptimizer$:430 - [Epoch 2 4416/5216][Iteration 301][Wall Clock 2054.953166037s] Trained 32.0 records in 6.7421023 seconds. Throughput is 4.746294 records/second. Loss is 0.030040426. 22-02-10 22:21:46 [Thread-4] INFO DistriOptimizer$:430 - [Epoch 2 4448/5216][Iteration 302][Wall Clock 2061.652367832s] Trained 32.0 records in 6.699201795 seconds. Throughput is 4.7766886 records/second. Loss is 0.014070113. 22-02-10 22:21:53 [Thread-4] INFO DistriOptimizer$:430 - [Epoch 2 4480/5216][Iteration 303][Wall Clock 2068.366014095s] Trained 32.0 records in 6.713646263 seconds. Throughput is 4.7664113 records/second. Loss is 0.0331312. 22-02-10 22:21:59 [Thread-4] INFO DistriOptimizer$:430 - [Epoch 2 4512/5216][Iteration 304][Wall Clock 2075.062861121s] Trained 32.0 records in 6.696847026 seconds. Throughput is 4.778368 records/second. Loss is 0.2497082. 22-02-10 22:22:06 [Thread-4] INFO DistriOptimizer$:430 - [Epoch 2 4544/5216][Iteration 305][Wall Clock 2081.80987463s] Trained 32.0 records in 6.747013509 seconds. Throughput is 4.7428393 records/second. Loss is 0.012969022. 22-02-10 22:22:13 [Thread-4] INFO DistriOptimizer$:430 - [Epoch 2 4576/5216][Iteration 306][Wall Clock 2088.511788305s] Trained 32.0 records in 6.701913675 seconds. Throughput is 4.7747555 records/second. Loss is 0.01994078. 22-02-10 22:22:20 [Thread-4] INFO DistriOptimizer$:430 - [Epoch 2 4608/5216][Iteration 307][Wall Clock 2095.261551654s] Trained 32.0 records in 6.749763349 seconds. Throughput is 4.7409067 records/second. Loss is 0.01698431. 22-02-10 22:22:26 [Thread-4] INFO DistriOptimizer$:430 - [Epoch 2 4640/5216][Iteration 308][Wall Clock 2101.914542336s] Trained 32.0 records in 6.652990682 seconds. Throughput is 4.809867 records/second. Loss is 0.104677595. 22-02-10 22:22:33 [Thread-4] INFO DistriOptimizer$:430 - [Epoch 2 4672/5216][Iteration 309][Wall Clock 2108.741846795s] Trained 32.0 records in 6.827304459 seconds. Throughput is 4.6870623 records/second. Loss is 0.084604755. 22-02-10 22:22:40 [Thread-4] INFO DistriOptimizer$:430 - [Epoch 2 4704/5216][Iteration 310][Wall Clock 2115.429156291s] Trained 32.0 records in 6.687309496 seconds. Throughput is 4.7851834 records/second. Loss is 0.020980034. 22-02-10 22:22:46 [Thread-4] INFO DistriOptimizer$:430 - [Epoch 2 4736/5216][Iteration 311][Wall Clock 2122.120532287s] Trained 32.0 records in 6.691375996 seconds. Throughput is 4.7822747 records/second. Loss is 0.02066752. 22-02-10 22:22:53 [Thread-4] INFO DistriOptimizer$:430 - [Epoch 2 4768/5216][Iteration 312][Wall Clock 2128.860778147s] Trained 32.0 records in 6.74024586 seconds. Throughput is 4.7476015 records/second. Loss is 0.011349203. 22-02-10 22:23:00 [Thread-4] INFO DistriOptimizer$:430 - [Epoch 2 4800/5216][Iteration 313][Wall Clock 2135.597747917s] Trained 32.0 records in 6.73696977 seconds. Throughput is 4.74991 records/second. Loss is 0.011085096. 22-02-10 22:23:07 [Thread-4] INFO DistriOptimizer$:430 - [Epoch 2 4832/5216][Iteration 314][Wall Clock 2142.335336538s] Trained 32.0 records in 6.737588621 seconds. Throughput is 4.749474 records/second. Loss is 0.019572549. 22-02-10 22:23:13 [Thread-4] INFO DistriOptimizer$:430 - [Epoch 2 4864/5216][Iteration 315][Wall Clock 2149.017871686s] Trained 32.0 records in 6.682535148 seconds. Throughput is 4.788602 records/second. Loss is 0.02794234. 22-02-10 22:23:20 [Thread-4] INFO DistriOptimizer$:430 - [Epoch 2 4896/5216][Iteration 316][Wall Clock 2155.658202899s] Trained 32.0 records in 6.640331213 seconds. Throughput is 4.8190365 records/second. Loss is 0.00976663. 22-02-10 22:23:27 [Thread-4] INFO DistriOptimizer$:430 - [Epoch 2 4928/5216][Iteration 317][Wall Clock 2162.362889224s] Trained 32.0 records in 6.704686325 seconds. Throughput is 4.7727814 records/second. Loss is 0.0084485095. 22-02-10 22:23:34 [Thread-4] INFO DistriOptimizer$:430 - [Epoch 2 4960/5216][Iteration 318][Wall Clock 2169.700619155s] Trained 32.0 records in 7.337729931 seconds. Throughput is 4.361022 records/second. Loss is 0.022894122. 22-02-10 22:23:41 [Thread-4] INFO DistriOptimizer$:430 - [Epoch 2 4992/5216][Iteration 319][Wall Clock 2176.999067873s] Trained 32.0 records in 7.298448718 seconds. Throughput is 4.3844934 records/second. Loss is 0.030613616. 22-02-10 22:23:48 [Thread-4] INFO DistriOptimizer$:430 - [Epoch 2 5024/5216][Iteration 320][Wall Clock 2183.947033116s] Trained 32.0 records in 6.947965243 seconds. Throughput is 4.605665 records/second. Loss is 0.013181107. 22-02-10 22:23:55 [Thread-4] INFO DistriOptimizer$:430 - [Epoch 2 5056/5216][Iteration 321][Wall Clock 2190.758897207s] Trained 32.0 records in 6.811864091 seconds. Throughput is 4.697686 records/second. Loss is 0.012222898. 22-02-10 22:24:02 [Thread-4] INFO DistriOptimizer$:430 - [Epoch 2 5088/5216][Iteration 322][Wall Clock 2197.469852111s] Trained 32.0 records in 6.710954904 seconds. Throughput is 4.768323 records/second. Loss is 0.03385044. 22-02-10 22:24:08 [Thread-4] INFO DistriOptimizer$:430 - [Epoch 2 5120/5216][Iteration 323][Wall Clock 2204.152217718s] Trained 32.0 records in 6.682365607 seconds. Throughput is 4.7887235 records/second. Loss is 0.011642132. 22-02-10 22:24:15 [Thread-4] INFO DistriOptimizer$:430 - [Epoch 2 5152/5216][Iteration 324][Wall Clock 2210.818264148s] Trained 32.0 records in 6.66604643 seconds. Throughput is 4.8004465 records/second. Loss is 0.035903994. 22-02-10 22:24:22 [Thread-4] INFO DistriOptimizer$:430 - [Epoch 2 5184/5216][Iteration 325][Wall Clock 2217.585153504s] Trained 32.0 records in 6.766889356 seconds. Throughput is 4.728908 records/second. Loss is 0.02853406. 22-02-10 22:24:29 [Thread-4] INFO DistriOptimizer$:430 - [Epoch 2 5216/5216][Iteration 326][Wall Clock 2224.273770891s] Trained 32.0 records in 6.688617387 seconds. Throughput is 4.7842474 records/second. Loss is 0.04148032. 22-02-10 22:24:29 [Thread-4] INFO DistriOptimizer$:475 - [Epoch 2 5216/5216][Iteration 326][Wall Clock 2224.273770891s] Epoch finished. Wall clock time is 2325974.391781 ms\r22-02-10 22:24:29 [Thread-4] INFO DistriOptimizer$:112 - [Epoch 2 5216/5216][Iteration 326][Wall Clock 2224.273770891s] Validate model...\r22-02-10 22:26:10 [Thread-4] INFO DistriOptimizer$:178 - [Epoch 2 5216/5216][Iteration 326][Wall Clock 2224.273770891s] validate model throughput is 6.124821 records/second\r22-02-10 22:26:10 [Thread-4] INFO DistriOptimizer$:181 - [Epoch 2 5216/5216][Iteration 326][Wall Clock 2224.273770891s] Top1Accuracy is Accuracy(correct: 419, count: 624, accuracy: 0.6714743589743589)\r\u0026lt;bigdl.orca.learn.pytorch.pytorch_spark_estimator.PyTorchSparkEstimator at 0x7f7cf0f7fc10\u0026gt;\rresult = est.evaluate(data=test_loader) for r in result: print(r, \u0026#34;:\u0026#34;, result[r]) [Stage 665:\u0026gt; (0 + 1) / 1]\r22-02-10 22:27:53 [Thread-4] INFO DistriOptimizer$:1518 - Top1Accuracy is Accuracy(correct: 426, count: 624, accuracy: 0.6826923076923077)\rTop1Accuracy : 0.682692289352417\rThe final accuracy is 68.26%.\nest.save(\u0026#34;/content/drive/MyDrive/Colab_Notebooks/integratedNet\u0026#34;) # We will finally save the model '/content/drive/MyDrive/Colab_Notebooks/integratedNet'\r# And stop the orca context when program finishes stop_orca_context() Stopping orca context\r","date":"February 11, 2022","hero":"/es/posts/master-en-biolog%C3%ADa-computacional/big-data/final-project/annex-i-code/hero.png","permalink":"https://www.pablomarcos.me/es/posts/master-en-biolog%C3%ADa-computacional/big-data/final-project/annex-i-code/","summary":"This jupyter notebook shall serve as accompanying material to this repositories\u0026rsquo; README, a report for the “Big Data Engineering” subject at UPM’s Master in Computational Biology. It is thus only intended as a recopilation of used code; for the full discussion, please refer to the README.\nDependencies Installation # First, we install jdk8 !apt-get install openjdk-8-jdk-headless -qq \u0026gt; /dev/null import os # And set the environment variable \u0026#39;JAVA_HOME\u0026#39;. os.environ[\u0026#34;JAVA_HOME\u0026#34;] = \u0026#34;/usr/lib/jvm/java-8-openjdk-amd64\u0026#34; !","tags":null,"title":"Annex I - Code"},{"categories":null,"contents":"by Yaiza ARNAIZ ALCACER and Pablo Ignacio MARCOS LOPEZ\nAbstract Pneumonia is a serious illness characterised by a severe cough with phlegm, fever, chills and shortness of breath, which is caused by inflammation of the alveoli in one or both lungs. Despite advances in the diagnosis and treatment of lung infections, pneumonia is the sixth leading cause of death in adults in the United States, with more than six million cases of acute pneumonia each year, one million of which require hospitalisation.\nThe simplest way to determine the extent and location of the infection, so that it can be treated more efficiently, is by taking chest X-rays; however, at present, analysis of these X-rays is limited to what humans are capable of, with the time delays and enormous costs that this entails.\nThe aim of this paper is to present a complete model of thoracic X-ray image analysis, which we have developed by stacking two different Machine Leaning models, so that future scientists can build on it to hopefully improve the waiting time and economic efficiency of this class of tests.\nThe present model has an accuracy of 68.26 %. Nevertheless, it is designed to handle big data and uses state of the art techniques presented in Anunay Gupta et al.1. The approach uses several networks pretrained on ImageNet and stacks their learned representations. It is up to future researchers (possibly for those with more resources) to improve the stacking process. As further experiment (not done yet in this field) we want to implement a voting system among the networks, which could help with accuracy.\nIntroduction Description of the illness Pneumonia is a disease characterised by infection and inflammation of the air sacs of one (unilateral) or two (bilateral) of the lungs, and especially of the alveoli. These are small \u0026ldquo;clusters\u0026rdquo; located at the end of each bronchus whose function is to allow oxygen to enter our blood; however, during pneumonia, part of them may end up filling with fluid or pus, preventing air from entering.\nThe spectrum of germs responsible for pneumopathies is broad, and new pathogens are constantly being identified: it may be caused by bacteria, but it is also possible that a virus, such as influenza or COVID-19, is responsible for the infection. Despite advances in the diagnosis and treatment of lung infections, pneumonia remains a major cause of morbidity and mortality in adults, representing the sixth leading cause of death in the United States, with more than six million cases each year, more than one million of which require hospitalisation.\nSymptoms\nSigns and symptoms of pneumonia vary from mild to severe, depending on factors such as the type of germ causing the infection, age and general health. Mild signs and symptoms are often similar to those of a cold or flu, but last longer. These may include:\nChest pain when breathing or coughing. Nausea, vomiting or diarrhoea. Difficulty breathing Coughing, which may produce phlegm Fatigue Fever, sweating and/or chills * Lower than normal body temperature In adults aged 65 years and older it may also cause confusion or changes in mental awareness and a lower than normal body temperature (the latter is common in people with a weakened immune system). Newborns and infants may sometimes show no signs of infection, but may also vomit, have fever and cough, appear restless or tired and lack energy, or have difficulty breathing and eating.\nRisk factors Children under 2 years of age or younger People 65 years of age or older Being hospitalised, especially if you are on a ventilator Tingling Chronic diseases such as asthma or heart disease * Weakened or suppressed immune system Weakened or suppressed immune system Diagnosis The most common methods are:\nBlood tests to confirm an infection and try to identify the type of organism that has caused the pneumonia. Chest x-ray to determine the extent and location of the infection. Pulse oximetry to measure the level of oxygen in the blood. Sputum test to obtain a sample of fluid from the lungs. In case of a more serious condition, a CT scan or pleural fluid culture may also be performed. As can be seen, the most common methods of diagnosis are based on clinical data, appropriate microbiological tests and chest X-rays, which can quickly demonstrate parenchymal abnormalities. Radiography is an important element in the initial evaluation of any patient with suspected pneumopathy, but diagnosis remains a challenge, as pneumopathies with identical clinical and radiological signs may have different germs as their origin.\nThe most common treatments include antibiotics to treat bacterial pneumonia, cough medicines and fever reducers, but the prognosis is poor: 2.63% of infected patients eventually die, a rate even higher than measles or HIV in their worst years, but lower than other monsters such as Ebola. It is because of this that proper and early diagnosis is essential, to prevent the disease from progressing further and further complicating patient survival.\nCurrent state of the art for classification of X Ray Images As we have seen, the inference of medical diagnoses from X-ray images is an essential feature in the diagnosis of pneumonia. However, during the CoViD-19 pandemic in which we live, the number of doctors available (either due to contagion or confinement) has radically decreased, while demand has increased due to coronavirus-derived pneumonia cases, creating a bottleneck that needs an urgent solution.\nThus, the potential of Deep Learning models of X-ray images has been investigated for some years now to hopefully find a model with sufficient accuracy, recall and precision for the detection of COVID-19 induced pneumonia using chest radiography without human supervision. In the paper \u0026ldquo;InstaCovNet-19: A deep learning classification model for the detection of COVID-19 patients using Chest X-ray\u0026rdquo;1, Gupta et al present a model that, precisely, is able to achieve greater than 99% accuracy (much higher than human) by stacking pre-existing networks called Iception, NASnet, Xception, MobileNetV2 and ResNet, all of which are image classification models.\nOur original idea was to study the performance of this model (InstaCovNet-19), but, since its code is not available, and knowing, thanks to this paper, that stacking neural networks significantly improves its results, we have decided to design our own model using (py)Spark and integrated stacking.\nMatherials and Methods Dataset Description To train, test and validate our models, we have used a dataset of validated OCT images and chest X-rays as described and analysed in \u0026ldquo;Deep learning-based classification and referral of treatable human diseases\u0026rdquo;, available on Mendeley Data under CC-By-Sa 4.0 licence.2\nThis dataset consists of a group of chest X-ray images (anterior-posterior) selected from retrospective cohorts of paediatric patients aged one to five years from the Guangzhou Women and Children\u0026rsquo;s Medical Center in Guangzhou (China), selecting for quality control all poor quality or unreadable scans. The diagnostic images were graded by two medical experts before being cleared for AI system training and, in order to account for any grading errors, the evaluation set was also reviewed by a third expert. All radiographs were performed as part of the patients\u0026rsquo; routine clinical care.3\nThe images are divided into a training set and a separate patient test set. Images are labelled as (disease)-(random patient ID)-(image number for each patient) and divided into 4 directories: CNV, DME, DRUSEN and NORMAL. The images are labelled as (disease)-(random patient ID)-(image number for each patient) and divided into 3 folders: Training and Test, which are part of the original dataset; and Validation, which separates 16 images for purposes of checking the validity of the model. Each of these three folders contains the subfolders \u0026ldquo;normal\u0026rdquo; and \u0026ldquo;Pneumonia\u0026rdquo;, to compare between positive and negative patients.\nWorkflow Different approaches can be taken in this respect, so we decided to look at the state of the art for inspiration.1 Our original idea was to study the performance of the InstaCovNet-19 model, but, since its code is not available, and knowing, thanks to this paper, that stacking neural networks significantly improves classification results, we have decided to design our own model using (py)Spark and integrated stacking.\nHowever, after performing said stacking, we encounter a RAM problem: neither the 12 gb of Google Collab Free, nor those available in its Pro version, are enough to run the code, for which, it seems, the authors of the respective paper must have had a lot of resources. Thus, it is not possible to define exactly the same neural network as in our reference paper, as the session is squashed due to the ram limit, but we have nonetheless defined the code and found that, until the limit is reached, it works without problems (it can be consulted in the Annex). However, and since with our current resources this code will not run, we will define a new neural network, one that only stacks two sub-networks, but that Google Collab can handle.\nWe found that the stacking works well and that we can concatenate the models if the last layer of both have the same shape. That means that we need to look at every last layer of each network and feed its learned representation through a small amount of layers. The networks themselves can be improved even further using industry-standard best practices for transfer learning.4\nFor the data itself, we will do the following preprocessing:\nCenterCrop - Resizes the image to 224 x 224 RandomFlip - Randomly flips 50% of the image horizontally ColorJitter- Randomly adjusts the brigthness of 50% of the images Normalize - Normalizes the images And then feed it to pytorch (a ML library initially developed at Facebook) using Orca from Analytics Zoo and pySpark for distributed running.\nResults and discussion The accuracy (~68.3%) is lower than expected, considering that our experiment stacks 2 different networks. As we can see in the annex, the loss is very small, but the accuracy is still quite low. This is strange, since the loss usually indicates how well the model learns. Nevertheless, a low loss does not mean better accuracy because the model could be learning in an unexpecting way. A possible explanation for low loss with a low accuracy is called high variance.\nWe can conclude that scaling an integrated neural network like that of Anunay Gupta et al.1 is extremely complicated, since 5 neural networks are used in total, some larger than others and all with a very different last layer. For example, inception_v3 is a really good model for X-ray image classification, as you can read in previous work with transfer learning. However, it is a very large model and stacking this model with another one requires a lot of ram memory in training that we do not have now.\nWith this in mind, taking inceptio_V3 and doing transfer learning could be a good option, but such an experiment would not be original or fun, as it has already been tried and tested by many previous works.\nAs a further experiment, we could use transfer learning of different good models in image classification, but without stacking them together, and then implement a voting system.\nReferences A. Gupta, Anjum, S. Gupta, y R. Katarya, «InstaCovNet-19: A deep learning classification model for the detection of COVID-19 patients using Chest X-ray», Applied Soft Computing, vol. 99, p. 106859, feb. 2021, doi: 10.1016/j.asoc.2020.106859. [Online]. Disponible en: https://www.sciencedirect.com/science/article/pii/S1568494620307973. [Accessed: February 11th, 2022]\u0026#160;\u0026#x21a9;\u0026#xfe0e;\u0026#160;\u0026#x21a9;\u0026#xfe0e;\u0026#160;\u0026#x21a9;\u0026#xfe0e;\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nD. Kermany, K. Zhang, y M. Goldbaum, «Labeled Optical Coherence Tomography (OCT) and Chest X-Ray Images for Classification», vol. 2, ene. 2018, doi: 10.17632/rscbjbr9sj.2. [Online]. Disponible en: https://data.mendeley.com/datasets/rscbjbr9sj/2. [Accessed: February 11th, 2022]\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nD. S. Kermany et al., «Identifying Medical Diagnoses and Treatable Diseases by Image-Based Deep Learning», Cell, vol. 172, n.º 5, pp. 1122-1131.e9, feb. 2018, doi: 10.1016/j.cell.2018.02.010. [Online]. Disponible en: https://www.cell.com/cell/abstract/S0092-8674(18)30154-5. [Accessed: February 11th, 2022]\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nF. Zhuang et al., «A Comprehensive Survey on Transfer Learning», arXiv:1911.02685 [cs, stat], jun. 2020 [Online]. Disponible en: http://arxiv.org/abs/1911.02685. [Accessed: February 11th, 2022]\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","date":"February 11, 2022","hero":"/es/posts/master-en-biolog%C3%ADa-computacional/big-data/final-project/paper/hero.png","permalink":"https://www.pablomarcos.me/es/posts/master-en-biolog%C3%ADa-computacional/big-data/final-project/paper/","summary":"by Yaiza ARNAIZ ALCACER and Pablo Ignacio MARCOS LOPEZ\nAbstract Pneumonia is a serious illness characterised by a severe cough with phlegm, fever, chills and shortness of breath, which is caused by inflammation of the alveoli in one or both lungs. Despite advances in the diagnosis and treatment of lung infections, pneumonia is the sixth leading cause of death in adults in the United States, with more than six million cases of acute pneumonia each year, one million of which require hospitalisation.","tags":null,"title":"Pneumonia identification from X-Ray Imagery"},{"categories":null,"contents":"Introduction Single-cell omics technologies are a series of scientific methods that allow us to investigate the variations that exist, at the individual level, in the expression of the different cells that make up a whole, whether this is a tissue, a colony of bacteria, or even a human being.[@paolillo_single-cell_2019] Through these ultra-detailed analyses, we can identify exactly which molecular characteristics correspond to certain clinical outcomes, and therefore allow us to go beyond the \u0026ldquo;macro\u0026rdquo; paradigm in which we have been working until now. Thus, since the publication of the first single-cell RNA-sequencing study in 2009,[@paolillo_single-cell_2019] the number of publications related to this field has skyrocketed, multiplying by more than 10 times between now and 2021.[@noauthor_search_nodate] In this work, our aim is to delve deeper into the molecular characteristics that correspond to certain clinical outcomes.\nIn this report, we aim to delve deeper into the use of this kind of analysis to better understand how it works and the kind of insigts it can provide us with, using advanced statistical analysis methods such as Principal Component Analysis, relying on the R programming language and packages such as matlib and Seurat.\nMatherials and Methods Exercise 1: Undertanding Mutagenesis Processes As part of this exercise, we have received a data file, table.csv, containing the results of a series of gene expression assays in plants. Thus, we have both wild type organisms (i.e. whose genome is the predominant one in nature)[@noauthor_heredity_nodate] and shr mutants, which have a loss-of-function mutation in the SHORT ROOT protein,[@koizumi_short-root_2012] and whose presence causes, precisely, reduced apical root growth due to deficiencies in the formation of separate endodermal and cortical cell layers. To study the effect of this mutation in detail, both cell lines have been supplemented with factors BlueJay, (BLJ), JackDaw (JKD), MagPie (MGP), NutCracker (NUC), Imperial Eagle (IME) and ScareCrow (SCR).\nThus, our aim is to understand how loss of function works, and what ways different factors affect it, for which we will use PCA type analysis. For this first exercise, we will use princomp, a function that comes pre-included in R-base, and show the initial PCA result. Then, to understand how the loss of function due to the shr mutant works, we will create a series of wt-shr \u0026ldquo;intermediate transcriptomes\u0026rdquo;, in which we will be able to see how the loss of function is reversed. Finally, we will add SCR to our intermediate transcriptomes, and see what happens to the 25 genes that contribute the most to the expression of the shr protein, in order to understand if, for example, it belongs to a family of genes and transcription factors, or simply to see what other parts of the organism it affects.\nExercise 2: Finding Stem Cell Biomarkers The aim of this second exercise will be to demonstrate the potential of PCA to make sense of a large set of data, the intention being, in this assignment, to find biomarkers that allow us to differentiate the presence of Stem Cells from a new set of assays on gene expression with multiple samples.\nFirst, we will perform a preliminary visualisation of the data using box plots, to try to determine which genes have extraordinary expression values and, therefore, which are more likely to influence the result of the PCA and end up finding good molecular markers. In the following, we will analyse the differences in the transcriptomes using PCA, indicating which are the most important components and elucidating whether these are related to the box plot results or not.\nHaving discovered the most important genes and components, and knowing that one of our genes, WOX5, is only expressed in stem cells, we can proceed to subtype only those markers that differentially express WOX5, performing a new PCA with these genes and analysing the result in detail in terms of components, loadings and scores.\nComparing the first PCA with the second, we will try to find out what is the significance of the Principal Components, see if we can discover the presence of biomarkers through their scores and speculate about which cell types are likely to present better markers.\nIn this case, and in order to fully understand how PCA works, we will perform the analysis completely manually, diagonalising the matrices and calculating their eigenvectors by hand, instead of using an integrated module as we did in the previous part. For this, we used R\u0026rsquo;s nmatlib package.\nExercise 3: Differential Expression Clustering Using Seurat Seurat is an R package designed for QC, analysis, and exploration of single-cell RNA-seq data, which aims to enable users to identify and interpret sources of heterogeneity from single-cell transcriptomic measurements, and to integrate diverse types of single-cell data.[@hao_integrated_2021] In this exercise, we will learn to use it through a practical example, in which we will analyse a database of Peripheral Blood Mononuclear Cells (PBMC) recovered from a healthy donor and sequenced on Illumina NovaSeq with approximately 43k read pairs per cell.[@10xgenomics_10k_nodate] This database is available for free, and under Creative Commons license, on the 10xGenomics website.[@10xgenomics_10k_nodate]\nOnce this data has been downloaded, we will perform a Quality Control, removing any entries with more than 1800 expressed genes per cell or less than 4500 read counts per cell, and Normalize the data using the LogNormalize method with a factor of 1000. Having removed this \u0026ldquo;incorrect\u0026rdquo; outliers, we would like to separate highly variable genes (those that show differential expression across tissues, for example), focusing on them for downstream analysis; seurat does this by modelling the mean-variance relationship od single-cell data, returning 2000 of the most important features. Using this, we will identify the top10 most important genes, centering and scaling them onto a linear model, assuring that the mean expression across cells is 0 and the variance is 1. This gives equal weight to all cells, ensuring there is no over-domination of the whole data-set by this highly-expressed cells.\nAfter pre-processing the data, we can perform a PCA analysis. To understand the results, first, we will group the genes based on behaviour and plot a heatmap with the 15 most variable genes. We can also plot the cells based on which component they belong to, and show the scores for each variant and component. Seurat may also plot the cells by their PCA scores, which would help us group genes by common characteristics (for example, gene families). To determine the number of components, seurat automatically calculates significancy values for N components and reruns the analysis until there isa good distribution of gene scores, in a method that was inspired by the JackStraw algorithm. There, significant components would be those with a strong enrichment of low p-values, represented as dots over a curved line. We may also plot a traditional elbow plot, in which we show the standard deviation against the number of Principal Components, to see if the result from the JackStraw graph is OK.\nAfter defining an adecuate number of Principal Components, we will run a clustering to see if, in fact, we can generate real groups of families of genes. This clusters can be visualized through non-linear dimensional reduction, and then plotted; to validate our results, we will reproduce the analysis by keeping only genes expressed in at least 3 cells and cells with at least 200 and at most 2500 genes, lognormalizing with a scale of 10000 and selecting the 3000 most variable features. We will also regress out any variability, and use PCs 1-5 with a resolution of 0.6\nFor each of the gene expression clusters that we will have just defined, we would like to find a series of associated biomarkers, to try and see whether they relate to a specific cellular function or not. For this, Seurat has a built-in function that analyzes each cluster with regards to all others, analyzing whether some biomarkers show a disproportionate expression in each cluster or not; we will visualize this genes in a heatmap, to see whether they are differentially expressed or not.\nFinally, we can directly show the distribution of the expression of these markers between the different clusters, by means of both violin plots and feature plots. For them, we can choose between t-SNE and uMap as methods; since t-SNE very good at creating a single map that reveals structure at many different scales, we decided to use that algorithm for our dimensionality reduction. We will also show our renamed clusters based on the information we will then know; since all of the data-processing steps will already be done, this will just be a visualization step, in which we get to see how our analysis went.\nResults Exercise 1 Principal Component Analysis After running the related code in the Annex, we get the following Figure 1:\nFor the Variance plot, we can see that only 2 components seem to explain more than 80% of the variance of the data. That is great! It means our PCA is really great at simplifying the understanding of the data. For the Scores plot, we see that there are two main directions for the data, too, and that there are lots of genes relatively separated from the 0 mark. This is good, too! Since the principal components could be defined as a new coordinate system on which to plot our data, high scores indicate that it is easier to relate these scores to a particular PC. For the loadings plot, we also find relatively high values, which probably correlate with the first plot and are forcing the scores to the left. Intermediate transcriptomes In Figure 2, as in the previous case, we can see that only 2 PCs explain more than 90% of the variance, and that, when plotting these two components, we can see that, indeed, there is a clear regression along component 1, where we can see that, as their values go down, we pass from the transcriptome at 25%, to the one with 50% and, finally, to the one at 75%. In component 2, however, this is not the case, and the 75% transcriptome appears before the 25% transcriptome.\nAdding another transcriptome: SCR As we can see in Figure 3, things dont chnage much, and SCRdomain appears as a bit of an outlier in our otherwise straight line.\nMost important genes For this section, our intention is to collect the 25 most differentially expressed genes from our entire list, attempting to elucidate how these are affected by the shortroot mutation through a Heatmap. However, after several hours of trying and much frustration, the author of this report has discovered that, apparently, R-base\u0026rsquo;s heatmap function is simply incapable of doing such a thing.[@noauthor_display_nodate] Leaving aside how incredible it seems that one of the most widely used programming languages in the world is incapable of doing something so simple in a natively simple way, the author has decided to go the lame way and capture them side by side. The result can be seen in Figure 4, just below:\nWe can see that AT2G22980 shows highly differential expression in gene J0517, and is almost unexpressed elsewere; however, for the mutant shrJ0571, we see no expression anywhere, which might give us a cue: this mutation must be a loss of function mutation, which, if we look at our \u0026ldquo;Heatmap for Intermediate transcriptomes\u0026rdquo;, gets restored when we get a part of the initial transcriptome in.\nExercise 2 Data Visualization The first idea is to do a simple data visualization, to see which genes among our data have more variable expression profiles, and thus are more likely to influence the number of PCs and the quality of our analysis. We can use the boxplot() function for that:\nAs Figure 5 shows, most of the genes have a very similar basal level of expression, and very low, close to 0, which is the reason why the boxes in our box plots (representing the values between Q1 and Q3) look so small and stuck to the bottom of the image. In addition, we can see that most of the genes have a large number of outliers, represented by circles that stack up to look like columns, and that, in almost all cases, do not exceed the 5000 barrier. However, for both E30 and S18, we observe a large number of outliers, which is precisely what we wanted to find with this representation: these are the genes that will be of most interest to us, as they are the ones that most modify the response of the system.\nAnalyzing transcriptome differences for all cell types In Figure 6 below, components 1-3 seem to explain most of the variance on our dataset. with the percentage of variance dropping significantly after PC4. This is equivalent to the more traditional \u0026ldquo;elbow plot\u0026rdquo;, in which we find an elbow after which adding additional PCs has little to no effect on variance. Seen the resultsof the above plot, we will keep 4 PCs for our analysis. With regards to the loadings plot, what we find is that, once again, there seem to be two clear outliers (maybe related to E30 and S18?), with two semi outliers which might actually be PC3 and PC4. The loadings seem to correlate with the scores, which is good; in both graphs, there is a cluster of uninteresting genes close to 0, with some outliers to the left bringing the scores to the left too, and adding variance that may be of interest.\nFinding biomarkers for Stem Cells Having decided on the number of Principal Components for our PCA analysis, we can proceed with the analysis itself. Since we want to find biomarkers that occur specifically in Stem Cells, and we know the genes that are differentially expressed there present the WOX5 domain, we will select for the PCA analysis only those rows that surpass a treshold of 1 for said gene, and that dont surpass it for any of the other genes.\nAs shown by Figure 7, the markers we have selected are, in fact, hightly selective towards WOX5 (and thus stem cells), ignoring most of the rest of the tissues.\nPCA of stem-cell markers After re-doing the PCA only for these WOX5-selective markers, we get Figure 8:\nUnlike what happened before, we can see that 2 PCs suffice to explain most of the variance of the data. This makes sense: we only have ~450 rows, down from ~63000 before! With regards to the loadings, we see much fewer outliers, to the left of the graph, and, while some do exist, forcing the scores a bit in their direction, the real spread occurs along the vertical axis, especially for scores: that is, Component 2 records a much wider diversity than Component 1. That a single component explains so much variance makes a lot of sense: on the one hand, because it is inherent to PCA processes to reduce as much as possible the dimensions we work with; and, on the other hand, because we ourselves have manually reduced those dimensions by selecting a variable of special interest, the overexpression of WOX5. It is therefore possible that PC1 is telling us just that, the inordinate importance of variance in WOX5 expression in understanding our data.\nComparing the graphs and identifying biomarkers We can compare the results obtained for stem cells and for normal cells and see what happens. For instance, we can check PC1 vs PC2, and PC3 vs PC4, and see how they correlate in Figures 9 and 10.\nTo interpret these graphs, we have to know that, to differentiate biomarkers, the key is the points that are separated from the rest, \u0026ldquo;marking\u0026rdquo; a differential expression. For example, in the PC1 vs PC2 loadings, we find that WOX5 is very clearly separated from the rest of the loadings in the Stem cell data graph, being located at the top left; as this does not happen in the full data, we can see that it is indeed a biomarker of this expression. We can also see that S18 and E30 tend to be located separately from the others in most loadings: this makes sense, as they are the genes with the highest differential expression, and therefore those for which it would be easiest to find biomarkers. To do this, we go to the Scores graph, where, if we see a point that corresponds to the loadings in terms of its separation from the rest, we can conclude that the marker it represents is specific to that gene (although we cannot know exactly which one because we have omitted the labels to avoid further blurring the image). This happens, apart from WOX5, for S18 and E30, which are the ones for which we can find biomarkers; not surprisingly, it was precisely S18 and E30 which were overexpressed in our original data, which is, indeed, why we can find biomarkers for them.\nExercise 3 Quality Control and Normalization The coding processes can be consulted in the joint script; essentially, for the quality control, the first step would be to draw some plots that show the internal distribution of our data and allow us to identify some outliers. Once again, since R clearly does not stand for \u0026ldquo;Really smart\u0026rdquo;, we found out that there is no easy, out-of the box way of merging the kind of figures Seurat draws into one canvas, so we had to use an internet resource;[@] the result can be seen in Figure 11\nIn each of the violin plots, each point represents an individual read, with the red area being the rotated kernel density plot for the values. The left plot represents the number of genes expressed per cell, with the one in the one on the right showing the number of reads per cell. At first sight, there seems to be more counts than features; this is because each gene can have one or more reads. When plotted against each other, a good correlation of 0.95 is found; however, some outliers are found with the distribution being really narrow because of that. Thus, outlier removal is advisable: the results are in Figure 12.\nHere, we see that, while the correlation does not get any better, it also does not get worse, and there are in fact way less outliers, which is always desirable.\nFeature Selection As explained in Matherials and Methods, Seurat aut-selects the 2000 most variable genes; they can be seen, labeled, on Figure 13:\nPCA Analysis and Clustering To decide the number of Principal Components in our PCA, Seurat uses a JackStraw-like algorithm shown in Figure 14(b). We can see that, between PC9 and 10 there is a large gap, which indicates that 10 is the adequate number of PCs to select. Using a t-SNE algorithm, we will cluster the data resulting from this PCA into 10 groups, a number obtained from the Elbow plot available in Figure 14(a), which shows a large drop in Standard Deviation between components 9 and 10, such that, from PC10 onwards, adding an additional component does not improve the SD explained by the system.\nThen, a clustering using t-SNE for dimensionality reduction was performed, and its results were compared, in Figure 15, to another clustering using PCs 1-5 with a resolution of 0.6, for which 10 clusters were selected as well according to the corresponding elbow plot (Figure 14(c)), which is not very different to the one in Figure 14(a); in both cases, there are two possible elbows, at 5 and 10 clusters, respectively. This is good! It means our results are robust, and do not depend so much in discretionary parameters which could be misinterpreted. If we look at the dim plots themselves, we will see that their shapes are quite different, but mostly in the group shapes, since their size is quite similar. However, the groups generated with 5 PCs seem to be, at first glance, more cohesive and close thogether than either those that existed or those created with 5 10 PCs. The two options, however, seem good enough, which reinforces the idea that our results are robust independently of the number of PCs chosen.\nFinding Biomarkers through Differential Gene Expression Since, as we have already discussed in the previous section, 5 PCs seem to show better adjustment to the data than 10 PCs, we will use the data from our 5-PC repetition for this analysis. The results in Figure 16 seem promising: for each gene cluster, from 0 to 9, we find that each one has a series of clearly differentiated markers (in yellow) and that, except in clusters 4-8-9 and 5-7, there are no duplicities, as they are marked on the diagonal. This indicates that we will surely be able to differentiate the gene clusters by their markers, and, therefore, that they probably have a differentiated biological function.\nShowing the expression distribution of these markers Once the PCA and differential gene expression analysis has been performed, we can show the FeaturePlots (Figure 19) and ViolinPlots (Figure 18) of the differential expression of the different markers in the different clusters, showing these labelled in order to associate them to specific functions (Figure 17). As we can see, it seems, for instance, that the biomarker dd-Smed-v6-9977-0, related to cluster 4, is clearly related to muscle progenitors, its expression being highly localised in the lower left side of the image, as can be seen in the individual feature plot. However, for dd-Smed-v6-1999-0, mainly related to epidermal, phagocyte and GABA neuron progenitors, it seems that its expression is much more diffuse, which may be due both to the fact that it is a poorer quality cluster and that, as it contains so many features, it is easier to find dispersion.\nThis analysis could be done individually for each of the clusters, but for the purpose of the exercise at hand, I believe that this short description serves to show how to proceed in the clusters.\nConclussions Throughout these three exercises, and thanks to packages such as Seurat and Matlib, we have learned how to use R packages to deal with Single Cell Genomics data in an efficient, effective, useful way, and in only 15 pages, which was the minimum required! Thanks to this complete text, as well as the accompanying code scripts, it has been proven that we know how to use PCA for the analysis of massive SCG data, demonstrating, we hope, a thorough knowledge of the subject.\nReferences This document, and the accompanying code, is availaible under the CC By SA 4.0 License, and was generated using pandoc\n","date":"February 6, 2022","hero":"/es/posts/master-en-biolog%C3%ADa-computacional/gdav/single-cell-genomics/final-report/hero.jpg","permalink":"https://www.pablomarcos.me/es/posts/master-en-biolog%C3%ADa-computacional/gdav/single-cell-genomics/final-report/","summary":"Introduction Single-cell omics technologies are a series of scientific methods that allow us to investigate the variations that exist, at the individual level, in the expression of the different cells that make up a whole, whether this is a tissue, a colony of bacteria, or even a human being.[@paolillo_single-cell_2019] Through these ultra-detailed analyses, we can identify exactly which molecular characteristics correspond to certain clinical outcomes, and therefore allow us to go beyond the \u0026ldquo;macro\u0026rdquo; paradigm in which we have been working until now.","tags":null,"title":"Report on Single Cell Genomics"},{"categories":null,"contents":"To any UPM\u0026rsquo;s Master in Computational Biology students that might be reading this, i have a strong warning to make:\nDO. NOT. PICK. THIS. SUBJECT. !!!!!!! The teacher was extremely rude to the students, to the point that, after being publicly humiliated, I decided to change it for \u0026ldquo;Evolutiva\u0026rdquo;, which I found much, much nicer. I have found absolutely no one from my promotion (2021/22) that can recommend this teacher, although the subject\u0026rsquo;s contents are, in fact, interesting. Thus, in case you decide to make the error of taking his course, I have uploaded all the assignments to make your life easier, although I am sure he will not be happy if he finds out I have done so (which delights me in a way 😋).\nPerhaps unsurprisingly, I didn\u0026rsquo;t get good grades in his assignments, so proceed with care!\n","date":"February 3, 2022","hero":"/es/posts/master-en-biolog%C3%ADa-computacional/gab/warning/hero.jpg","permalink":"https://www.pablomarcos.me/es/posts/master-en-biolog%C3%ADa-computacional/gab/warning/","summary":"To any UPM\u0026rsquo;s Master in Computational Biology students that might be reading this, i have a strong warning to make:\nDO. NOT. PICK. THIS. SUBJECT. !!!!!!! The teacher was extremely rude to the students, to the point that, after being publicly humiliated, I decided to change it for \u0026ldquo;Evolutiva\u0026rdquo;, which I found much, much nicer. I have found absolutely no one from my promotion (2021/22) that can recommend this teacher, although the subject\u0026rsquo;s contents are, in fact, interesting.","tags":null,"title":"DO NOT PICK THIS SUBJECT!!"},{"categories":null,"contents":"In the file \u0026rsquo;table.csv\u0026rsquo; rest a series of samples for gene expression assays in tissues shr, wt and complemented lines with factors BLUEJAY (BLJ), JACKDAW (JKD), MAGPIE (MGP), NUTCRACKER(NUC), IMPERIAL EAGEL (IME) and SCARECROW (SCR). We want to perform a PCA on this data to see how the data is structured and to try and make some educated guesses.\n# Set up images knitr::opts_chunk$set(dev = \u0026#34;png\u0026#34;, dpi = 1000, echo = FALSE, cache = TRUE, out.width = \u0026#34;450px\u0026#34;) Principal Component Analysis For this, we will be using princomp, a function from baseline R. We will use cor=FALSE as an option, since the correlation matrix can only be used if there are no constant variables, and process the output to plot loadings, scores and components.\nmy_table \u0026lt;-read.csv(\u0026#34;./table.csv\u0026#34;, header=TRUE, row.names = 1) # Remove empty columns my_table \u0026lt;- my_table[,apply(my_table, 2, function(x) { sum(!is.na(x)) \u0026gt; 0 })] # Calculate the initial PCA PCA\u0026lt;-princomp(na.omit(my_table),scores=TRUE) variance\u0026lt;-((PCA$sdev^2/sum(PCA$sdev^2))*100) # We can now plot the different parameters png(file=\u0026#34;ex1_pca1_plots.jpeg\u0026#34;,res=300, width = 5200, height = 5200, units = \u0026#34;px\u0026#34;) par(mfrow=c(3,1)) plot(variance, type=\u0026#34;s\u0026#34;, main=\u0026#34;Variance plot for Tissue data\u0026#34;, cex.main = 3, ylab=\u0026#34;Percentage of variance\u0026#34;, xlab=\u0026#34;PCA component\u0026#34;, cex.lab = 2) plot(PCA$loadings, main=\u0026#34;Loadings plot for Tissue data\u0026#34;, cex.main = 3, cex.lab = 2) plot(PCA$scores, main=\u0026#34;Scores plot for Tissue data\u0026#34;, cex.main = 3, cex.lab = 2) # Text labels could be added, but there are so many that they make it ilegible #text(PCA$scores,labels=row.names(PCA$scores),pos=c(4,4,4,2,2,4)) dev.off() For the Variance plot, we can see that only 2 components seem to explain more than 80% of the variance of the data. That is great! It means our PCA is really great at simplifying the understanding of the data. For the Scores plot, we see that there are two main directions for the data, too, and that there are lots of genes relatively separated from the 0 mark. This is good, too! Since the principal components could be defined as a new coordinate system on which to plot our data, high scores indicate that it is easier to relate these scores to a particular PC. For the loadings plot, we also find relatively high values, which probably correlate with the first plot and are forcing the scores to the left. Intermediate transcriptomes We can then create a series of intermediate transcriptomes, in order to see how these relate to the PCA loadings set, and whether these make it easier to recover expression in each one or not. Thus:\n# First, we create the intermediate transcriptomes intermediate \u0026lt;- my_table[,c(\u0026#34;J0571\u0026#34;, \u0026#34;shrJ0571\u0026#34;)] intermediate[,\u0026#34;shr:wt_25\u0026#34;] \u0026lt;- 0.75*my_table[,\u0026#34;shrJ0571\u0026#34;] + 0.25*my_table[,\u0026#34;J0571\u0026#34;] intermediate[,\u0026#34;shr:wt_50\u0026#34;] \u0026lt;- 0.50*my_table[,\u0026#34;J0571\u0026#34;] + 0.50*my_table[,\u0026#34;J0571\u0026#34;] intermediate[,\u0026#34;shr:wt_75\u0026#34;] \u0026lt;- 0.25*my_table[,\u0026#34;shrJ0571\u0026#34;] + 0.75*my_table[,\u0026#34;J0571\u0026#34;] # And we do the PCA PCA_intermediate\u0026lt;-princomp(na.omit(intermediate),scores=TRUE) variance_intermediate\u0026lt;-((PCA_intermediate$sdev^2/sum(PCA_intermediate$sdev^2))*100) # We can now plot the different parameters png(file=\u0026#34;ex1_pca2_plots.jpeg\u0026#34;,res=300, width = 5400, height = 5200, units = \u0026#34;px\u0026#34;) par(mfrow=c(2,1)) plot(variance_intermediate, type=\u0026#34;s\u0026#34;, main=\u0026#34;Variance plot for Intemediate Transcriptomes\u0026#34;, ylab=\u0026#34;Percentage of variance\u0026#34;, xlab=\u0026#34;PCA component\u0026#34;, , cex.main = 3, cex.lab = 2) plot(PCA_intermediate$loadings, xlim = c(0.25,0.8), ylim = c(-0.7,0.65), main=\u0026#34;Loadings plot for Intemediate Transcriptomes\u0026#34;, cex.main = 3, cex.lab = 2) text(PCA_intermediate$loadings, labels=row.names(PCA_intermediate$loadings), pos=c(2,4,4,4,3,3,3,4)) dev.off() Here, as in the previous case, we can see that only 2 PCs explain more than 90% of the variance, and that, when plotting these two components, we can see that, indeed, there is a clear regression along component 1, where we can see that, as their values go down, we pass from the transcriptome at 25%, to the one with 50% and, finally, to the one at 75%. In component 2, however, this is not the case, and the 75% transcriptome appears before the 25% transcriptome.\nAdding another transcriptome: SCR We add the SCR domain and re-do the PCA:\nintermediate[,\u0026#34;SCRdomain\u0026#34;] \u0026lt;- my_table[,\u0026#34;SCRdomain\u0026#34;] PCA_scr \u0026lt;-princomp(na.omit(intermediate),scores=TRUE) variance_scr\u0026lt;-((PCA_scr$sdev^2/sum(PCA_scr$sdev^2))*100) png(file=\u0026#34;ex1_pca3_plots.jpeg\u0026#34;,res=300, width = 5400, height = 5200, units = \u0026#34;px\u0026#34;) par(mfrow=c(2,1)) plot(variance_scr, type=\u0026#34;s\u0026#34;, main=\u0026#34;Variance plot for SCR Transcriptomes\u0026#34;, ylab=\u0026#34;Percentage of variance\u0026#34;, xlab=\u0026#34;PCA component\u0026#34;, cex.main = 3, cex.lab = 2) plot(PCA_scr$loadings, xlim = c(0.1,0.9), ylim = c(-0.9,0.85), main=\u0026#34;Loadings plot for SCR Transcriptomes\u0026#34;, cex.main = 3, cex.lab = 2); text(PCA_scr$loadings, labels=row.names(PCA_scr$loadings), pos=c(2,4,4,4,3,3,3,4)) dev.off() As we can see, things dont chnage much, and SCRdomain appears as a bit of an outlier in our otherwise straight line.\nMost important genes Finally, we will find the most important genes in the whole population. For this, we will choose the 25 highest-scoring genes in the \u0026ldquo;PCA_scr\u0026rdquo; PCA:\nVariable_genes = head(sort(PCA_scr$scores[, 1]), 20) png(file=\u0026#34;ex1_heatmap1.jpeg\u0026#34;,res=300, width = 5400, height = 5200, units = \u0026#34;px\u0026#34;) heatmap(as.matrix(my_table[rownames(as.matrix(Variable_genes)), ]), main=\u0026#34;Heatmap for all transcriptomes\u0026#34;, cex.main = 3, cex.lab = 2) dev.off() png(file=\u0026#34;ex1_heatmap2.jpeg\u0026#34;,res=300, width = 5400, height = 5200, units = \u0026#34;px\u0026#34;) heatmap(as.matrix(intermediate[rownames(as.matrix(Variable_genes)), ]), main=\u0026#34;Heatmap for Intermediate transcriptomes\u0026#34;, cex.main = 3, cex.lab = 2) dev.off() We can see that AT2G22980 shows highly differential expression in gene J0517, and is almost unexpressed elsewere; however, for the mutant shrJ0571, we see no expression anywhere, which might give us a cue: this mutation must be a loss of function mutation, which, if we look at our \u0026ldquo;Heatmap for Intermediate transcriptomes\u0026rdquo;, gets restored when we get a part of the initial transcriptome in.\n","date":"February 1, 2022","hero":"/es/posts/master-en-biolog%C3%ADa-computacional/gdav/single-cell-genomics/exercise-1/hero.png","permalink":"https://www.pablomarcos.me/es/posts/master-en-biolog%C3%ADa-computacional/gdav/single-cell-genomics/exercise-1/","summary":"In the file \u0026rsquo;table.csv\u0026rsquo; rest a series of samples for gene expression assays in tissues shr, wt and complemented lines with factors BLUEJAY (BLJ), JACKDAW (JKD), MAGPIE (MGP), NUTCRACKER(NUC), IMPERIAL EAGEL (IME) and SCARECROW (SCR). We want to perform a PCA on this data to see how the data is structured and to try and make some educated guesses.\n# Set up images knitr::opts_chunk$set(dev = \u0026#34;png\u0026#34;, dpi = 1000, echo = FALSE, cache = TRUE, out.","tags":null,"title":"GDAV Single Cell - Ex 1"},{"categories":null,"contents":"We have been given a file, \u0026lsquo;TableS5.csv\u0026rsquo;, which contains the data for a series of assays on gene expression with multiple samples. Our goal is to perform a Principal Component Analysis across the samples, to try and find out if we can relate said gene expresion profiles to a series of known biomarkers.\n# Set up images knitr::opts_chunk$set(dev = \u0026#34;png\u0026#34;, dpi = 1000, echo = FALSE, out.width = \u0026#34;450px\u0026#34;, cache = TRUE) #install.packages(\u0026#34;matlib\u0026#34;) library(matlib) # First, we import the needed libraries Data Visualization The first idea is to do a simple data visualization, to see which genes among our data have more variable expression profiles, and thus are more likely to influence the number of PCs and the quality of our analysis. We can use the boxplot() function for that:\n# Of course, first we have to read the data table \u0026lt;- read.csv(\u0026#34;./TableS5.csv\u0026#34;, row.names = 1, header = TRUE) png(file=\u0026#34;ex2_boxplots.png\u0026#34;,res=300, width = 5200, height = 2000, units = \u0026#34;px\u0026#34;) par(mfrow=c(1,1)) boxplot(table, main=\u0026#34;Boxplot for gene expression profiles\u0026#34;, cex.main = 3, cex.lab = 2) # After that, we can generate a quick boxplot dev.off() As we can see, most of the genes have a very similar basal level of expression, and very low, close to 0, which is the reason why the boxes in our box plots (representing the values between Q1 and Q3) look so small and stuck to the bottom of the image. In addition, we can see that most of the genes have a large number of outliers, represented by circles that stack up to look like columns, and that, in almost all cases, do not exceed the 5000 barrier. However, for both E30 and S18, we observe a large number of outliers, which is precisely what we wanted to find with this representation: these are the genes that will be of most interest to us, as they are the ones that most modify the response of the system.\nAnalyzing transcriptome differences for all cell types Here, we would like to calculate how many PCs are optimal for our PCA analysis, and to try and see any relationships among the gene expression values seen in the boxplots and the PCA loadings representation of each component. For this, we will first standarize the data, to get it to have a mean of 0 and a variance of 1, and then plot the variance graph:\n# Standarize data. Scale = FALSE means it will just substract the mean S\u0026lt;-scale(table,center=TRUE,scale=FALSE); # We will make the matrix diagonalizable so that later code will work C \u0026lt;-cov(S); det(C); E\u0026lt;-eigen(C) # Round some stuff round(inv(E$vectors)%*%C%*%E$vectors,digits=2) round(E$values,digits=2) # We first calculate the weights, loadings and scores: weight\u0026lt;-((E$values/sum(E$values))*100) loadings\u0026lt;-E$vectors; round(loadings,digits=3); S\u0026lt;-scale(table,center=TRUE,scale=FALSE); Scores\u0026lt;-S%*%loadings # And then plot them: png(file=\u0026#34;ex2_pca1_plots.png\u0026#34;,res=300, width = 5200, height = 5200, units = \u0026#34;px\u0026#34;) par(mfrow=c(2,2), oma=c(0,0,2,0)) plot(loadings, main=\u0026#34;Loadings plot for Full data\u0026#34;, xlim=c(-0.6,-0.1), xlab = \u0026#34;Component 1\u0026#34;, ylab = \u0026#34;Component 2\u0026#34;, cex.main = 3, cex.lab = 2) text(loadings,labels=colnames(table),pos=c(2,2,2,4,4,2)) plot(Scores, main=\u0026#34;Scores plot for Full data: C1 vs C2\u0026#34;, xlab = \u0026#34;Component 1\u0026#34;, ylab = \u0026#34;Component 2\u0026#34;, cex.main = 3, cex.lab = 2) plot(Scores[,3], Scores[,4], main=\u0026#34;Scores plot for Full data: C3 vs C4\u0026#34;, xlab = \u0026#34;Component 3\u0026#34;, ylab = \u0026#34;Component 4\u0026#34;, cex.main = 3, cex.lab = 2) plot(weight, type=\u0026#34;s\u0026#34;, main=\u0026#34;Weights plot for Full data\u0026#34;, cex.main = 3, cex.lab = 2, ylab=\u0026#34;Percentage of variance\u0026#34;, xlab=\u0026#34;PCA component\u0026#34;) dev.off() As we can see, components 1-3 seem to explain most of the variance on our dataset. with the percentage of variance dropping significantly after PC4. This is equivalent to the more traditional \u0026ldquo;elbow plot\u0026rdquo;, in which we find an elbow after which adding additional PCs has little to no effect on variance. Seen the resultsof the above plot, we will keep 4 PCs for our analysis. With regards to the loadings plot, what we find is that, once again, there seem to be two clear outliers (maybe related to E30 and S18?), with two semi outliers which might actually be PC3 and PC4. The loadings seem to correlate with the scores, which is good; in both graphs, there is a cluster of uninteresting genes close to 0, with some outliers to the left bringing the scores to the left too, and adding variance that may be of interest.\nFinding biomarkers for Stem Cells Having decided on the number of Principal Components for our PCA analysis, we can proceed with the analysis itself. Since we want to find biomarkers that occur specifically in Stem Cells, and we know the genes that are differentially expressed there present the WOX5 domain, we will select for the PCA analysis only those rows that surpass a treshold of 1 for said gene, and that dont surpass it for any of the other genes. Thus:\n# First, we subset the data to find only those markers with differential expression stem_cells \u0026lt;- subset(table, WOX5 \u0026gt;= 1 \u0026amp; (APL \u0026lt; 1 \u0026amp; CO2 \u0026lt; 1 \u0026amp; COBL9 \u0026lt; 1 \u0026amp; COR \u0026lt; 1 \u0026amp; E30 \u0026lt; 1 \u0026amp; GL2 \u0026lt; 1 \u0026amp; PET111 \u0026lt; 1 \u0026amp; S17 \u0026lt; 1 \u0026amp; S18 \u0026lt; 1 \u0026amp; S32 \u0026lt; 1 \u0026amp; S4 \u0026lt; 1 \u0026amp; SCR \u0026lt; 1 \u0026amp; WER \u0026lt; 1 \u0026amp; WOL \u0026lt; 1) ) # We can show the markers\u0026#39; names: rownames(stem_cells) # And heatmap their values: png(file=\u0026#34;ex2_heatmap1.png\u0026#34;,res=300, width = 5200, height = 5200, units = \u0026#34;px\u0026#34;) heatmap(as.matrix(stem_cells, scale=\u0026#34;column\u0026#34;), main=\u0026#34;Heatmap for Stem Cell data\u0026#34;, ) dev.off() # We scale along columns to account for relative value of markers As shown by the plotted heatmap, the markers we have selected are, in fact, highly selective towards WOX5 (and thus stem cells), ignoring most of the rest of the tissues. We can now proceed with a PCA analysis of this data:\nPCA of stem-cell markers We begin by doing what we already did for the full table:\nStem_Scaled\u0026lt;-scale(stem_cells,center=TRUE,scale=FALSE); Stem_Cov \u0026lt;-cov(Stem_Scaled); det(Stem_Cov); Stem_Eigens\u0026lt;-eigen(Stem_Cov) round(inv(Stem_Eigens$vectors)%*%Stem_Cov%*%Stem_Eigens$vectors,digits=2) round(Stem_Eigens$values,digits=2) Stem_weights\u0026lt;-((Stem_Eigens$values/sum(Stem_Eigens$values))*100) Stem_loadings\u0026lt;-Stem_Eigens$vectors; round(Stem_loadings,digits=3); Stem_Scaled\u0026lt;-scale(stem_cells,center=TRUE,scale=FALSE) Stem_Scores\u0026lt;-Stem_Scaled%*%Stem_loadings png(file=\u0026#34;ex2_pca2_plots.png\u0026#34;,res=300, width = 5200, height = 3000, units = \u0026#34;px\u0026#34;) par(mfrow=c(1,3), oma=c(0,0,2,0)) plot(Stem_loadings, main=\u0026#34;Loadings plot for Stem Cell data\u0026#34;, xlab = \u0026#34;Component 1\u0026#34;, ylab = \u0026#34;Component 2\u0026#34;, cex.main = 3, cex.lab = 2) text(Stem_loadings,labels=colnames(table),pos=c(2,2,2,4,4,2)) plot(Stem_weights, type=\u0026#34;s\u0026#34;, main=\u0026#34;Weights plot for Stem Cell data\u0026#34;, ylab=\u0026#34;Percentage of variance\u0026#34;, xlab=\u0026#34;PCA component\u0026#34;, cex.main = 3, cex.lab = 2) plot(Stem_Scores, main=\u0026#34;Scores plot for Stem Cell data\u0026#34;, xlab = \u0026#34;Component 1\u0026#34;, ylab = \u0026#34;Component 2\u0026#34;, cex.main = 3, cex.lab = 2) dev.off() Unlike what happened before, we can see that 2 PCs suffice to explain most of the variance of the data. This makes sense: we only have ~450 rows, down from ~63000 before! With regards to the loadings, we see much fewer outliers, to the left of the graph, and, while some do exist, forcing the scores a bit in their direction, the real spread occurs along the vertical axis, especially for scores: that is, Component 2 records a much wider diversity than Component 1. That a single component explains so much variance makes a lot of sense: on the one hand, because it is inherent to PCA processes to reduce as much as possible the dimensions we work with; and, on the other hand, because we ourselves have manually reduced those dimensions by selecting a variable of special interest, the overexpression of WOX5. It is therefore possible that PC1 is telling us just that, the inordinate importance of variance in WOX5 expression in understanding our data.\nComparing the graphs and identifying biomarkers Finally, we can compare the results obtained for stem cells and for normal cells and see what happens. For instance, we can check PC1 vs PC2, and PC3 vs PC4, and see how they correlate in both experiments:\n# We can do this for scores png(file=\u0026#34;ex2_compare_scores.png\u0026#34;,res=300, width = 3000, height = 3000, units = \u0026#34;px\u0026#34;) par(mfrow=c(2,2), oma=c(0,0,2,0)) plot(Stem_Scores, xlab = \u0026#34;Component 1\u0026#34;, ylab = \u0026#34;Component 2\u0026#34;, main=\u0026#34;Scores for Stem Cell data\u0026#34;) plot(Stem_Scores[,3], Stem_Scores[,4], xlab = \u0026#34;Component 3\u0026#34;, ylab = \u0026#34;Component 4\u0026#34;, main=\u0026#34;Scores for Stem Cell data\u0026#34;) plot(Scores, xlab = \u0026#34;Component 1\u0026#34;, ylab = \u0026#34;Component 2\u0026#34;, main=\u0026#34;Scores for Full data\u0026#34;) plot(Scores[,3], Scores[,4], xlab = \u0026#34;Component 3\u0026#34;, ylab = \u0026#34;Component 4\u0026#34;, main=\u0026#34;Scores for Full data\u0026#34;) dev.off() # For loadings png(file=\u0026#34;ex2_compare_loadings.png\u0026#34;,res=300, width = 3000, height = 3000, units = \u0026#34;px\u0026#34;) par(mfrow=c(2,2), oma=c(0,0,2,0)) plot(Stem_loadings, xlab = \u0026#34;Component 1\u0026#34;, ylab = \u0026#34;Component 2\u0026#34;, main=\u0026#34;Loadings for Stem Cell data\u0026#34;, xlim = c(-1.2,0.3)) text(Stem_loadings,labels=colnames(table),pos=c(2,2,2,4,4,2)) plot(Stem_loadings[,3], Stem_loadings[,4], xlab = \u0026#34;Component 3\u0026#34;, ylab = \u0026#34;Component 4\u0026#34;, main=\u0026#34;Loadings for Stem Cell data\u0026#34;, xlim = c(-0.6,0.3)) text(Stem_loadings[,3], Stem_loadings[,4],,labels=colnames(table),pos=c(2,2,2,4,4,2)) plot(loadings, xlab = \u0026#34;Component 1\u0026#34;, ylab = \u0026#34;Component 2\u0026#34;, main=\u0026#34;Loadings for Full data\u0026#34;, xlim = c(-0.6,-0.1)) text(loadings,labels=colnames(table),pos=c(2,2,2,4,4,2)) plot(loadings[,3], loadings[,4], xlab = \u0026#34;Component 3\u0026#34;, ylab = \u0026#34;Component 4\u0026#34;, main=\u0026#34;Loadings for Full data\u0026#34;, xlim = c(-0.45,0.1)) text(loadings[,3], loadings[,4], labels=colnames(table),pos=c(2,2,2,4,4,2)) dev.off() To interpret these graphs, we have to know that, to differentiate biomarkers, the key is the points that are separated from the rest, \u0026ldquo;marking\u0026rdquo; a differential expression. For example, in the PC1 vs PC2 loadings, we find that WOX5 is very clearly separated from the rest of the loadings in the Stem cell data graph, being located at the top left; as this does not happen in the full data, we can see that it is indeed a biomarker of this expression. We can also see that S18 and E30 tend to be located separately from the others in most loadings: this makes sense, as they are the genes with the highest differential expression, and therefore those for which it would be easiest to find biomarkers. To do this, we go to the Scores graph, where, if we see a point that corresponds to the loadings in terms of its separation from the rest, we can conclude that the marker it represents is specific to that gene (although we cannot know exactly which one because we have omitted the labels to avoid further blurring the image). This happens, apart from WOX5, for S18 and E30, which are the ones for which we can find biomarkers; not surprisingly, it was precisely S18 and E30 which were overexpressed in our original data, which is, indeed, why we can find biomarkers for them.\n","date":"February 1, 2022","hero":"/es/posts/master-en-biolog%C3%ADa-computacional/gdav/single-cell-genomics/exercise-2/hero.png","permalink":"https://www.pablomarcos.me/es/posts/master-en-biolog%C3%ADa-computacional/gdav/single-cell-genomics/exercise-2/","summary":"We have been given a file, \u0026lsquo;TableS5.csv\u0026rsquo;, which contains the data for a series of assays on gene expression with multiple samples. Our goal is to perform a Principal Component Analysis across the samples, to try and find out if we can relate said gene expresion profiles to a series of known biomarkers.\n# Set up images knitr::opts_chunk$set(dev = \u0026#34;png\u0026#34;, dpi = 1000, echo = FALSE, out.width = \u0026#34;450px\u0026#34;, cache = TRUE) #install.","tags":null,"title":"GDAV Single Cell - Ex 2"},{"categories":null,"contents":"Seurat is an R package designed for QC, analysis, and exploration of single-cell RNA-seq data. Seurat aims to enable users to identify and interpret sources of heterogeneity from single-cell transcriptomic measurements, and to integrate diverse types of single-cell data. In this exercise, we will learn to use it!\n## Set up images knitr::opts_chunk$set(dev = \u0026#34;png\u0026#34;, dpi = 1000, echo = FALSE, cache = TRUE) Using the raw data in file “SCOplanaria.txt” we will:\n## First, we will import the necessary packages packages \u0026lt;- c(\u0026#34;Matrix\u0026#34;, \u0026#34;dplyr\u0026#34;, \u0026#34;Seurat\u0026#34;) ##for (x in packages) {install.packages(x)} lapply(packages, require, character.only = TRUE) Initialize a Seurat object The steps can be consulted below:\n## First, we read the data from the .txt file: table \u0026lt;- read.table(\u0026#34;./SCOplanaria.txt\u0026#34;, row.names = 1, header = FALSE) ## To save memory, and since the current dataset has lots of 0\u0026#39;s, we convert ## the table -\u0026gt; sparse matrix table \u0026lt;- as.matrix(table, sparse=TRUE) ## And, we create the Seurat object SCOplanaria \u0026lt;- CreateSeuratObject(counts = table, project = \u0026#34;SCOPlanaria\u0026#34;) ## We have created an RNA assay with 24883 features for 1871 cells. ## We do not want to filter, but, rather, to use all the available data ## We can inspect both the data propper SCOplanaria[[\u0026#34;RNA\u0026#34;]]@counts[1:5,1:5] ## And the meta-data SCOplanaria@meta.data [1:5,] ## Here, we can see the read counts per cell (nCount_RNA) and the total ## number of expressed genes per cell (nFeature_RNA) Pre-process the data Perform quality control For the QC, we would like to select only those samples that are acceptable in quality. For this, we can do a number of data visualizations: for example, we can show the meta.data matrix using a violin plot like this:\n# Violin plot and scatter plots are for data discovery only png(file=\u0026#34;ex3_violin_pre.png\u0026#34;,res=300, width = 5400, height = 5200, units = \u0026#34;px\u0026#34;) VlnPlot(object = SCOplanaria, features= c(\u0026#34;nFeature_RNA\u0026#34;, \u0026#34;nCount_RNA\u0026#34;), ncol = 2) dev.off() In this plot, each point represents an individual read, with the red area being the rotated kernel density plot for the values. As one can read in the title, the left plot represents the number of genes expressed per cell, with the one in the one on the right showing the number of reads per cell. At first sight, there seems to be more counts than features; this is because each gene can have one or more reads.\nNow, we can visualize the metadata with the GenePlot function, to try and see if there are any outliers which we should remove.\npng(file=\u0026#34;ex3_scatter_pre.png\u0026#34;,res=300, width = 5400, height = 5200, units = \u0026#34;px\u0026#34;) FeatureScatter(SCOplanaria, feature1 = \u0026#34;nFeature_RNA\u0026#34;, feature2 = \u0026#34;nCount_RNA\u0026#34;) dev.off() This function generates an scatter plot for our data, in which we can see that there is a 95% positive correlation, which is pretty high and actually statistically significant. We can try to improve this by removing the values with a nFeature_RNA over 1800, and those with a nCount_RNA \u0026lt; 4500.\n# Quality control png(file=\u0026#34;ex3_scatter_post.png\u0026#34;,res=300, width = 5400, height = 5200, units = \u0026#34;px\u0026#34;) SCOplanaria \u0026lt;- subset(SCOplanaria, subset = nFeature_RNA \u0026lt; 1500 \u0026amp; nCount_RNA \u0026lt; 4000) FeatureScatter(SCOplanaria, feature1 = \u0026#34;nFeature_RNA\u0026#34;, feature2 = \u0026#34;nCount_RNA\u0026#34;) dev.off() # Normalization png(file=\u0026#34;ex3_violin_post.png\u0026#34;,res=300, width = 5400, height = 5200, units = \u0026#34;px\u0026#34;) SCOplanaria \u0026lt;- NormalizeData(SCOplanaria, normalization.method = \u0026#34;LogNormalize\u0026#34;) normaldata \u0026lt;- colSums(SCOplanaria[[\u0026#34;RNA\u0026#34;]]@data) SCOplanaria \u0026lt;- AddMetaData (SCOplanaria, normaldata, col.name=\u0026#34;Normalized_Data\u0026#34;) VlnPlot(object = SCOplanaria, features= c(\u0026#34;nCount_RNA\u0026#34;, \u0026#34;Normalized_Data\u0026#34;), ncol = 2) dev.off() Here, we see that, while the correlation does not get any better, it also does not get worse, and there are in fact way less outliers, which is always desirable.\nNormalization We can normalize the gene expression for all cells by multiplying each cell\u0026rsquo;s baseline expression by a scale factor of 10,000, and log-transforming the result. The new values will be stored in [[\u0026ldquo;RNA\u0026rdquo;]]@data (as Seurat forces us to use an existing slot for the data), and can be visualized, using its equivalent violin plot(s), as follows:\nSCOplanaria \u0026lt;- NormalizeData(SCOplanaria, normalization.method = \u0026#34;LogNormalize\u0026#34;) normaldata \u0026lt;- colSums(SCOplanaria[[\u0026#34;RNA\u0026#34;]]@data) SCOplanaria \u0026lt;- AddMetaData (SCOplanaria, normaldata, col.name=\u0026#34;Normalized_Data\u0026#34;) VlnPlot(object = SCOplanaria, features= c(\u0026#34;nCount_RNA\u0026#34;, \u0026#34;Normalized_Data\u0026#34;), ncol = 2) As we can see, due to the normalization, the data is much more evenly split over the values.\nFeature selecion We would also like to separate highly variable genes (those that show differential expression across tissues, for example), focusing on them for downstream analysis; seurat does this by modelling the mean-variance relationship od single-cell data, returning 2000 of the most important features. Using this, we will identify the top10 most important genes, and plot them to better understand which they are:\npng(file=\u0026#34;ex3_feature_selection.png\u0026#34;,res=300, width = 2000, height = 2000, units = \u0026#34;px\u0026#34;) SCOplanaria \u0026lt;- FindVariableFeatures(SCOplanaria, selection.method = \u0026#34;vst\u0026#34;) best_hits \u0026lt;- head(VariableFeatures(SCOplanaria), 10) VariableFeaturePlot(SCOplanaria) # One can also add the labels using the following code; NoLegend because it was ugly LabelPoints(plot = VariableFeaturePlot(SCOplanaria), points = best_hits, repel = TRUE ) + NoLegend() dev.off() Scaling of the data Finally, we can center and scale the data into a linear model, assuring that the mean expression across cells is 0 and the variance is 1 and savung the results in SCOplanaria[[\u0026ldquo;RNA\u0026rdquo;]]@scale.data. This gives equal weight to all cells, ensuring there is no over-domination by highly-expressed cells.\nSCOplanaria \u0026lt;- ScaleData(SCOplanaria, features = rownames(SCOplanaria)) PCA Analysis After pre-processing the data, we can finally run the PCA analysis! To understand the results, first, we will group the genes based on behaviour and plot a heatmap with the 15 most variable genes. We can also plot the cells based on which component they belong to, and show the scores for each variant and component, with the following code:\n# Run the PCA itself SCOplanaria \u0026lt;- RunPCA(SCOplanaria, features = VariableFeatures(object = SCOplanaria)) # Some graphs we dont care much about DimHeatmap(SCOplanaria, dims = 1:9, cells = 500, balanced = TRUE) VizDimLoadings(SCOplanaria, dims = 1, reduction = \u0026#34;pca\u0026#34;) DimPlot(SCOplanaria, reduction = \u0026#34;pca\u0026#34;) Seurat may also plot the cells by their PCA scores, which would help us group genes by common characteristics (for example, gene families). To determine the number of components, seurat automatically calculates significancy values for N components and reruns the analysis until there isa good distribution of gene scores, in a method that was inspired by the JackStraw algorithm. There, significant components would be those with a strong enrichment of low p-values, represented as dots over a curved line.\nSCOplanaria \u0026lt;- JackStraw(SCOplanaria, num.replicate = 100) SCOplanaria \u0026lt;- ScoreJackStraw(SCOplanaria, dims = 1:20) # Draw JackStraw and ElbowPlot png(file=\u0026#34;ex3_jackstraw_1.png\u0026#34;,res=300, width = 2000, height = 2000, units = \u0026#34;px\u0026#34;) JackStrawPlot(SCOplanaria, dims = 1:15) dev.off() png(file=\u0026#34;ex3_elbowplot_1.png\u0026#34;,res=300, width = 1300, height = 2000, units = \u0026#34;px\u0026#34;) ElbowPlot(SCOplanaria) dev.off() We may also plot a traditional elbow plot, in which we show the standard deviation against the number of Principal Components. As it turns out, a sharp drop in SD occurs after PC 19, which means that, at said point of inflexion, adding an additional PC does not add much to the data; thus, we can keep 10 as our number of PCs.\nCell clustering Now that we have defined an adecuate number of Principal Components, we can run a clustering to see if, in fact, we can generate real groups of families of genes. This clusters can be visualized through non-linear dimensional reduction, and then plotted, like so:\nSCOplanaria \u0026lt;- FindNeighbors(SCOplanaria, dims = 1:10) SCOplanaria \u0026lt;- FindClusters(SCOplanaria, resolution = 0.5) SCOplanaria \u0026lt;- RunUMAP(SCOplanaria, dims = 1:10) DimPlot(SCOplanaria, reduction = \u0026#34;umap\u0026#34;) SCOplanaria \u0026lt;- RunTSNE(SCOplanaria, dims = 1:10) png(file=\u0026#34;ex3_dimpolot_10PCs_2.png\u0026#34;,res=300, width = 1300, height = 2000, units = \u0026#34;px\u0026#34;) DimPlot(SCOplanaria, reduction = \u0026#34;tsne\u0026#34;) dev.off() New Parameters To validate our results, we will reproduce the analysis by keeping only genes expressed in at least 3 cells and cells with at least 200 and at most 2500 genes, lognormalizing with a scale of 10000 and selecting the 3000 most variable features. We will also regress out any variability, and use PCs 1-5 with a resolution of 0.6\nNew_plan \u0026lt;- CreateSeuratObject(counts = table, project = \u0026#34;Planaria\u0026#34;, min.cells = 3, min.features = 200) New_plan \u0026lt;- subset(New_plan, subset = nFeature_RNA \u0026lt; 2500 \u0026amp; nFeature_RNA \u0026gt; 200) New_plan \u0026lt;- NormalizeData(New_plan, normalization.method = \u0026#34;LogNormalize\u0026#34;, scale.factor = 10000) New_plan \u0026lt;- AddMetaData(New_plan,colSums(New_plan[[\u0026#34;RNA\u0026#34;]]@data),col.name=\u0026#34;Norma Data\u0026#34;) New_plan \u0026lt;- FindVariableFeatures(New_plan, selection.method = \u0026#34;vst\u0026#34;, nfeatures = 3000) New_plan \u0026lt;- ScaleData(New_plan, features = rownames(New_plan)) New_plan \u0026lt;- RunPCA(New_plan, features = VariableFeatures(object = New_plan)) png(file=\u0026#34;ex3_elbowplot_2.png\u0026#34;,res=300, width = 1300, height = 2000, units = \u0026#34;px\u0026#34;) ElbowPlot(New_plan) dev.off() New_plan \u0026lt;- FindNeighbors(New_plan, dims = 1:5) New_plan \u0026lt;- FindClusters(New_plan, resolution = 0.6) New_plan \u0026lt;- RunUMAP(New_plan, dims = 1:5) DimPlot(New_plan, reduction = \u0026#34;umap\u0026#34;) New_plan \u0026lt;- RunTSNE(New_plan, dims = 1:5) DimPlot(New_plan, reduction = \u0026#34;tsne\u0026#34;) As we can see in the elbow plot, there is not much of a difference when we account for the small changes introduced now; in both cases, there are two possible elbows, at 5 and 10 clusters, respectively. This is good! It means our results are robust, and do not depend so much in discretionary parameters which could be misinterpreted. If we look at the graph itself, we will see that its shape is quite different, but mostly in the group shapes, since their size is quite similar. The two options, however, seem good enough, which reinforces the idea that our results are robust independently of the number of PCs chosen.\nFinding Biomarkers through Differential Gene Expression For each of the gene expression clusters that we have previously defined, we would like to find a series of associated biomarkers, to try and see whether they relate to a specific cellular function or not. For this, Seurat has a built-in function that analyzes each cluster with regards to all others, analyzing whether some biomarkers show a disproportionate expression in each cluster or not. Since, as we have already discussed in the previous section, 5 PCs seem to show better adjustment to the data than 10 PCs, we will use the data from our repetition for this analysis.\n## Calculate biomarkers New_plan.biomarkers \u0026lt;- FindAllMarkers(New_plan, only.pos = TRUE, min.pct = 0.25, logfc.threshold = 0.25) ## Select the best hits (we chose 5) best_hit_biomarkers \u0026lt;- (New_plan.biomarkers %\u0026gt;% group_by(cluster) %\u0026gt;% slice_max(n = 5, order_by = avg_log2FC)) ## And show them as a heatmap # And show them as a heatmap png(file=\u0026#34;ex3_heatmap.png\u0026#34;,res=300, width = 2000, height = 1500, units = \u0026#34;px\u0026#34;) DoHeatmap(New_plan, features = best_hit_biomarkers$gene) + NoLegend() dev.off() The results seem promising: for each gene cluster, from 0 to 9, we find that each one has a series of clearly differentiated markers (in yellow) and that, except in clusters 4-8-9 and 5-7, there are no duplicities, as they are marked on the diagonal. This indicates that we will surely be able to differentiate the gene clusters by their markers, and, therefore, that they probably have a differentiated biological function.\nShowing the expression distribution of these markers Finally, we can directly show the distribution of the expression of these markers between the different clusters, by means of both violin plots and feature plots. For them, we can choose between t-SNE and uMap as methods; since t-SNE very good at creating a single map that reveals structure at many different scales, we decided to use that algorithm for our dimensionality reduction.\n## First, we set the col names. Since R does not let us create a dictionary, ## we must declare two vectors: gene_names = c(\u0026#39;dd-Smed-v6-61-0\u0026#39;, \u0026#39;dd-Smed-v6-2178-0\u0026#39;, \u0026#39;dd-Smed-v6-298-0\u0026#39;, \u0026#39;dd-Smed-v6-1410-0\u0026#39;, \u0026#39;dd-Smed-v6-702-0\u0026#39;, \u0026#39;dd-Smed-v6-2548-0\u0026#39;, \u0026#39;dd-Smed-v6-9977-0\u0026#39;, \u0026#39;dd-Smed-v6-48-0\u0026#39;, \u0026#39;dd-Smed-v6-175-0\u0026#39;, \u0026#39;dd-Smed-v6-1161-1\u0026#39;) cell_identities = c(\u0026#39;Early epidermal progenitors\u0026#39;, \u0026#39;Late epidermal progenitors\u0026#39;, \u0026#39;Epidermis\u0026#39;, \u0026#39;Muscle progenitors\u0026#39;, \u0026#39;Muscle body\u0026#39;, \u0026#39;Neural progenitors\u0026#39;, \u0026#39;GABA neurons\u0026#39;, \u0026#39;Phagocytes\u0026#39;, \u0026#39;Parenchymal cells\u0026#39;, \u0026#39;Pigment\u0026#39;) # The full plots. Not of much use, but, if I dont save them # R just makes them hyper mega o sea super-super ugly # (gosh I hate R) png(file=\u0026#34;ex3_full_violinplots.png\u0026#34;, res=300, width = 5000, height = 4000, units = \u0026#34;px\u0026#34;) VlnPlot(New_plan, features = gene_names) dev.off() png(file=\u0026#34;ex3_full_featureplots.png\u0026#34;, res=300, width = 5000, height = 4000, units = \u0026#34;px\u0026#34;) FeaturePlot(New_plan, features = gene_names, reduction = \u0026#34;tsne\u0026#34;) dev.off() We can also show our renamed clusters based on the information we now know; since all of the data-processing steps are already done, this is just a visualization step, in which we get to see how our analysis went:\n## Rename the columns names(cell_identities) \u0026lt;- levels(New_plan) New_plan \u0026lt;- RenameIdents(New_plan, cell_identities) ## Or the feature/violin plot for some of the elements VlnPlot(New_plan, features = \u0026#39;dd-Smed-v6-9977-0\u0026#39;) FeaturePlot(New_plan, features = \u0026#39;dd-Smed-v6-9977-0\u0026#39;, reduction = \u0026#34;tsne\u0026#34;) VlnPlot(New_plan, features = \u0026#39;dd-Smed-v6-1999-0\u0026#39;) FeaturePlot(New_plan, features = \u0026#39;dd-Smed-v6-1999-0\u0026#39;, reduction = \u0026#34;tsne\u0026#34;) As we can see, it seems that the biomarker dd-Smed-v6-9977-0, related to cluster 4, is clearly related to muscle progenitors, its expression being highly localised in the lower left side of the image, as can be seen in the individual feature plot. However, for dd-Smed-v6-1999-0, mainly related to epidermal, phagocyte and GABA neuron progenitors, it seems that its expression is much more diffuse, which may be due both to the fact that it is a poorer quality cluster and that, as it contains so many features, it is easier to find dispersion.\nThis analysis could be done individually for each of the clusters, but for the purpose of the exercise at hand, I believe that this short description serves to show how to proceed in the clusters.\n# We can show the labeled dimplot names(cell_identities) \u0026lt;- levels(New_plan) New_plan \u0026lt;- RenameIdents(New_plan, cell_identities) png(file=\u0026#34;ex3_labeled_dimplot.png\u0026#34;, res=300, width = 3000, height = 2000, units = \u0026#34;px\u0026#34;) DimPlot(New_plan, reduction = \u0026#34;tsne\u0026#34;) dev.off() ","date":"February 1, 2022","hero":"/es/posts/master-en-biolog%C3%ADa-computacional/gdav/single-cell-genomics/exercise-3/hero.png","permalink":"https://www.pablomarcos.me/es/posts/master-en-biolog%C3%ADa-computacional/gdav/single-cell-genomics/exercise-3/","summary":"Seurat is an R package designed for QC, analysis, and exploration of single-cell RNA-seq data. Seurat aims to enable users to identify and interpret sources of heterogeneity from single-cell transcriptomic measurements, and to integrate diverse types of single-cell data. In this exercise, we will learn to use it!\n## Set up images knitr::opts_chunk$set(dev = \u0026#34;png\u0026#34;, dpi = 1000, echo = FALSE, cache = TRUE) Using the raw data in file “SCOplanaria.","tags":null,"title":"GDAV Single Cell - Ex 3"},{"categories":null,"contents":"Abstract The Rabies Virus is one of the most terrible viruses in existence today: not only has it been tormenting mankind for years, but to this day it continues to kill more than 59000 people a year, making it one of the most neglected zoonotic diseases in existence, and one that, because of its terrible consequences, continues to be relevant to this day. In this article, we attempt to elucidate, using solely computational methods, when it emmerged. Although the results are generally unremarkable and inadequate, we hope that this article will inspire others to improve our results, and serve as a guide on how to perform this kind of analysis.\nKeywords: rabies, zoonotic diseases, evolutionary biology, molecular clock\nIntroduction The rabies virus, a single-stranded, negative RNA virus of the Lyssavirus family,1 is one of the most terrible viruses currently in existence: not only has it been tormenting mankind for years, but to this day it continues to kill an estimated 59,000 people a year,2 representing one of the most neglected zoonotic diseases in existence, and one that, because of its terrible consequences, remains relevant to this day. Its symptoms include excitability, hydrophobia and death by cardio-respiratory arrest in the case of furious rabies, which represents 80% of cases; and radiant paralysis extending from the site of the bite to death in the case of paralytic rabies, which accounts for 20% of infections.3\nThe fact that this disease is still killing people today is particularly upsetting given that safe vaccines to prevent infection have been available since 1885,4 and is mainly due to two factors: on the one hand, the difficulty of access to vaccines in poor countries, such as India, one of the countries with the highest number of infections at present (35% of the global burden)3 and, on the other, to the fact that, being a disease of zoonotic transmission, it would not suffice to vaccinate all humans, but animals, such as dogs, in close contact with humans, would also have to be immunised, since their bites represent 99% of cases to date. 5\nIn this work, we aim to use computational tools and evolutionary biology to better understand the behaviour of the virus, its date of emergence and the possible recombinations present in its genome today, thus trying to elucidate the origin and time to the last common ancestor of this virus, which has been tormenting mankind for at least 4000 years.6\nMatherials and Methods To perform our analyses, we used a database of a series of Rabies lyssavirus sequences taken intermittently between 1963 and 2006, with the aim of (if these samples are representative of the total genetic variability of the species) discovering both their recombination and evolutionary rates, as well as the time to the most recent common ancestor.\nRabies Recombination Rates Viruses, like humans and, in general, all sexually reproducing organisms, can undergo recombination processes in their genetic material during reproduction. In the case of viruses, since they do not have gametes, this recombination occurs directly inside the host cells: when two viruses of the same species infect the same host at the same time, their polymerases can \u0026ldquo;confuse\u0026rdquo; each other due to the similarity of their genetic material (which is also very small and accessible), \u0026ldquo;jumping\u0026rdquo; between the DNA or RNA strands and causing recombinational events.7 Due to the high abundance and short lifecycle of viruses, the number of recombination events, and thus their interannual ratio, is expected to be high.\nTo calculate this, we used RDP, a Recombination Detection Program created by the University of Cape Town that automates the process in an interactive way, allowing it to use many methods and to explore recombination events in depth, although it only runs on MS Windows.8 To maximise reliability, we have instructed the program to run 7 different methods: RDP, GENECONV, Chimaera, MaxChi, BoolScan, SiScan and 3Seq, leaving out LARD and keeping the fast analysis options for BoolScan, SiScan to keep computation time affordable. As is usual in the field, and to compensate for the possibility that the peculiarities of a particular algorithm introduce artifacts not present in reality, we will only consider as valid those recombination events detected by at least 4 of our 7 methods; as we expect the number of recombination events to be high, this should not be a problem. We have also indicated that the genome type of the virus in question is linear,1 and we have selected 0.05 as the threshold value to mark a recombination as significant (this value is extrapolated to each method), using a Bonferroni correction of 100 permutations to avoid false positives.\nRegarding the output filtering methods, we have required topological evidence (a recombination can only be valid if there is a phylogenetic tree in which it fits), checking alignment consistency to avoid the effect of gaps and defining the clustering of recombinants as realistic (i.e. that parentals are in closely related phyla).\nOnce the total number of recombination events has been calculated, we will calculate their frequency by dividing that value by the number of years under study (43).\nEvolutionary rate and most recent common ancestor Another parameter that may be of interest is the evolutionary rate, which should not be confused with the mutation rate: while the latter only tells us about the raw mutations that appear in the genome of a virus (i.e. the failure rate of the polymerase), the former tells us which of these are permanently fixed in the genome, thus excluding reversions or other mutations that, for whatever reason, are not fixed. 9 This is of great interest because, for example, for a virus with a very high mutation rate, such as influenza, it can be more difficult to design an effective vaccine; it is also essential for designing biological clocks, allowing us to form plausible models of the evolutionary biology of the virus that shows us where and when it emerged, something that, as we have seen with CoViD-19 and the Wuhan market, is of enormous public interest.\nTo carry out this analysis, the first thing we need to check is that the data we use have an appropriate time structure. To check if this is true, we have generated a maximum likelihood prior tree using FastTree10 (as this method does not force temporal structure on the data), and opened it in TempEst, a tool for investigating the temporal signal and \u0026lsquo;clocklikeness\u0026rsquo; of molecular phylogenies.11 To ensure that the fit of the data is good, we have selected \u0026ldquo;best-fitting root\u0026rdquo; and fitted the data by R².\nHowever, the results of this analysis with TempEst are only preliminary, and we need a different method, which actually calculates phylogenies, to compare with this one and bring robustness to the analysis. To do this, we used BEAST, a Bayesian-statistics based tool that works by using Markov Chain Monte Carlo algorithms to find out the most likely real tree among a set of possible correlations of the data. In order to use it, we must perform a series of steps beforehand; first of all, we must generate a control file, under .xml format, which defines the options that the program will use, and whose generation process is facilitated by BEAUTi, a companion app to BEAST. In it, we have a series of options:\nIn the Tips section, we have indicated it how to parse dates appropriately, using \u0026ldquo;_\u0026rdquo; as a separator between host species and year.\nTo decide the nucleotide substitution model, essential for defining the biological clock that serves as the basis for evolutionary biology analyses, we have used JModelTest, a Java version of the popular ModelTest program presented in 2008 by David Posada.12 This program works by computing a series of likelihood scores, that is, the probabilities that the existing frequency of each base in our program follows a certain distribution or another. In our case, to calculate these likelihood scores we have selected the Maximum-Likelihood-optimised method, since it is the most accurate, although it is more computationally intensive. Once the distribution of the bases has been calculated, we can see which model is more efficient in describing them, using two methods: on the one hand, the Akaike Information Criterion corrected for small sample sizes (AICc); and, on the other hand, the Bayesian Information Criterion, which are the most accurate ones offered by JModelTest.13 These data will be transmitted to BEAUTi, and thus to BEAST, through the \u0026ldquo;Sites\u0026rdquo; tab.\nFor the \u0026ldquo;Clocks\u0026rdquo; tab, instead of using a strict molecular clock, the default option which assumes a constant rate of variation across branches and across the tree (and which is therefore too restrictive and unnatural due to possible variations in selection pressure), we used an \u0026ldquo;uncorrelated relaxed cock\u0026rdquo;, which allows for rate variation between branches using, in our case, a lognormal.\nFor the \u0026ldquo;Trees\u0026rdquo; tab, we have used the previous maximum likelihood tree generated for the TempEst section, converted to Newick format with the help of FigTree and using resolve_polytomy,10 a function of the python module ete3 (developed, among others, by Jaime Huerta-Cepas, professor of this Master (yay!)14), to ensure that the tree has only 2 branches per node (both are BEAST requirements). This should make the program not only more efficient, but also more accurate in predicting results.\nIn the \u0026ldquo;MCMC\u0026rdquo; tab, I selected 100.000.000 Markov Chains, logging every 1000 states. This is computationally intensive (although, thanks to the maximum likelihood tree, it only took 2 hours in our computer), but assures us that we will have more accurate results.\nHaving selected the adequate parameters, we save the .xml and run BEAST. The .log output might be analysed using Tracer, a program which will tell us the evolutionary rate and time to most recent common ancestor, and the .trees output was coerced into a single maximum clade credibility heights, with 0.85 posterior probability and median node heights using TreeAnotator, another BEAST companion program.\nResults Rabies Recombination Rates After running RDP, we found no recombination events taking place, as can be seen in Figure 1. This strongly contradicts our initial hypothesis, and we are not sure whether this is due to computational problems or biological reality; it could be, for example, that we have chosen very restrictive methods, although this is unlikely because of the variety of methods and because, for example, we have allowed gaps to be ignored, which gives more, not less, laxity to the program; It could also be that the input sequences are too similar to show recombination, or that the rhabdovirus polymerase is very specific, making errors difficult and lowering the recombination rate until it is impossible to detect; it may even be that the rabies virus is less likely to make multiple infections, and recombinations are therefore rarer to find.\nHowever, none of these explanations seems to be entirely satisfactory for our particular case; it is therefore up to future researchers to find a more appropriate explanation, perhaps using a database with more sequences, more individuals or more time periods.\n{ width=575px }\nEvolutionary rate and most recent common ancestor Regarding the temporal disposition of the data, the results yielded by TempEst are unpromising; even adjusting the root by R², R² remains dismally small, at 0. 016, as can be seen in Figure 2; furthermore, the x-intercept (in this case, the date on which the mutation originated) is given as 2378, which would indicate that the mutations are reversing over time, with a negative slope showing that this molecular works in the opposite way as it is customary; this is unlikely, and is probably due to the input tree being poorly rooted.\n{ width=450px }\nDespite these results, we have decided to continue with the analysis with BEAST, hoping for it to find a new root for the data that makes its time structure more logical and understandable. Thus,for the nucleotide substitution model, the two algorithms used by JModelTest, AICC and BIC, agree in the result, as can be seen in Figure 3, which suggests that the appropriate method to use would be a simple Jukes Cantor (JC), which assumes that all substitutions are equally possible, and does not use neither gammas nor invariants.\n{ width=90%}\nOnce the analysis with BEAST has been carried out with the rest of the specified parameters, and the logs have been read with Tracer, we obtain the values available in Table 1. As we can see, for all the statistics the Auto-Correlation Time (ACT) is very high, which makes it difficult to take these results as valid, as it indicates that the data are not sufficiently independent from each other. Moreover, in general, the Effective Sample Size (i.e. the number of Markov chains taken to generate the each statistic) is quite low for all the statistics, reaching its maximum for Likelihood (18464, which is low compared to the Total Sample Size of 900001 and thus makes it non-significant). And, finally, the traces of the different parameters, available in Figure 4, are quite erratic and unclear, confirming that it is difficult to make predictions from our data, and that, as TempEst indicated, these do not present temporal structure.\nStatistic Mean ESS Value Range 95% Interval ACT Time to Root 724.8 784.2 [308.043, 8198.001] [374.5495, 1191.7946] 114770 Age 1281.10 784.2 [-6192.00, 1697.956] [814.205, 1631.4505] 114770 CSR $1.882e^{-4}$ 636.7 [$1.506e^{-5}$, $3.9262e^{-4}$] [$9.253e^{-5}$, $2.891e^{-4}$] 1413700 Prior -1227.5 663.9 [-1405.244, -1171.111] [-1269.156, -1189.400] 135560 Likelihood -12277.6 18464.6 [-12318.22, -12249.41] [-12293.51, -12262.34] 4874.2465 \\centerline{Table 1: Statistics obtained using BEAST. For each row, the total number of samples is 90001}\n{ height=40%}\nTo make matters worse, we can see that the \u0026ldquo;Time to root\u0026rdquo; predicted by BEAST is 724,898, and so the predicted year of emergence is 1281; this, as we have commented in the introduction, does not fit with the historical record, which makes it even clearer that this analysis is not valid; since, although the \u0026ldquo;Value Range\u0026rdquo; includes more reasonable dates (from 6192 BC to 1697 AD) these are not within the 95% confidence interval. There is, however, one study that suggests that rabies emmerged 500-750 years ago,15 which if all other parameters were not so bad, might imply our conclussions are fine; it may simply be that our dataset is not representative of the virus as a whole, and that some mutation emmerged 724 years ago that we are detecting here; or, maybe, molecular clock does not follow an uncorrelated, lognormal pattern. Finally, for the Corrected Substitution Rate (CRS), we obtain a value of $1.8821e^{-4}$, with a very wide confidence interval of [$1.5036e^{-5}$, $3.9262e^{-4}$], and which is not even within the ranges established in previous literature ($2.32e^{-4}$ to $1.38e^{-3}$).16\nFinally, we can show the consensus tree, which is found in Figure 5 after being opened in FigTree:17\n{ height=40%}\nConclussions Although evolutionary biology is a discipline that is undoubtedly useful in the real world to understand more deeply how living things work, our attempts in this paper have not been very fruitful. On the one hand, we have not been able to find recombination events (and thus to estimate the recombination rate) in the rabies virus genome, which, no doubt, may be due to biological factors such as the functioning of the virus polymerases or to a meagre recombination rate that has not allowed us to detect events in 43 years; however, this is rare, and it is more likely that the samples were taken in a way that is not showing this natural variability, either because of their poor temporal structure or because of their similarity. Regarding the rate of evolution and time to the most recent common ancestor, our attempt to estimate these values using coalescence theory and molecular clocks has yielded the figures of 724 years and $1. 8821e^{-4}$ substitutions per site and year; although these values are moderately consistent with previous literature, and not entirely unreasonable, the foundations on which they are based are not the most solid, so, as in the first section, we believe that it would be appropriate for future research teams to study the question further, perhaps with more representative data for the species under study or with higher quality data.\nReferences This document, and the accompanying code, is availaible under the CC By SA 4.0 License, and was generated using pandoc: pandoc --pdf-engine=xelatex --highlight-style tango --biblio Bibliography.bib \u0026quot;Report.md\u0026quot; -o \u0026quot;MarcosLópez_PabloIgnacio_EVO_EX2.pdf\u0026quot;\nNishizono A, Yamada K. [Rhabdoviruses]. Uirusu 2012;62(2):183–96.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nRabies Information Cheatsheet [Internet]. [cited 2022 Jan 28];Available from: https://www.who.int/westernpacific/health-topics/rabies\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nRabies epidemiology and burden [Internet]. [cited 2022 Jan 28];Available from: https://www.who.int/activities/improving-data-on-rabies/rabies-epidemiology-and-burden\u0026#160;\u0026#x21a9;\u0026#xfe0e;\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nWhy the world is not yet rabies-free [Internet]. World Economic Forum [cited 2022 Jan 28];Available from: https://www.weforum.org/agenda/2015/09/why-the-world-is-not-yet-rabies-free/\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nFooks AR, Cliquet F, Finke S, Freuling C, Hemachudha T, Mani RS, et al. Rabies. Nature Reviews Disease Primers [Internet] 2017 [cited 2022 Jan 28];3(1):1–19. Available from: https://www.nature.com/articles/nrdp201791\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nTarantola A. Four Thousand Years of Concepts Relating to Rabies in Animals and Humans, Its Prevention and Its Cure. Tropical Medicine and Infectious Disease [Internet] 2017 [cited 2022 Jan 28];2(2):5. Available from: https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6082082/\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nStedman KM. Deep Recombination: RNA and ssDNA Virus Genes in DNA Virus and Host Genomes. Annual Review of Virology 2015;2(1):203–17.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nMartin DP, Murrell B, Golden M, Khoosal A, Muhire B. RDP4: Detection and analysis of recombination patterns in virus genomes. Virus Evolution [Internet] 2015 [cited 2022 Jan 28];1(1). Available from: https://academic.oup.com/ve/ve/article/2568683/RDP4:\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nPeck KM, Lauring AS. Complexities of Viral Mutation Rates. Journal of Virology [Internet] 2018 [cited 2022 Jan 28];Available from: https://journals.asm.org/doi/abs/10.1128/JVI.01031-17\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nChang J. FastTree [Internet]. Bioinformatics Workbook [cited 2022 Jan 28];Available from: https://bioinformaticsworkbook.org/phylogenetics/FastTree.html\u0026#160;\u0026#x21a9;\u0026#xfe0e;\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nRambaut A, Lam TT, Max Carvalho L, Pybus OG. Exploring the temporal structure of heterochronous sequences using TempEst (formerly Path-O-Gen). Virus Evolution [Internet] 2016 [cited 2022 Jan 28];2(1):vew007. Available from: https://academic.oup.com/ve/article-lookup/doi/10.1093/ve/vew007\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nPosada D. jModelTest: Phylogenetic model averaging. Molecular Biology and Evolution 2008;25(7):1253–6.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\njModelTest [Internet]. Evolution and Genomics Exercises [cited 2022 Jan 28];Available from: http://evomics.org/learning/phylogenetics/jmodeltest/7\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nHuerta-Cepas J, Serra F, Bork P. ETE 3: Reconstruction, Analysis, and Visualization of Phylogenomic Data. Molecular Biology and Evolution [Internet] 2016 [cited 2022 Jan 28];33(6):1635–8. Available from: https://academic.oup.com/mbe/article-lookup/doi/10.1093/molbev/msw046\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nHughes GJ. A reassessment of the emergence time of European bat lyssavirus type 1. Infection, Ge-netics and Evolution: Journal of Molecular Epidemiology and Evolutionary Genetics in Infectious Diseases 2008;8(6):820–4.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nHughes GJ, Orciari LA, Rupprecht CE. Evolutionary timescale of rabies virus adaptation to North American bats inferred from the substitution rate of the nucleoprotein gene. The Journal of General Virology 2005;86(Pt 5):1467–74.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nReleases · rambaut/figtree [Internet]. GitHub [cited 2022 Jan 28];Available from: https://github.com/rambaut/figtree/releases\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","date":"January 28, 2022","hero":"/es/posts/master-en-biolog%C3%ADa-computacional/biolog%C3%ADa-evolutiva/exercise-2/hero.png","permalink":"https://www.pablomarcos.me/es/posts/master-en-biolog%C3%ADa-computacional/biolog%C3%ADa-evolutiva/exercise-2/","summary":"Abstract The Rabies Virus is one of the most terrible viruses in existence today: not only has it been tormenting mankind for years, but to this day it continues to kill more than 59000 people a year, making it one of the most neglected zoonotic diseases in existence, and one that, because of its terrible consequences, continues to be relevant to this day. In this article, we attempt to elucidate, using solely computational methods, when it emmerged.","tags":null,"title":"Using Computational Methods to Analyse Rabies Virus Recombination and Mutation Rates"},{"categories":null,"contents":"The following is a list of the commands used for the Final Project of the \u0026ldquo;Genomic Data Analysis and Visualization\u0026rdquo; subject, from UPM\u0026rsquo;s Master in Computational Biology.\nProject Description Our lab has identified an hot spring in Iceland, which, in the spring, undergoes events of high temperature (~90 ºC), producing algae blooms. Our intention is to study this blooms, performing an in-depth genomic and metagenomic exploration of this singular ecosystem.\nMetagenomics Most abundant organisms and relative abundance As a first step, we run a shotgun metagenomic sequencing of the prokaryotic microbiome in two of kind of samples, obtained at different times: one sample taken during the high temperature episodes, and another right after them, when the temperature is back to normal and there is a bloom of algae. We extracted the DNA present in each sample, performing an Illumina sequencing which results can be accessed in GDAV\u0026rsquo;s server:\n# First, we connect to the server: ssh pablo.marcos.lopez@138.4.139.16 # We can list the contents of the final-project folder, where Illumina results are: ls ~/final_project/ # We found two files: # Forward and reverse reads from the high temperature sample ./metagenomics-hotspring-hightemp.1.fq.gz ./metagenomics-hotspring-hightemp.2.fq.gz # And Forward and reverse reads from the normal temperature sample ./metagenomics-hotspring-normaltemp.1.fq.gz ./metagenomics-hotspring-normaltemp.2.fq.gz First, we used the mOTUs tool1 to perform a taxonomic profiling of your samples, using config files to ensure that both forward and reverse strands are processed:\n# First, we create a results folder to store them: mkdir ~/Results/ mkdir ~/Results/Metagenomics/ # Navigate to the local files folder cd ~/final_project/ # Run the motus analysis for the High Temperature Samples: motus profile -f metagenomics-hotspring-hightemp.1.fq.gz -r metagenomics-hotspring-hightemp.2.fq.gz \\ -o ~/Results/Metagenomics/high_temperatures.motu # And for the Low Temperature Samples, too: motus profile -f metagenomics-hotspring-normaltemp.1.fq.gz -r metagenomics-hotspring-normaltemp.2.fq.gz \\ -o ~/Results/Metagenomics/normal_temperatures.motu To analyze the results, we can use mOTUs\u0026rsquo; official guide; one of the first warnings we get is that The length of the first 2500 reads is 151. It is suggested to quality control the reads before profiling; this might mean that Illumina\u0026rsquo;s sequencing was not of a good quality. Now, we would like to see what is the most abundant organism in high-temperature, and what is its relative abundance.\n# For this, we can first inspect the file: nano ~/Results/Metagenomics/high_temperatures.motu # We see that it has a header and some columns. We can first read the file removing said column in python: import pandas as pd # Do the necessary imports high_heat = pd.read_csv(\u0026#34;~/Results/Metagenomics/high_temperatures.motu\u0026#34;, skiprows=2, header = 0, sep = \u0026#34;\\t\u0026#34;) # And print only those organisms that are present: high_heat.sort_values(by=\u0026#34;unnamed sample\u0026#34;, ascending = False).query(\u0026#39;`unnamed sample` \u0026gt; 0\u0026#39;) As we can see if we run the code above, the most abundant organism is Aquifex aeolicus [ref_mOTU_v25_10705], with a relative abundance of 0.846838. That is, well, ¡huge! and comparable to none of the other values, which are all smaller than 0.15.\nAquifex aeolicus: an in-depth analysis This organism, Aquifex aeolicus, is a known species 2, an extreme heat-loving bacterium that is known for feeding on gases and inorganic chemicals. Its genome has previously been sequenced in whole, and it consists on 1,512 open-reading frames,3 4 which have been decoded as part of a Nature article,2 which may be consulted in NCBI.5 5 In general lines, is a chemolithoautotrophic, Gram-negative, motile, hyperthermophilic bacterium. It is the best detailed species of the genus aeolicus, named after it and because the genus was discovered in submarine volcanic vents near the Aeolian Islands, located north of Sicily.6 Like the other species of its genus, it is rod-shaped and about 4 μm in length and 0.5 in radius.5\nIts preferred growth conditions are between 85 and 95 ºC and with a pH between 7 and 9,7 and, although it requires oxygen to grow, it prefers microaerophilic conditions, such as those found in our pond as the temperature rises. Regarding its genome, as we have seen, it is completely sequenced, and there is reason to believe that it is one of the oldest species of bacteria, since it has many genes in common with both filamentous and archaea (more than 16% of the genome).8\nOther abundant species To find which are the most abundant species in the \u0026ldquo;normal temperature scenario, and how they compare with the ones in the high-temp scenario, we can run the following code:\nimport pandas as pd # Do the necessary imports # First, we can generate the normal temperature database. normal_temp = pd.read_csv(\u0026#34;~/Results/Metagenomics/normal_temperatures.motu\u0026#34;, skiprows=2, header = 0, sep = \u0026#34;\\t\u0026#34; ).sort_values(by=\u0026#34;unnamed sample\u0026#34;, ascending = False).query(\u0026#39;`unnamed sample` \u0026gt; 0\u0026#39;) normal_temp_filtered = normal_temp.query(\u0026#39;`unnamed sample` \u0026gt; 0.01\u0026#39;) # Here, we filter only for those with abundance \u0026gt; 0.01 (1%) normal_temp # If you are using IPython, display like this. If not, print() # We can recreate the previous database higher_temp = pd.read_csv(\u0026#34;~/Results/Metagenomics/high_temperatures.motu\u0026#34;, skiprows=2, header = 0, sep = \u0026#34;\\t\u0026#34; ).sort_values(by=\u0026#34;unnamed sample\u0026#34;, ascending = False).query(\u0026#39;`unnamed sample` \u0026gt; 0\u0026#39;) # Finally, we get the comparison: higher_temp.merge(normal_temp_filtered,how=\u0026#34;outer\u0026#34;, on=\u0026#34;#consensus_taxonomy\u0026#34; ).rename(columns = {\u0026#34;#consensus_taxonomy\u0026#34;:\u0026#34;Species\u0026#34;,\u0026#34;unnamed sample_x\u0026#34;:\u0026#34;High Temp. Abundance\u0026#34;, \u0026#34;unnamed sample_y\u0026#34;:\u0026#34;Normal Temp. Abundace\u0026#34;} ).fillna(0) As we can see in the code output, it most abundant species is Methanococcus maripaludis [ref_mOTU_v25_01426], with a relative abundance of 4.8%. Aquifex aeolicus [ref_mOTU_v25_10705] has a relative abiundance of 2.1%, and the rest of the unknown species, -1, have a relative abundace of 1.1%. As we can see, these three groups do not sum up to 100%, which indicates us that, at a normal temperature, there is a much bigger amount of diversity, which reduces tremendously when temperature grows; thus, this periods of high temperature act as a \u0026ldquo;bottleneck\u0026rdquo; on the population. This is because, at high temperatures, only Aquifex aeolicus and Methanococcus maripaludis remain at a significant relative level: with 84.68 and 13.17% of the bacterial population, respectively; the rest of the bacteria represent a measly 1.1% of the individuals.\nHere, we can introduce an interesting concept: $\\alpha$-diversity, also known as species richness, which is the diversity within a particular area or ecosystem, usually expressed by the number of species therein present.9 This can be calculated using the following code:\n# Apply the same code as in the old chunk and do: print(f\u0026#34;The a-diversity for the Higher-Temperature scenario is {len(higher_temp)}\u0026#34;) print(f\u0026#34;The a-diversity for the Normal-Temperature scenario is {len(normal_temp)}\u0026#34;) Since the $\\alpha$-diversity for the Normal-Temperature scenario is 230, whereas the Normal-Temperature scenario has only 16 species, we can conclude that, without a doubt, it is the Normal-Temperature scenario that presents a higher level of species richness.\nAn interesting detail, however, can be noted: there is no algae diversity here. This can be checked using the following code:\n# First, we generate the mOTUs databases: motus profile -f metagenomics-hotspring-hightemp.1.fq.gz -r metagenomics-hotspring-hightemp.2.fq.gz \\ -o ~/Results/Metagenomics/kingdoms_high.motu -k kingdom motus profile -f metagenomics-hotspring-normaltemp.1.fq.gz -r metagenomics-hotspring-normaltemp.2.fq.gz \\ -o ~/Results/Metagenomics/kingdoms_normal.motu -k kingdom # And then, we read them: tail -n+4 ~/Results/Metagenomics/kingdoms_high.motu | cat tail -n+4 ~/Results/Metagenomics/kingdoms_normal.motu | cat As one can see, on both cases, no Eukaryot was detected, with around 90% of the individuals in both cases being bacteria and the resting 9% being archea, with 1% being unclassified individuals (it is interesting to note that, in the High temperature sample, the Archaea proportion grows from 6% to 13% - this makes sense! Archaeas are more extremophilic than most bacteria!). Since algae are part of the Eukaryota kingdom, this means that no algae were detected. This makes sense, due to two things:\nOn the first hand, when we decided to do our first shotgun analysis, we picked only prokaryotic samples, which means that, of course, no algae where selected. Moreover, mOTUs works by identifying species based on their 16S ribosomic RNA sequences,1 which means that it will never be able to detect algae, Eukaryotic beings wich lack a rRNA 16S component, even if they were present in our shotgun analysis . In all, we can conclude that the normal temperature is the one with the most biodiversity, with a 20-fold increase in species counts telling us that, definitely, most bacterial species are bad at surviving at super high (~90 ºC) temperatures. That both A. aeolicus\u0026rsquo;s and Archaea\u0026rsquo;s relative abundance rises dramatically when the heat is turned on also tells us a lot about their hyperthermophilicity: they are the only ones able to prosper in such a terrible environment, with the rest of the microorganisms probably facing bottleneck speciation events.\nGenome Analysis Sample Analysis Having found an organism that is relatively abundant in high-temperature conditions, we would like to further characterize it, sequencing its whole genome and performing an RNAseq analysis of samples from both cultures at both high and normal temperature conditions, with two replicates each. After performing quality checks, and picking only high quality reads, we get 8 files, 2 replicates for 2 organisms in 2 different conditions. We can find out the number of reads in the following way:\nimport os # First, we uncompress the databases; we need to import os to do that from Bio import SeqIO # We will also import SeqIO for later # We create the output directory os.system(\u0026#34;mkdir ~/Results/Genome_Analysis/\u0026#34;) # And uncompress the files os.chdir(os.path.expanduser(\u0026#34;~/final_project/RNAseq\u0026#34;)) os.system(\u0026#39;for f in *.gz ; do gunzip -c \u0026#34;$f\u0026#34; \u0026gt; ~/Results/Genome_Analysis/\u0026#34;${f%.*}\u0026#34; ; done\u0026#39;) # We can now check the number of reads: for file in os.listdir(os.path.expanduser(\u0026#34;~/Results/Genome_Analysis\u0026#34;)): records = list(SeqIO.parse(os.path.join(os.path.expanduser(\u0026#34;~/Results/Genome_Analysis\u0026#34;),file), \u0026#34;fastq\u0026#34;)) print(f\u0026#34;Total reads for {file}: {len(records)}\u0026#34;) We find that all the high-temperature samples have the same number of reads (318693), as do all the normal-temperature ones (288742). These samples are paired-end reads, since we have two files, one forward and one reverse, for each read. This can be further confirmed by looking at the files themselves:\nhead -n 4 ~/Results/Genome_Analysis/normal02.r1.fq head -n 4 ~/Results/Genome_Analysis/normal02.r2.fq Reads Analysis By printing the first 4 lines (which correspond to the most abundant organism, AQUIFEX_00001_47, we can see that they follow Illumina\u0026rsquo;s FastQ format 10, where each individual has, at the end of their identifier, a 1 or a 2, depending on whether they are read from the forward or reverse strand. 11. To find out whether the sequences are all the same length or not, we can use the following code, which builds on our previous program, and was derived from StackOverflow:12\nimport os; from Bio import SeqIO; import statistics as stats for file in os.listdir(os.path.expanduser(\u0026#34;~/Results/Genome_Analysis\u0026#34;)): sizes = [len(rec) for rec in SeqIO.parse(os.path.join(os.path.expanduser(\u0026#34;~/Results/Genome_Analysis\u0026#34;),file), \u0026#34;fastq\u0026#34;)] if len(set(sizes)) \u0026lt;= 1: print(f\u0026#34;All sequences in file: {file} have an equal length of {sizes[0]}\u0026#34;) else: print(f\u0026#34;For file: {file}, the average size was {stats.mean(sizes)}, with a maximum of {max(sizes)} and a minimum of {min(sizes)}\u0026#34;) As we can see, all of the files have the same length, 100 base pairs. In Illumina paired-end sequencing, our reads are always presented 5\u0026rsquo; -\u0026gt; 3\u0026rsquo;, with both strands being sequenced in opposite directions in order to keep said direction; this means that one of the files (R1) will be forward-oriented, while the other will be reverse-oriented.13\nAdditional Comments Some additional comments can be made on our reads:\nimport os; from Bio import SeqIO; import collections; import statistics as stats for file in os.listdir(os.path.expanduser(\u0026#34;~/Results/Genome_Analysis\u0026#34;)): all_qualities = []; all_sequences = [] for record in SeqIO.parse(os.path.join(os.path.expanduser(\u0026#34;~/Results/Genome_Analysis\u0026#34;),file), \u0026#34;fastq\u0026#34;): all_qualities += record.letter_annotations[\u0026#39;phred_quality\u0026#39;] all_sequences += list(str(record.seq).strip(\u0026#34;\\n\u0026#34;)) relative_seqs = [(each, all_sequences.count(each) / len(all_sequences)) for each in set(all_sequences)] relative_qual = [(each, all_qualities.count(each) / len(all_qualities)) for each in set(all_qualities)] print(f\u0026#34;For file: {file}, the average quality was {stats.mean(all_qualities)}, and each base had a frequency of:\\n{relative_seqs}\\n\u0026#34;) As we can see, the average quality for our reads is 40, which is the maximum available in Illumina, and which, in the FastQs produced by this programme, is represented by an I.14 We can also observe that all bases have a more or less equal chance of appearing, with A/T being slightly more likely to appear in the genome than G/C (55.7% vs 44.3%, respectively), across temperatures and samples. This has surprised me, since, when I coded the solution, I expected to find a bigger proportion of G/C base pairs in high-temperature samples, as it is known G/C has a higher resistance to temperature than A/T due to having three hydrogen bonds, instead of two.15\nRead Mapping Alignment workflow After checking our reads, we would like to perform several downstream analyses, including variant calling and expression analysis. First, we will map out current reads to the existing genome using bwa, a Burrows-Wheeler Aligner program which efficiently aligns short sequencing reads against a large reference sequence such as the human genome, allowing mismatches and gaps.16 After doing this, we performed the alignment itself using MEM, a more modern algorithm that has the best performance among those offered by BWA.17 We can use the following piece of code:\n# To be able to work with our files, we copy them to a file with write permission: mkdir ~/Results/Read_Mapping/ cp ~/final_project/genome.fasta ~/Results/Read_Mapping/genome.fasta # We begin by creating an index of the reference genome bwa index ~/Results/Read_Mapping/genome.fasta # We create the alignments. Here, we are not using -t since we *intentionally* want to use **all possible threads** bwa mem ~/Results/Read_Mapping/genome.fasta ~/Results/Genome_Analysis/hightemp01.r1.fq ~/Results/Genome_Analysis/hightemp01.r2.fq \u0026gt; ~/Results/Read_Mapping/hightemp01.sam 2\u0026gt; ~/Results/Read_Mapping/hightemp01.err bwa mem ~/Results/Read_Mapping/genome.fasta ~/Results/Genome_Analysis/hightemp02.r1.fq ~/Results/Genome_Analysis/hightemp02.r2.fq \u0026gt; ~/Results/Read_Mapping/hightemp02.sam 2\u0026gt; ~/Results/Read_Mapping/hightemp02.err bwa mem ~/Results/Read_Mapping/genome.fasta ~/Results/Genome_Analysis/normal01.r1.fq ~/Results/Genome_Analysis/normal01.r2.fq \u0026gt; ~/Results/Read_Mapping/normaltemp01.sam 2\u0026gt; ~/Results/Read_Mapping/normaltemp01.err bwa mem ~/Results/Read_Mapping/genome.fasta ~/Results/Genome_Analysis/normal02.r1.fq ~/Results/Genome_Analysis/normal02.r2.fq \u0026gt; ~/Results/Read_Mapping/normaltemp02.sam 2\u0026gt; ~/Results/Read_Mapping/normaltemp02.err Samtools analysis and number of mappings Having the alignment done, we can analyze it using samtools, a suite of programs for interacting with high-throughput sequencing data.18 For this, we will first generate some binary files as input for the program, obtaining some stats using the flagstat module. This module essentially counts the number of flags for alignments for each FLAG type,19 telling us whether they passed Quality Control or not, and helping us run find out their numbers in general. If we run the following code:\nconda activate samtools # Activate the samtools-containing environment cd ~/Results/Read_Mapping/ # Go to the results folder, which makes it easier to process stuff # Generate BAM files, the binary versions of SAM files for file in *.sam; do samtools view -h -b \u0026#34;$file\u0026#34; \u0026gt; \u0026#34;${file%.sam}.bam\u0026#34;; done # And we generate stats for them using samtools for file in *.bam do echo \u0026#34; #### FLAGSTAT REPORT FOR $file #### \u0026#34; samtools flagstat $file echo -e \u0026#34;\\n\u0026#34; echo \u0026#34; ***** \u0026#34; echo -e \u0026#34;\\n\u0026#34; done \u0026gt; flagstats_report.txt conda deactivate # We can consult the results of the flagstat command easily using grep: cat flagstats_report.txt | grep -e \u0026#34;REPORT\u0026#34; -e \u0026#34;QC-passed\u0026#34; -e \u0026#34;read1\u0026#34; -e \u0026#34;read2\u0026#34; We get the following output:\n#### FLAGSTAT REPORT FOR hightemp01.bam #### 637890 + 0 in total (QC-passed reads + QC-failed reads) 318693 + 0 read1 318693 + 0 read2 #### FLAGSTAT REPORT FOR hightemp02.bam #### 637950 + 0 in total (QC-passed reads + QC-failed reads) 318693 + 0 read1 318693 + 0 read2 #### FLAGSTAT REPORT FOR normaltemp01.bam #### 577503 + 0 in total (QC-passed reads + QC-failed reads) 288742 + 0 read1 288742 + 0 read2 #### FLAGSTAT REPORT FOR normaltemp02.bam #### 577498 + 0 in total (QC-passed reads + QC-failed reads) 288742 + 0 read1 288742 + 0 read2 as we can see, there are always the same number of reads for read1 and read2, and the number of QC-passed reads ~= read1 + read2. We can also note that there are absolutely 0 QC-failed reads; this makes sense, since we have already specified in the last section that our reads are really, really high-quality. The number of records would thus be the number of QC-passed reads for each file, while the number of reads would be either read1 or read2, depending if we are talking about the forward or reverse reads. If read1 and read2 do not exactly equal the full number of records, this is because some of them were \u0026ldquo;supplementary\u0026rdquo;, that is, they couldn\u0026rsquo;t be paired in sequencing.\nWe can compare this numbers with the original samples:\nimport os; from Bio import SeqIO # We can now check the number of reads: for file in os.listdir(os.path.expanduser(\u0026#34;~/Results/Genome_Analysis\u0026#34;)): records = list(SeqIO.parse(os.path.join(os.path.expanduser(\u0026#34;~/Results/Genome_Analysis\u0026#34;),file), \u0026#34;fastq\u0026#34;)) print(f\u0026#34;Total reads for {file}: {len(records)}\u0026#34;) Which outputs:\nTotal reads for hightemp02.r1.fq: 318693 Total reads for normal02.r2.fq: 288742 Total reads for normal01.r1.fq: 288742 Total reads for hightemp02.r2.fq: 318693 Total reads for hightemp01.r2.fq: 318693 Total reads for hightemp01.r1.fq: 318693 Total reads for normal02.r1.fq: 288742 Total reads for normal01.r2.fq: 288742 As we can see, the numbers we get are absolutely equal, which is, of course, coherent with our previous statements. One might then ask, however: what are those \u0026ldquo;supplementary\u0026rdquo; alignments? According to the samtools documentation,20 these represent \u0026ldquo;chimeric alignments\u0026rdquo;, which are a set of linear alignments that do not have large overlaps, with one of them, decided at random, considered to be \u0026ldquo;representative\u0026rdquo;, with the rest being \u0026ldquo;supplementary\u0026rdquo; to it.\nWith respect to Copy number variation, by definition this requires the occurrence of repeated genome sections; however, as we have not found duplicates in either the readings or the mappings, it is difficult to believe that this type of analysis is possible in our data.\nVariant Calling Variant Calling workflow To find out whether a given variant is responsible for the huge uptick in Aquifex aeolicus [ref_mOTU_v25_10705] when temperature peaks, we will perform a variant calling analysis. First, we will sort the samtools bam files, since we want to use all of the samples. Then, we will merge all the mappings into a single file, transfering the headers to avoid information loss, and, finally, proceed with the variant calling itself, which uses bcftools mpileup and bcftools call, but not --ploidy 1, which does not work in out server (this will make the program assume all samples are diploid). We can also repeat the process with the files individually, so as to be sure no artefacts were introduced by the merge:\ncd ~/Results/Read_Mapping/ # Go to input dir to make things easier mkdir ~/Results/Variant_Calling/ # Create output dir conda activate samtools # First, we generate the sorted samtools binary files for file in *.bam; do samtools sort -@ 1 $file \u0026gt; \u0026#34;${HOME}/Results/Variant_Calling/${file%.bam}.sorted.bam\u0026#34;; done cd ~/Results/Variant_Calling/ # Merge all samtools files into one. We chose normal01.sorted.bam for the headers, but anyone is good, really: samtools merge -h normaltemp01.sorted.bam merged.bam normaltemp01.sorted.bam normaltemp02.sorted.bam hightemp01.sorted.bam hightemp02.sorted.bam # We neeed to have the reference genome indexed samtools faidx ~/Results/Read_Mapping/genome.fasta conda activate bcftools # Now,for the variant calling itself. Once again, I want to use **all threads** bcftools mpileup -f ~/Results/Read_Mapping/genome.fasta merged.bam \u0026gt; merged.vcf bcftools call -mv -Ob -o variant_calling_merged.bcf merged.vcf # We repeat everything separately bcftools mpileup -f ~/Results/Read_Mapping/genome.fasta normaltemp01.sorted.bam normaltemp02.sorted.bam hightemp01.sorted.bam hightemp02.sorted.bam \u0026gt; separate.vcf bcftools call -mv -Ob -o variant_calling_separate.vcf separate.vcf conda deactivate Number of variants With regards to the number of expected variants, I dont necessarily expect to find any, since the genome we are comparing the samples against is derived from the samples themselves, and Aquifex aeolicus\u0026rsquo;s emergence in high-temperature scenarios can be attributed simply to said bacteria being a hyperthermophile; however, some might be found, due to heterocigosis and intra-population variation in general. We can find out by running this code on the \u0026ldquo;separated\u0026rdquo; file, since we belive it will introduce less errors than the additional step of merging:\n# Find all the variants bcftools stats separate.vcf | grep -P \u0026#34;ST\\t|SN\\t|IDD\\t\u0026#34; | grep -vP \u0026#34;:\\t0\u0026#34; # And asses their quality (must be \u0026gt; 100) bcftools view variant_calling_separate.vcf | tail -4 | awk -F \u0026#34;\\t\u0026#34; \u0026#39;$6 \u0026gt; 100 {print $0}\u0026#39; # To find out Depth of Coverage, since bcftools is terrible at formatting, we print and do manual search in column 8 bcftools view variant_calling_separate.vcf | tail -4 As we can see, none there are only 3 variants detected, all of them being Single Nucleotide Polimorphisms, or SNPs; two of them are T\u0026gt;A substitutions, while one of them is a C\u0026gt;A change. No insertions or deletions (code IDD) were found. Of this variants, we can find out, by running the code above, that all have a quality higher than 100: 127 for the T\u0026gt;As, and a swooping 931 for the C\u0026gt;A. We can also find out their depth of coverage using the code above; in this case, we have only one with DP \u0026gt; 100, the C\u0026gt;A SNP, which has DP = 694.\nSeparated vs Merged: finding the best workflow for our use case We have re-runned the analysis for the merged file, and the results can be observed here:\nType Change Quality (Separated) Quality (Merged) Depth of Coverage (Separated) Depth of Coverage (Merged) Position SNP C\u0026gt;A 931 183 694 249 1265109 SNP T\u0026gt;A 127 36.41 30 12 1265734 SNP T\u0026gt;A 127 36.41 19 10 1265735 As we can see, in both cases we find the same number of variants (3), and of the same type (SNPs), with the only difference being the quality and depth parameters. Since the separated file shows greater quality and depth of coverage than the merged one, and since the improvement in computational time by merging the files is neglegible, I believe using the separated method is advisable, bringing more quality to out analysis.\nBest-Quality mutation The position on the best-quality mutation (C\u0026gt;A, the one with the highest quality and coverage values overall), which was extracted from the stats file and shown in the table, is 1265109. We can correlate this on the original genome\u0026rsquo;s gff file, using the code below; we will find that the affected gene is nifA, the gene for the nitrogen fixation protein A. This SNP could affect the gene in some different ways: if it is in the non-coding part of the gene (think, for example, in an intron), it will most likely have no effect at all, except if it appears in a Ribosome-binding site, or a promoter or repressor binding site, in which case it might affect basal expression of the protein. Whereas if it is in the coding part, two things could happen: either it is a synonymous mutation, which will produce the same aminoacid as the non-mutated version (this is specially common if the aminoacid is the third of a triplet); or, it might be non-synonymous, which might, and might not, create problems for the affected protein.\ncp ~/final_project/genome.gff ~/Results/Variant_Calling/genome.gff cd ~/Results/Variant_Calling/ cat genome.gff | awk -F \u0026#34;\\t\u0026#34; \u0026#39;{ if ((1265109 \u0026lt;= $5 ) \u0026amp;\u0026amp; (1265109 \u0026gt;= $4 )) print $0 }\u0026#39; Integrative Genomics Viewer Finally, we would like to re-generate the files, in this case, the different temperatures separately. This way, we can be sure if our variants are related to temperature or not, and we can compare the differential expression in different climates. Then, we will download the files to our local computer, and load them on the Integrative Genomics Viewer, an interactive program for such purposes.21\ncd ~/Results/Variant_Calling/; conda activate samtools # Merge files by temperature: samtools merge -h hightemp01.sorted.bam merged_hightemp.bam hightemp01.sorted.bam hightemp02.sorted.bam samtools merge -h normaltemp01.sorted.bam merged_normtemp.bam normaltemp01.sorted.bam normaltemp02.sorted.bam conda deactivate #### To do on the user\u0026#39;s local machine #### # Download appropriate files rsync -avuL --progress pablo.marcos.lopez@138.4.139.16:/home/pablo.marcos.lopez/Results/Read_Mapping/genome.fasta \u0026#34;$HOME/Documentos/Trabajos del cole/UNI/Master/GDAV/Entrega Final Parte 1/IGV/Reference_genome.fasta\u0026#34; rsync -avuL --progress pablo.marcos.lopez@138.4.139.16:/home/pablo.marcos.lopez/Results/Variant_Calling/merged_normtemp.bam \u0026#34;$HOME/Documentos/Trabajos del cole/UNI/Master/GDAV/Entrega Final Parte 1/IGV/merge_normtemp.bam\u0026#34; rsync -avuL --progress pablo.marcos.lopez@138.4.139.16:/home/pablo.marcos.lopez/Results/Variant_Calling/merged_hightemp.bam \u0026#34;$HOME/Documentos/Trabajos del cole/UNI/Master/GDAV/Entrega Final Parte 1/IGV/merge_hightemp.bam\u0026#34; # On IGV: # Genomes → Create .genome file → Introduce Reference_genome.fasta # Tools → Run igvtools... → Index → Select merge_normtemp.bam # Tools → Run igvtools... → Index → Select merge_hightemp.bam # File → Load from file → merge_normtemp.bam # File → Load from file → merge_hightemp.bam This is coherent with our previous analysis, which showed a C\u0026gt;A SNP.\nDifferential expression analysis Workflow Since we have the expression patterns for different temperature conditions, we can perform a differential expression analysis to try if we can find additional info about which genes are involved and in which way. To do this, first we need to index the sorted files using samtools (otherwise htseq-count will not work), and, then, we use htseq-count to count the number of reads within each feature.[^noauthor_htseq-count_nodate] We can then join the files by feature, and find out how they correlate:\nmkdir cd ~/Results/DEA/; cd ~/Results/Variant_Calling/ # Create directories # Index files using samtools conda activate samtools; for file in *.sorted.bam; do samtools index $file; done; conda deactivate # Generate counts files for file in *.sorted.bam; do htseq-count -i locus_tag -t CDS $file genome.gff \u0026gt; \u0026#34;${HOME}/Results/DEA/${file%.sorted.bam}.counts\u0026#34;; done # Join all counts files under a common header and in the appropriate dir cd ~/Results/DEA/; echo -e \u0026#34;Gene_ID NormTemp_R NormTemp_R2 HighTemp_R1 HighTemp_R2\u0026#34; \u0026gt; all.counts join normaltemp01.counts normaltemp02.counts | join - hightemp01.counts | join - hightemp02.counts \u0026gt;\u0026gt; all.counts sed -i \u0026#39;s/ /\\t/g\u0026#39; all.counts # Make the file a tsv Now that we have generated the counts for each read and each feature, we can use DESeq2, an R package, to generate the Differential Expression Analysis:\nlibrary(DESeq2) # Load the package counts = read.table(\u0026#34;all.counts\u0026#34;, header=T, row.names=1) # Load the counts table colnames = c(\u0026#34;Normal\u0026#34;,\u0026#34;Normal\u0026#34;,\u0026#34;High\u0026#34;,\u0026#34;High\u0026#34;) # And rename the column header my.design \u0026lt;- data.frame(row.names = colnames( counts ), group = c(\u0026#34;Normal\u0026#34;,\u0026#34;Normal\u0026#34;,\u0026#34;High\u0026#34;,\u0026#34;High\u0026#34;) ) # Create the experiment my.design$group2 \u0026lt;- as.factor(paste(my.design$Time,sep=\u0026#34;_\u0026#34;)) # And change column type to fit DSeq2 # Create DSeq Data set and add metadata to it dds \u0026lt;- DESeqDataSetFromMatrix(countData = counts, colData = my.design, design = ~ group + group:group) dds \u0026lt;- DESeq(dds); resultsNames(dds) # Create the results dataframe and omit NA\u0026#39;s res \u0026lt;- results(dds, contrast=c(\u0026#34;group\u0026#34;,\u0026#34;High\u0026#34;,\u0026#34;Normal\u0026#34;)); res = na.omit (res) # Plot a histogram hist(res$padj, breaks=100, col=\u0026#34;darkorange1\u0026#34;, border=\u0026#34;darkorange4\u0026#34;, main=\u0026#34;\u0026#34;) # Find out which are significant and save the result significant \u0026lt;- res[res$padj\u0026lt;0.01,]; write.csv(significant, \u0026#34;./significant_results.csv\u0026#34;) # And draw the MA plot plotMA(res) To be able to comment on them, we rsync the plots and extract the images:\n# On out local computer, we can do: rsync -avuL --progress pablo.marcos.lopez@138.4.139.16:/home/pablo.marcos.lopez/Results/DEA/Rplots.pdf \u0026#34;$HOME/Documentos/Trabajos del cole/UNI/Master/GDAV/Entrega Final Parte 1/DEA/Rplots.pdf\u0026#34; rsync -avuL --progress pablo.marcos.lopez@138.4.139.16:/home/pablo.marcos.lopez/Results/DEA/significant_results.csv \u0026#34;$HOME/Documentos/Trabajos del cole/UNI/Master/GDAV/Entrega Final Parte 1/DEA/significant_results.csv\u0026#34; rsync -avuL --progress pablo.marcos.lopez@138.4.139.16:/home/pablo.marcos.lopez/Results/DEA/annotated_genes.csv \u0026#34;$HOME/Documentos/Trabajos del cole/UNI/Master/GDAV/Entrega Final Parte 1/DEA/annotated_genes.csv\u0026#34; rsync -avuL --progress pablo.marcos.lopez@138.4.139.16:/home/pablo.marcos.lopez/Results/Functional_Analysis \u0026#34;$HOME/Documentos/Trabajos del cole/UNI/Master/GDAV/Entrega Final Parte 1/\u0026#34; pdftk ./DEA/Rplots.pdf cat 1 output \u0026#34;./DEA/Histogram.pdf\u0026#34; Pdftk ./DEA/Rplots.pdf cat 2 output \u0026#34;./DEA/MA_Plot.pdf\u0026#34; And, we can show them here:\np-adjacency histogram As we can see, the majority of the genes have a p-adjacency value of 1, which means that their expression is not significative; however, just a small number of genes on the right, have a differential expression of 0, which means that they have a significative differential expression. The fact that we get a strictly bimodal distribution is suspicious, as it does not seem realistic that all genes are distributed perfectly either in 0 or in 1; and, although it makes sense for our further analysis (as the p-values are not uniform), it would be worth investigating in more detail what is happening in the background.22 23\nDifferentially-expressed genes (annotated) The genes that show a statistically significant differential expression are:\nbaseMean log2FoldChange lfcSE stat pvalue padj AQUIFEX_01423 10,026.5 5.06 0.046 111.21 0.00000000000000 0.000000000000 AQUIFEX_01754 124.9 -10.41 1.454 -7.16 0.00000000000079 0.000000000203 AQUIFEX_01759 3,034.9 4.89 0.080 60.89 0.00000000000000 0.000000000000 AQUIFEX_01760 159.3 -10.76 1.451 -7.41 0.00000000000012 0.000000000037 AQUIFEX_01761 2,790.9 5.09 0.088 57.72 0.00000000000000 0.000000000000 __no_feature 300,054.4 0.14 0.003 43.65 0.00000000000000 0.000000000000 These can be annotated with the help of our .gff file:\nimport pandas as pd df = pd.read_csv(\u0026#34;../Variant_Calling/genome.gff\u0026#34;, skiprows=2, header=None, sep = \u0026#34;\\t\u0026#34;) source = pd.read_csv(\u0026#34;significant_results.csv\u0026#34;, header=0) new_df = pd.DataFrame(columns=[\u0026#34;ID\u0026#34;, \u0026#34;Name\u0026#34;, \u0026#34;Description\u0026#34;, \u0026#34;log2FoldChange\u0026#34;,\u0026#34;padj\u0026#34;, \u0026#34;pvalue\u0026#34;]) pd.options.display.max_colwidth = 1000 for element in source[\u0026#39;Unnamed: 0\u0026#39;]: minidf = df[df[8].str.contains(element)] description_vect = minidf[8].to_string().split(\u0026#34;;\u0026#34;) if (len(description_vect) \u0026gt; 0): name = product = \u0026#34;-\u0026#34; for each in description_vect: if \u0026#34;Name\u0026#34; in each: name = each.split(\u0026#34;=\u0026#34;)[1] if \u0026#34;product\u0026#34; in each: product = each.split(\u0026#34;=\u0026#34;)[1] values = source[source[\u0026#39;Unnamed: 0\u0026#39;]==element][[\u0026#34;log2FoldChange\u0026#34;,\u0026#34;pvalue\u0026#34;, \u0026#34;padj\u0026#34;]] row = ([element, name, product] + values.values.tolist()[0]) new_df.loc[len(new_df)] = row new_df.to_csv(\u0026#34;./annotated_genes.csv\u0026#34;, header=True, index=False) Which outputs:\nID Name Description log2FoldChange pvalue padj AQUIFEX_01423 nifA Nif-specific regulatory protein 5.06529 0 0 AQUIFEX_01754 - hypothetical protein -10.410 0.000000000000799 0.000000000203 AQUIFEX_01759 nifB FeMo cofactor biosynthesis NifB 4.89894 0 0 AQUIFEX_01760 - hypothetical protein -10.761 0.000000000000121 0.000000000037 AQUIFEX_01761 nifH1 Nitrogenase iron protein 1 5.09307 0 0 __no_feature - - 0.14319 0 0 MA Plot The MA plot is an auto-generated plot that is part of the DSeq2 package. In it, genes which meet the adjusted p-value treshold set in our script (0.01) are colored in blue and depicted as triangles, whether those that dont are colored in grey. We can also see a blue dot, which represents the \u0026ldquo;__no_feature\u0026rdquo; row.24 In all, we can conclude that there are 5 differentially expressed genes in our dataset: the three that present a \u0026ldquo;name\u0026rdquo; in the previous table (AQUIFEX_01423, AQUIFEX_01759 and AQUIFEX_01761) are over expressed under high-heat conditions, while the two that dont (AQUIFEX_01754 and AQUIFEX_01760) are under expressed.\nSince the null hypothesis in the DSeq package is that there is no differential expression across samples, and given that we have found the p-values to be pretty low (\u0026laquo;\u0026lt;0.01), we can conclude that there is, in fact, differential expression for genes AQUIFEX_01423, AQUIFEX_01759, AQUIFEX_01761, AQUIFEX_01754 and AQUIFEX_01760\nFunctional analysis Having found that some genes over express themselves in high-temperature conditions, we would like to see how this genes work, exactly. To do this, we will extract the sequences from the assembled proteome.faa and search for them using some known, high-quality databases, such as PFAM, PHMMER,eggNOG, KEGG, NCBI Blast, NCBI Taxonomy, STRING-DB, Uniprot, Ensembl or SMART. Thus:\nimport os, shutil from Bio import SeqIO os.mkdir(os.path.expanduser(\u0026#34;~/Results/Functional_Analysis/\u0026#34;)) os.chdir(os.path.expanduser(\u0026#34;~/Results/Functional_Analysis/\u0026#34;)) shutil.copy(os.path.expanduser(\u0026#34;~/final_project/proteome.faa\u0026#34;), os.path.expanduser(\u0026#34;~/Results/Functional_Analysis/proteome.faa\u0026#34;)) records = SeqIO.parse(os.path.expanduser(\u0026#34;~/Results/Functional_Analysis/proteome.faa\u0026#34;), \u0026#34;fasta\u0026#34;) SeqIO.write((seq for seq in records if seq.id in [\u0026#34;AQUIFEX_01423\u0026#34;,\u0026#34;AQUIFEX_01759\u0026#34;,\u0026#34;AQUIFEX_01761\u0026#34;]), \u0026#34;./overexpressed_seqs.fasta\u0026#34;, \u0026#34;fasta\u0026#34;) Overexpressed genes\u0026rsquo; function To better understand the meaning of these sequences, the first step is to perform a BLAST analysis to see what they correspond to, and if the data we have in the gff file are correct. As we are interested in performing a functional analysis, we have decided to perform a blastp, a similarity analysis directly between the protein sequences, as we estimate that these will be better detailed than the genome as a whole if we were to perform, for example, a tblastn.\nFor AQUIFEX_01423, we found that there is a 100% match with sigma-54-dependent Fis family transcriptional regulator of both Aquifex aeolicus and Aquifex sp.. As it is a perfect match, and one with 100% coverage and e-value of 0, we can be sure that it is this gene. If we search for its Gene Ontolog in uniprot (accession WP_010881164) we obtain that it is in charge of ATP and DNA binding, regulating transcription; this is confirmed by Kegg, who indicates that it is a transcriptional regulator of the NifA subfamily, also related to ATPase AAA+.25\nFor AQUIFEX_01759, blastp gives 97% confidence to FeMo cofactor biosynthesis protein NifB, and 96% to radical SAM protein, both from Methanococcus maripaludis. If we check these proteins in uniprot and kegg, we see that, indeed, the two diagnoses are coincident, since it is a cofactor of the maturase NifB related to the apical meristem asembler protein.26\nFinally, for AQUIFEX_01761, we have a 100% coverage match with nitrogenase iron protein from several organisms, but mostly Methanococcus maripaludis. It is possible that this nitrogenase is highly conserved among several organisms, and that is why it is at the same time in Methanococcus maripaludis and, presumably, in Aquifex aeolicus, since the file we are analysing comes from it. Uniprot suggests that it is part of a 2-subunit complex responsible for N2-binding at the molecular level. Kegg adds that it could also be involved in energy metabolism, xenobiotic degradation and chloroalkane and chloroalkene degradation.27\nOverexpressed genes\u0026rsquo; known domains Regarding the presence of known domains, we can explore them using PFAM:28\nFor AQUIFEX_01423, we found two domains: GAF, a general domain found, among others, in cGMP-specific phosphodiesterases, adenylyl cyclases and FhlA; and Sigma_54_activat, an interaction domain between core RNA polymerase and ATP, allowing signal transduction. Finally, it contains HTH-8, a regulatory domain.\nFor AQUIFEX_01759 we find a single domain, radical sam, a superfamily of enzymes that use a [4Fe-4S]+ cluster to reductively cleave S-adenosyl-L-methionine to generate a radical.\nFor AQUIFEX_01761, we also found a single domain, Fer4_NifH, a family which includes the bacterial nitrogenase iron protein NifH, chloroplast encoded chlL , and archaeal Ni-sirohydrochlorin a,c-diamide reductive cyclase complex component.\nFunctional relations As we can see, all the overexpressed genes belong to the Nif family, which encodes proteins responsible for the fixation of atmospheric nitrogen to ammonium under conditions of low dissolved N2. Thus, when there is a lack of nutrients, NifA, AQUIFEX_01423, is responsible for activating the fixation, depetrolling the rest of the genes through NifC (AQUIFEX_01759), which, as we know, is a transcriptional regulator. The enzyme responsible for this is nitrogenase, and precisely one of its cofactors is encoded by one of the overexpressed genes (AQUIFEX_01761), so it is to be expected that its production will also increase.\nThus, we seem to have found a regulatory network of interrelated genes that is activated in response to high temperatures. This might make sense since gases dissolved in water become less soluble with increasing temperature,29 generating conditions of low O2 and N2 concentration that activate our system. This conclussion is supported by existing literature on how nitrogenase works.30\nAlgae Bloom It is likely that this system we have just described is related to the algal blooms observed in the hot spring after the high-temperature episodes, since DEA indicates that, after these episodes, the expression of genes involved in our regulatory system increases, favouring the development of Aquifex aeolicus, which begins to fix nitrogen. By the time the high-temperature episodes are over, the lake in which Aquifex grows will be full of nitrogen, which serves as the perfect natural fertiliser for uncontrolled algal growth.\nPhylogenetic analysis Closest ortholog of each overexpressed gene We can find the closest ortholog of each overexpressed gene by doing a BLAST analysis and looking at the first find from a different species:\nmkdir ~/Results/Phylo; cd ~/Results/Phylo makeblastdb -dbtype prot -in ~/final_project/all_reference_proteomes.faa -out blast_db for file in ~/Results/Functional_Analysis/*.fa; do blastp -query \u0026#34;$file\u0026#34; -db blast_db -evalue 0.001 -outfmt \u0026#34;6\u0026#34; -out \u0026#34;$(basename -- $file .fa)_result.tsv\u0026#34;; done For AQUIFEX_01423, the closesr orthologue is tr|A0A497XW95|A0A497XW95_9AQUI, a protein from the Hydrogenivirga caldilitoris species For AQUIFEX_01759, the closesr orthologue is tr|Q6LZH0|Q6LZH0_METMP, a protein from the Methanococcus maripaludis species For AQUIFEX_01761, the closesr orthologue is sp|P0CW57|NIFH_METMP, a protein from the Methanococcus maripaludis species This orthology analysis supports our previous functional annotations: for AQUIFEX_ 01423, we get A0A497XW95_9AQUI, which is involved in ATP binding and DNA regulation, as expected; for AQUIFEX_01759, Q6LZH0_METMP is involved in Fe-Mo cofactor biosynthesis; and, for AQUIFEX_01761, the closesr orthologue, NIFH_METMP, is part of the nitrogenase iron protein, just as expected. The only difference being, of course, that they are from other species. Since only AQUIFEX_01423 produced an autohit (that is, a hit from A. aeolicus), we can assure that neither AQUIFEX_01759 nor AQUIFEX_01761 are present on NCBI\u0026rsquo;s public database.\nWith regards to each gene\u0026rsquo;s origins, for AQUIFEX_01423, it seems as if it is simply a A.aeolicus native gene, so its origin would be evolution. However, for both AQUIFEX_01759 and AQUIFEX_01761, it seems like they appeared via vertical or horizontal transfer from Methanococcus maripaludis. Since Methanococcus maripaludis was actually the most abundant species in high-heat condition, it seems like horizontal gene transfer happened at some point in time, helping A.aeolicus adapt to such hostile conditions, and therefore compete with the original Methanococcus maripaludis.\nPhylogenetic tree analysis Finally, we can perform a phylogenetic analysis, by using ete3 and SeqIO, to generate some trees:\nimport os; import pandas as pd; from Bio import SeqIO from ete3 import Tree for file in os.listdir(\u0026#34;./\u0026#34;): if file.endswith(\u0026#34;.tsv\u0026#34;): df = pd.read_csv(file, sep=\u0026#34;\\t\u0026#34;, header = None) records = SeqIO.parse(os.path.expanduser(\u0026#34;~/final_project/all_reference_proteomes.faa\u0026#34;), \u0026#34;fasta\u0026#34;) SeqIO.write((seq for seq in records if seq.id in list(df[1])), f\u0026#34;{file.split(\u0026#39;.\u0026#39;)[0]}.fa\u0026#34;, \u0026#34;fasta\u0026#34;) f = open(f\u0026#34;{file.split(\u0026#39;.\u0026#39;)[0]}.fa\u0026#34;, \u0026#34;a\u0026#34;) original_seq = open(f\u0026#34;../Functional_Analysis/{file.split(\u0026#39;_result\u0026#39;)[0]}.fa\u0026#34;) f.write(original_seq.read()) os.system(\u0026#39;for file in *_result.fa; do mafft $file \u0026gt; \u0026#34;$(basename -- $file .fa).aligned\u0026#34;; done\u0026#39;) os.system(\u0026#39;for file in *.aligned; do iqtree -s $file -m LG; done\u0026#39;) for file in os.listdir(\u0026#34;./\u0026#34;): if file.endswith(\u0026#34;.treefile\u0026#34;): t = Tree(file) t.set_outgroup(t.get_midpoint_outgroup()) f = open(f\u0026#34;{file.split(\u0026#39;.\u0026#39;)[0]}_final.tree\u0026#34;, \u0026#34;w\u0026#34;) f.write(t.write()) We then proceed to download the files:\nrsync --include=\u0026#34;*_final.tree\u0026#34; --exclude=\u0026#34;*.*\u0026#34; -arv pablo.marcos.lopez@138.4.139.16:/home/pablo.marcos.lopez/Results/Phylo \u0026#34;$HOME/Documentos/Trabajos del cole/UNI/Master/GDAV/Entrega Final Parte 1/\u0026#34; Which can be seen here:\nConclussions Through a detailed and multifaceted analysis of the genome of the species present in our variable temperature environment, we can establish a clear account of what is going on: most of the time we will find a pond at room temperature, perfect for a large number of organisms and therefore presenting high alpha-biodiversity. However, periodically, we will find \u0026ldquo;ultra-high temperature\u0026rdquo; phenomena, in which such brutal changes occur (both in the temperature itself and in the concentration of dissolved gases in the water) that lead to bottleneck phenomena where only a few individuals of each species survive, unless they are adapted to high temperatures, as is the case of Methanococcus maripaludis. In one of these events, which usually lead to speciation, some bacterium of the species Aquifex aeolicus must have acquired, perhaps by lateral transfer, the genes that allow M. maripaludis to fix nitrogen from the air, thus gaining the competitive advantage that it previously had exclusively, and which allows it to survive in these conditions, since, although Aquifex aeolicus is already resistant to high temperatures, it might not have been so resistant to the low concentration of dissolved gases that these entail until the mutation occured.\nThen, when temperatures return to normal, gas concentration returns to normal, and alpha-biodiversity returns to its default values (since they had dropped during the bottleneck), thanks to the few individuals that survived the event, perhaps through sporulation or simple luck. However, due to the ability of both M. maripaludis and A. aeolicus (acquired) to fix nitrogen, the water will be full of this nutrient, providing the perfect fertiliser for a temporary algal bloom, at least until all dissolved nitrogen is depleted.\nThanks to our analyses, we have been able to determine that the genes involved in this nitrogen fixation would be NifA, NifB and NifH, such that NifA is responsible for recruiting, by phosphorylating the necessary sites in the DNA, the FeMo cofactor (NifB) necessary to synthesise extra nitrogenase (NifH) and thus proceed with the fixation.\nAfter our computational analysis, it is now up to future researchers to confirm the effect of the mutations (SNPs) detected in the aforementioned genes on nitrogen assimilation by A. aeolichus, in order to confirm, by in vivo analysis, our theories developed in silico.\nReferences Milanese A, Mende DR, Paoli L, Salazar G, Ruscheweyh H-J, Cuenca M, et al. Microbial abundance, activity and population genomic profiling with mOTUs2. Nat Commun 2019;10(1):1014.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nDeckert G, Warren PV, Gaasterland T, Young WG, Lenox AL, Graham DE, et al. The complete genome of the hyperthermophilic bacterium Aquifex aeolicus. Nature 1998;392(6674):353–8.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nAquifex aeolicus VF5, complete sequence - Nucleotide - NCBI [Internet]. [cited 2022 Jan 31];Available from: https://www.ncbi.nlm.nih.gov/nuccore/NC_000918.1\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nAquifex aeolicus VF5 plasmid ece1, complete sequence - Nucleotide - NCBI [Internet]. [cited 2022 Jan 31];Available from: https://www.ncbi.nlm.nih.gov/nuccore/NC_001880.1\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nDeckert G, Warren PV, Gaasterland T, Young WG, Lenox AL, Graham DE, et al. The complete genome of the hyperthermophilic bacterium Aquifex aeolicus. Nature 1998;392(6674):353–8.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\u0026#160;\u0026#x21a9;\u0026#xfe0e;\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nGupta RS, Lali R. Molecular signatures for the phylum Aquificae and its different clades: proposal for division of the phylum Aquificae into the emended order Aquificales, containing the families Aquificaceae and Hydrogenothermaceae, and a new order Desulfurobacteriales ord. nov., containing the family Desulfurobacteriaceae. Antonie van Leeuwenhoek 2013;104(3):349–68.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nGuiral M, Giudici-Orticoni M-T 2021. Microbe Profile: Aquifex aeolicus: an extreme heat-loving bacterium that feeds on gases and inorganic chemicals. Microbiology 167(1):001010.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nReysenbach AL, Wickham GS, Pace NR. Phylogenetic analysis of the hyperthermophilic pink filament community in Octopus Spring, Yellowstone National Park. Appl Environ Microbiol 1994;60(6):2113–9.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n7: Alpha, Beta, and Gamma Diversity [Internet]. Biology LibreTexts2018 [cited 2022 Jan 31];Available from: https://bio.libretexts.org/Bookshelves/Ecology/Biodiversity_(Bynum)/7%3A_Alpha_Beta_and_Gamma_Diversity\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nFASTQ files explained [Internet]. [cited 2022 Jan 31];Available from: https://emea.support.illumina.com/bulletins/2016/04/fastq-files-explained.html\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nbioinformatics - How to check if a fastq file has single or paired end reads [Internet]. Biology Stack Exchange [cited 2022 Jan 31];Available from: https://biology.stackexchange.com/questions/15502/how-to-check-if-a-fastq-file-has-single-or-paired-end-reads\u0026#160;\u0026#x21a9;\u0026#xfe0e;\npython - Parsing fasta file with biopython to count number sequence reads belonging to each ID [Internet]. Stack Overflow [cited 2022 Jan 31]; Available from: https://stackoverflow.com/questions/31540845/parsing-fasta-file-with-biopython-to-count-number-sequence-reads-belonging-to-ea\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nOrientation in paired-end sequencing? [Internet]. [cited 2022 Jan 31];Available from: https://www.biostars.org/p/103773/\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nQuality Score Encoding [Internet]. [cited 2022 Jan 31];Available from: https://support.illumina.com/help/BaseSpace_OLH_009008/Content/Source/Informatics/BS/QualityScoreEncoding_swBS.htm\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nWu H, Zhang Z, Hu S, Yu J. On the molecular mechanism of GC content variation among eubacterial genomes. Biology Direct 2012;7(1):2.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nFast and accurate short read alignment with Burrows–Wheeler transform | Bioinformatics | Oxford Academic [Internet]. [cited 2022 Jan 31];Available from: https://academic.oup.com/bioinformatics/article/25/14/1754/225615\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nBurrow-Wheeler Alignment tool Source Code [Internet]. [cited 2022 Jan 31];Available from: http://bio-bwa.sourceforge.net/bwa.shtml\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nSamtools [Internet]. [cited 2022 Jan 31];Available from: http://www.htslib.org/\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nsamtools-flagstat manual page [Internet]. [cited 2022 Jan 31];Available from: http://www.htslib.org/doc/samtools-flagstat.html\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nThe SAM/BAM Format Specification Working Group. Sequence Alignment/Map Format Specification [Internet]. Github; 2021. Available from: https://samtools.github.io/hts-specs/SAMv1.pdf\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nIGV: Integrative Genomics Viewer [Internet]. [cited 2022 Jan 31];Available from: https://igv.org/\u0026#160;\u0026#x21a9;\u0026#xfe0e;\np-Value Histograms: Inference and Diagnostics [Internet]. [cited 2022 Jan 31];Available from: https://www.mdpi.com/2571-5135/7/3/23\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nHow to interpret a p-value histogram – Variance Explained [Internet]. [cited 2022 Jan 31];Available from: http://varianceexplained.org/statistics/interpreting-pvalue-histogram/\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nAnalyzing RNA-seq data with DESeq2 [Internet]. [cited 2022 Jan 31];Available from: http://bioconductor.org/packages/devel/bioc/vignettes/DESeq2/inst/doc/DESeq2.html#ma-plot\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nntrC2 - Transcriptional regulator (NtrC family) - Aquifex aeolicus (strain VF5) - ntrC2 gene \u0026amp; protein [Internet]. [cited 2022 Jan 31];Available from: https://www.uniprot.org/uniprot/O67661\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nnifB - FeMo cofactor biosynthesis protein NifB - Methanococcus maripaludis - nifB gene \u0026amp; protein [Internet]. [cited 2022 Jan 31];Available from: https://www.uniprot.org/uniprot/A0A2L1C8R9\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nnifH - Nitrogenase iron protein - Methanococcus maripaludis (strain S2 / LL) - nifH gene \u0026amp; protein [Internet]. [cited 2022 Jan 31];Available from: https://www.uniprot.org/uniprot/P0CW57\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nMistry J, Chuguransky S, Williams L, Qureshi M, Salazar GA, Sonnhammer ELL, et al. Pfam: The protein families database in 2021. Nucleic Acids Research 2021;49(D1):D412–9.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nDissolved Gas Concentration in Water | ScienceDirect [Internet]. [cited 2022 Jan 31];Available from: https://www.sciencedirect.com/book/9780124159167/dissolved-gas-concentration-in-water\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nBurén S, Jiménez-Vicente E, Echavarri-Erasun C, Rubio LM. Biosynthesis of Nitrogenase Cofactors. Chem Rev 2020;120(12):4921–68.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","date":"January 20, 2022","hero":"/es/posts/master-en-biolog%C3%ADa-computacional/gdav/final-project/hero.png","permalink":"https://www.pablomarcos.me/es/posts/master-en-biolog%C3%ADa-computacional/gdav/final-project/","summary":"The following is a list of the commands used for the Final Project of the \u0026ldquo;Genomic Data Analysis and Visualization\u0026rdquo; subject, from UPM\u0026rsquo;s Master in Computational Biology.\nProject Description Our lab has identified an hot spring in Iceland, which, in the spring, undergoes events of high temperature (~90 ºC), producing algae blooms. Our intention is to study this blooms, performing an in-depth genomic and metagenomic exploration of this singular ecosystem.","tags":null,"title":"GDAV Final Project"},{"categories":null,"contents":"by Yaiza ARNAIZ ALCACER (251), Pablo MARCOS LOPEZ (269), Alexandre VERGNAUD (178) and Lexane LOUIS (179)\nIntroduction Advanced statistics, through methods such as Principal Component Analysis, Factor Analysis and Correspondence Analysis, allows us to find the most important variables in a dataset, facilitating its understanding and the making of decisions and conclusions. Throughout the following 3 exercises, we will demonstrate the use of these three methods in different fields, showing their usefulness and applications.\nExercise 1 In this first exercise, our aim is to analyze the different species of phytoplankton algae present in lakes of different regions at two different times each year. Using PCA and descriptive statistics, we want to find out if the different parameters present in our data are good predictors species type distribution over time, and, if possible, to determine which of these components are the most important in this distribution.\nTo do this, the first step is to detect whether the different species are correlated; and, if so, to see how significant said correlation is. To do this, we created Table 1, which shows the correlation matrix between the different species using the p-value of for the pearson correlation quotient adjusted by the Holm-Bonferroni method, which corrects the family-wise error rate (the probability of finding false positives) in multiple hypothesis tests; helping us discern only trully correlated variables. Estimating an acceptable p-value of \u0026lt; 0.05, as is usual in statistics, we obtain a correlation, in decreasing order of significance, between the following species: Cianophyceae and Chlorocococales, Zygophyceae and Chlorococales, Euglenophyceae and Cianophyceae; and Euglenophyceae and Bacillariophyceae.\nBacillariophyceae Chlorococales Cianophyceae Euglenophyceae Zygophyceae Bacillariophyceae 1.0000 1.0000 0.0049 0.0853 Chlorococales 1.0000 \u0026lt;.0001 0.7839 0.0269 Cianophyceae 1.0000 \u0026lt;.0001 0.0343 1.0000 Euglenophyceae 0.0049 0.7839 0.0343 1.0000 Zygophyceae 0.0853 0.0269 1.0000 1.0000 Matrix of Pearson correlations showing the pairwise two-sided p-values adjusted by Holm's method. As it is shown, the correlation is significant for the following pairs: Euglenophyceae and Bacillariophyceae, Euglenophyceae and Cianophyceae and Zygophyceae and Chlorococales; and highly significant for the pair Cianophyceae and Chlorococales. Now that we are sure about possible correlations, we would like to know about the dimensionality of our data, and, therefore, its complexity. Thus, we performed a Principal Component Analysis using the covariance matrix of our different types of bacteria. In Table 2, we can see that, indeed, 71.6% of the variability in our data can be explained by looking at just 2 components. Now, what are these 2 components? One possible theory is that they are, for example, the 2 annual periods in which we have sampled the data. Therefore, we will work only with these components, trying to clarify whether their relevance is indeed due to the fact that they represent the different periods of the year or not.\nComponent 1 Component 2 Component 3 Component 4 Component 5 Proportion of Variance 0.4903891 0.2258744 0.1574135 0.08693947 0.03938336 Proportion of Variance explainaible by each component after PCA of the data.\nAnother way to approach this problem is through a sedimentation plot. These figures, which represent the eigenvalues, usually show a \u0026quot;bend\u0026quot;, a bend between the most important components and those that are not so interesting. Such a bend is, of course, a relative thing, as there is no \u0026quot;set in stone\u0026quot; definition;1 but, in Figure 1, we would argue that it is precisely the first two components (especially the first, although the second is more subjective) that are above the curve. We therefore consider our selection validated.\nFigure 1. The PCA barplot (principal component analysis) of the different components, shows how the 2 first PCs (principal components) explain the 71.6% of the variability of the data.\nWe can also assess which are the most interesting variables for each component using a loading plot, which interprets the values of the coefficients (loadings), so that those variables furthest away from the origin of coordinates and the centre of the graph are those that will provide more information for each component. In Figure [first_second]{reference-type=\u0026ldquo;ref\u0026rdquo; reference=\u0026ldquo;first_second\u0026rdquo;}, we see that the most important variables for the first component selected are Chlorococales and Euglenophycea, while for the second, these would be Bacilloriaphyceae and Chlorocales.\nFigure 1. The PCA plot of the loadings shows the variables more related to: A) The first PC: Chlorococales, Cianophyceae and Euglenophyceae; B) The second PC: Bacilloriaphyceae and Chlorocales.\nFinally, we can use various scatter plots to represent the selected components across regions, lakes and time-spans. In Figure 3 a), we can see that Component 1 of the PCA explains the difference between two biomes: on the one hand, the Andean and Caribbean regions; and, on the other hand, the Amazon region. In subfigure c) we see that a clear differentiation for PC2, which allows us to separate the two time periods (high and low tides), as we had previously predicted. For subgraph b), the interpretation is more complicated: it seems that the separations between lakes Iguaque and Tota, Purisima, Momil and Fuquequeque are explained by PC2, while the difference between lakes Purisima and Tota and lake Tarapoto is explained by PC1. This, on the one hand, agrees with our initial hypothesis, since PC1 and PC2 allow us to distinguish the time of sampling; but, on the other hand, these components allow us to explain even more differences: after all, as we have explained, they account for 71% of the total variance.\nExercise 2 For this second exercise, we would like to analyze the limnological properties of several neotropical lakes in order to find out their productivity degree and adequacy for life, which, a priori, seem like they might be related to values such as nutrient concentration, depth, pH or amount of dissolved oxygen. We therefore seek to use a multifactor analysis to check if we can find a suitable model to explain the data. To do this, we will, again, see if the variables correlate with each other. Thus, we made a new table, Table 3, of the p-values of the correlation coefficients fitted by Holm\u0026rsquo;s method. Although the correlations are too numerous to detail in writing in this report, the correlation values between the variables are, in general, rather poor, which is to be expected: it is unlikely, for example, that the amount of a certain gas in the lake influences the others, beyond certain isolated cases. 2\nTo understand how these variables influence the productivity degree and the adequacy for life, we carried out a Factor Analysis, which works in the opposite way to PCA: instead of seeking to create \u0026quot;new components\u0026quot;, as if it were the greatest common divisor, it seeks to find a least common multiple, a subset of factors common to all the data which are implicit, neither observable nor directly measurable. In this way, we hope to see whether the model we have devised, in which different lake concentration variables are related to fitness to support life, fits reality or not. We would expect concentration of nutrients and dissolved organic carbon to be correlated with productivity degree, with the \u0026quot;chemicals\u0026quot; determining if the lake is adequate for life or not.\nOne of the ways we can find out whether or not the system does indeed fit the two-factor model we have chosen is to use the Kaiser-Meyer-Olkin, or sample adequacy, index. In our case, this takes a value of 0.66, which means that the model, while not unacceptable, is only an approximate fit to the data.\nNext, we can analyse the results of trying this model, which a priori we have seen is not very good, on our data. To do this, we have two parameters: the loadings, which are the values of the fit coefficients; and the communalities, the sum of the squares of these loadings. According to the result of our analysis, which can be consulted in detail in Annex I - Code, there are two variables with low uniqueness: Conductivity (0.005) and Temperature (0.084), which will therefore have high communality, and respond satisfactorily to our model. However, the other nine variables have high uniqueness, especially pH (87.6%), NH4 (74.4%), Depth (74.2%), NO3 (72.4%) and PO4 (59.4%) which means that they will not be adequately explained by the factorial model because of their low uniqueness.\nAbout the loadings, we found that they can help us correlate Factor 1 with adequacy, and Factor 2 with productivity. However, pH doesn\u0026rsquo;t seem to correlate well with Factor 1, and the carbon, NO2, P04, Si02 seem to be more linked with the Factor 2 than with the second. This does not fit our previous hypothesis, and makes the model even more messy and difficult to understand.\nWith regards to the percentage of variability explained by our factors, we found that, collectively, Factors 1 and 2 only account for 55.9% of our model\u0026rsquo;s cumulative variability, which is too little to be acceptable. What is more, the p-value of the contrast is approximately zero (1.68e-03), which forces us to reject the hypthesis that two factors are enough, and, thus, that only the productivity degree and the habitat\u0026rsquo;s adequacy for life are correlated with the provided variables.\nFinally, as we did in the previous exercise, we will build and interpret the loading plot, availaible in Figure [[second_first]]{#second_first label=\u0026ldquo;second_first\u0026rdquo;}. There, we can see that Oxygen, Depth, PO4, Carbon, Conductivity, SiO2, NO2 and Temperature are mostly correlated to Factor 1, whereas Conductivity, Carbon, NO3 and NH4 are mostly correlated to Factor 2. These correlations do not match our pre-assumptions that concentration of nutrients and dissolved organic carbon would be mostly correlated with productivity degree, with the \u0026quot;chemical compounds data\u0026quot; being mostly connected with habitat adequacy for life, and instead mix together in strange ways, which suggests that the model is neither good not coherent for this kind of analysis\nFigure 1. The Factor Analysis plot of the loadings shows that the variables more related to Factor 1 are Oxygen, Depth, PO4, Carbon, Conductivity, SiO2, NO2 and Temperature; whereas Factor 2 is mostly related to Conductivity, Carbon, NO3 and NH4.\nIn any case, and as we have seen, a more complete model would be required, perhaps with three factors or more. This conclusion is consistent with the previous observation that nine of the 11 variables are not sufficiently explained by the two factors due to their low communality.\nExercise 3 Finally, we will use cars.xls, a database of random samples of car brands in different European countries, with the aim of discovering whether or not there is an association between the different brands and countries. For this, we will use Correspondence Analysis, a dimension reduction technique that allows users to visualize a cloud of multidimensional points in two dimensions respecting the relative positions of the points in the original point cloud.\nTo do so, the first step is to perform the Correspondent Analysis analysis and its factor map, which shows the points distributed in the different dimensions. Thus, in Figure [[third_first]]{#third_first label=\u0026ldquo;third_first\u0026rdquo;}, we can see how brands A and B seem to be clustered quite close to each other, together with most of the Central European countries; while brands C and D are the most distant from the distribution, with C appearing mostly in Scadinavia and Russia (perhaps Volvo) and D in Ireland and the UK, as might be the case for Rolls-Roice. For B, it seems to be mostly associated with Central European countries, such as Germany or Czech Republic, while A is closer to Spain and France.\nFigure 1. Correspondent analysis factor map. The plot shows the associations between the brands A and B, while the huge different between the brands D and C. Regarding the countries, Portugal, Spain, Italy, Greece, Serbia and France are associated with the brand A; Holland, Germany, Switzerland, Belgium, Czech Republic and Hungary are distributed between A and B; Sweden, Norway, Poland and Finland are mainly distributed between B and C; finally, UK and Ireland are associated to brand D.\nThe next step, to understand whether the model is valid or not, is to analyse the variances of the axes, the absolute and relative contributions, and the inertia:\nVariances of the axes: As seen on the graph, we have close coordinates between Portugal, Spain, Italy, Greece, Serbia and France especially along the dimension 1, which might mean that their auto industries are closely intertwined together. UK and Ireland, two island nations partially decoupled from the EU\u0026rsquo;s single market and Schengen Area, are far from the others. Absolute contributions: Dimension 1 is most closely related to Finland, Norway, Uk, Ireland and Russia. Dimension 2 explains UK and Ireland, and Dimension 3 works for mostly Greece, the Czeck Republic, Holland and Belgium. Relative contributions: Dimension 1 is good at explaining variations in car buying in Finland, Sweden, Norway, Russia and Poland with a relative value of more than 90%. This increases for Dimension 2, which explains Hungary with a 95% value, although this only reached 70% for the UK and Ireland. Dimension 3 is good at explaining variance in Holland and Belgium: around 80%. Inertia: The most important countries in the CA analysis are Russia (20,6%), UK (17,6%) and Ireland (18,4%). Overall, these values make a lot of sense, and are consistent with what was obtained in Figure [\\ Brands A and B are thus much more closely related to each other and to Dimension 3, with Dimension 2 mostly related to Brand D and Dimension 1 related to Brand C. Moreover, the relative correlation values are very high, always above 50% (see Annex I - Code). Moreover, Dimensions 1 and 2 seem to explain up to 87,5% percent of the variance on our data (see Table 4), being the most important ones, with Dimension 3 only explaining 12,5% of the variance. This makes sense: automotive industries are generally highly correlated with the countries of origin of their parent groups, especially in Europe, one of the world\u0026rsquo;s largest automotive production regions. 3\nEigenvalue % of Variance Cumulative % of Variance Dimension 1 0.493421000637901 48.6932328057494 48.6932328057494 Dimension 2 0.393197841420016 38.802714124917 87.4959469306664 Dimension 3 0.126706772625117 12.5040530693336 100 Eigenvalues and variance of each dimension. As shown in the table, the dimension 1 and 2 explain the 87,5% percent of the variance while the dimension 3 only explain the 12,5%.\nConclusions Using Principal Component Analysis, Factor Analysis and Correspondence Analysis, we have been able to solve a number of statistical problems in fields as far apart as automobiles, biology and the emergence of life in lakes. Although it has not always produced perfect results, we believe that the use of these models has, in general, been very useful, and has allowed us to solve the problems we faced in a creative and satisfactory way.\nBibliography This document, and the accompanying code, is availaible under the CC By SA 4.0 License\nS. E. Jørgensen. Application of models in limnological research. SIL Proceedings, 1922-2010, 24(1) :61–67, Dec. 1990.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nL. C. Kristiansen, S. Jacobsen, F. Jessen, and B. M. Jørgensen. Using a cross-model loadings plot to identify protein spots causing 2-DE gels to become outliers in PCA. PROTEOMICS, 10(8) :1721–1723, 2010.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nS. Radosevic and A. Rozeik. Foreign Direct Investment and Restructuring in the Automotive Industry in Central and East Europe, Mar. 2005.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","date":"January 17, 2022","hero":"/es/posts/master-en-biolog%C3%ADa-computacional/estad%C3%ADstica/assignment-3/hero.jpg","permalink":"https://www.pablomarcos.me/es/posts/master-en-biolog%C3%ADa-computacional/estad%C3%ADstica/assignment-3/","summary":"by Yaiza ARNAIZ ALCACER (251), Pablo MARCOS LOPEZ (269), Alexandre VERGNAUD (178) and Lexane LOUIS (179)\nIntroduction Advanced statistics, through methods such as Principal Component Analysis, Factor Analysis and Correspondence Analysis, allows us to find the most important variables in a dataset, facilitating its understanding and the making of decisions and conclusions. Throughout the following 3 exercises, we will demonstrate the use of these three methods in different fields, showing their usefulness and applications.","tags":null,"title":"Practical Assignment 3"},{"categories":null,"contents":"Absctract In Alóndiga et al, our team presented a modified SEIR model, which we called \u0026ldquo;SEIRHCM\u0026rdquo;, to model the effect of mobility restrictions on deaths during the first wave of the CoViD-19 pandemic. In this paper, we build on that model, extending it to cover the 2 years of the pandemic so far, and trying to add other interesting features, such as waning immunity after 3 months. We have also added a spatial component to the model, representing the 20 most populated provinces in Spain by means of a graph that indicates the interconnections between them. Our model, which has a linear fit of 73% with respect to reality for the Infected compartment, can be further refined to take into account, for example, the effects of the different measures in each province or autonomous community, or the total numbers of Hospitalised and Deceased.\nKeywords: CoViD-19, Compartmental Models, Bioinformatics, Biological Systems Modelling\nIntroduction In Alóndiga et al, a paper published at the Biotechnology Student Congress 2021, our team used a model,1 which we called SEIRHCM, to model the evolution of the Coronavirus pandemic over the first few months of 2020, and quantify the effects of the mobility restrictions implemented by the Spanish government on the number of deaths and the overall incidence of the virus. One of the most common criticisms of this type of models, called compartmental because they assign the population to a series of groups through which it circulates over time, is that they do not take into account the influence of space,2 assigning the same chances of infection to, for example, two people from Madrid as to a person from Madrid and a person from Galicia, when we know that this is not the case.\nTo solve this problem, and to update a series of parameters relating to the passage of time and the formation of new waves of the pandemic, we have decided to update the model, providing it with spatial perception using a graph-based system that represents the 20 most populated provinces in Spain.\nMatherials and Methods Model equations The initial model has the following 7 compartments: S, Susceptible to contract the disease; E, Exposed who, incubate the disease but are not yet contagious; I, Infected, who spread the disease; R, Recovered, who have overcome the disease, do not spread the virus and possess immunity; H, infected individuals requiring hospital admission; C, infected individuals requiring intensive care; and M, COVID-19 deceased patients.\nThese compartments are related to each other by the following equations:\n$$\\frac{dS}{dt} = -\\beta:I:\\frac{S}{N} $$\n$$\\frac{dE}{dt} = \\beta:I:\\frac{S}{N} - \\delta E $$\n$$\\frac{dI}{dt} = \\delta:E - (1-a):\\gamma:I - \\eta:a:I $$\n$$\\frac{dH}{dt} = \\eta:(1-a):I-\\tau:(1-u):H - u:\\sigma:H $$\n$$\\frac{dC}{dt} = u:\\sigma:H - m:\\rho:min(UCI, C) - max(0, C-UCI) - \\omega:(1-m):min(UCI,C) $$\n$$\\frac{dR}{dt} = \\gamma:(1-a):I + \\tau:(1-u):H + \\omega:(1-m):min(UCI,C) $$\n$$\\frac{dM}{dt} = m:\\rho:min(UCI, C) + max(0, C-UCI) $$\nIn our improved model, we have kept these 7 compartments, but with several modifications: first, we have converted it from a SEIRHCM model to a SEIRHCMS model, which means that individuals who have successfully recovered from the virus can contract it again, under a rate of loss of immunity, $\\omicron$, which is activated 180 days after the onset of the pandemic. The R, C and M compartments, whose values depend on the number of ICU beds, have also varied slightly, since we have modified the function describing the increase in the number of beds to generate a peak around day 200 (in July, when the worst of the first wave was over)3, and then gradually decrease until day 300 (approximately when the last medicalised hospital was closed)4, maintaining a higher rate of ICU beds than the initial rate to take into account the public health actions; for example, the creation of the Hospital Enfermera Isabel Zendal in Madrid.5\nAdditionally, the $R_{0}$ has gone from being approximated by a logistic equation, as we did in the original paper, to being estimated over time using the Python package epyestim,6 which takes the time series of infected data to estimate $R_{0}$ at each point in time. In this way, we can take into account the appearance of new variants, with different levels of contagiousness, since $R_{0}$ is one of the most important parameters when estimating the behaviour of the virus. However, the epyestim results did not seem very accurate when modelling the pandemic, so in order to improve the model\u0026rsquo;s fit to real caseload data, and to favour the appearance of the different peaks related to the different waves of the pandemic, we have modified its values at several points, resulting in a change of the average value of $R_{0}$ of 12%.\nThe modified equations would look like so:\n$$ \\frac{dS}{dt} = -\\beta:I:\\frac{S}{N} + R:\\omicron $$ $$ \\frac{dR}{dt} = \\gamma:(1-a):I + \\tau:(1-u):H + \\omega:(1-m):min(UCI,C) - R:\\omicron $$\nInterconnection network design To account for the role of space, we have divided the data province-by-province, taking into account only the 20 most populated provinces, and we have represented the relationships between those 20 provinces using a graph from the Python networkx package,7 which facilitates both weaving the connections (using an adjacency matrix generated using graphonline.ru),8 as well as finding neighbours. To design the graph, whose approximate representation can be seen in Figure 1, we have taken into account the main train lines,9 motorways and roads in Spain (in the background of the figure),10 as well as the busiest air routes.11\n![Graph representing the different interconnections between the 20 most populated provinces in Spain taken into account by our model\u0026rsquo;s adjacency matrix.](./Spain Network.png)\nOnce the interconnections between nodes have been established, we have taken into account a mobility factor of 10%; in this way, at the beginning of each day, we consider that 10% of the population of each node is distributed among its neighbours in a manner weighted by its population, receiving from them, also in a weighted manner, equivalent population to make up for this loss.\nFor our analysis, we chose the time period between 20 February 2020 and 28 December 2021, the time periods in which epyestim estimates values are central enough to make predictions about $R_{0}$.\nResults The model we have created fits real data acceptably well, with an R² of 0.73 with respect to the cumulative data produced between provinces, as can be seen in Table 1. Moreover, one of the results we expected to find is confirmed, namely that the presence of a mobility factor between the different provinces increases the number of total cases from 3,878,189 to 4,001,762; this makes sense, since we know from graph theory that more connected nodes tend to pay a higher price in terms of infections in the event of pandemics. However, it seems that this increase in cases does not correspond to reality, since the other scenario, which has no mobility, has a slightly higher R² of 0.731; this scenario would be equivalent to analysing the same SEIRHCMS model without provinces all together, since what we would be doing in this case is running 20 separate simulations and summing the total cases. However, the change in R² is, in any case, small, and, moreover, can be explained, among other things, because we have kept the same average $R_{0}$ among all the provinces, instead of calculating it for each one. This, which greatly simplifies the workload of this assignment-overloaded student, is not the best way to proceed, and we could possibly obtain a better fit to the data if we performed the 20 simulations in a more individualised way. In any case, the relatively high R², of \u0026gt; 0.7, makes us think that this model does not differ greatly from reality and is therefore valid.\nTotal Infections R² Mobility Scenario 4.003.445 0.730 No Mobility Scenario 3.880.528 0.731 Table 1: Statistical descriptors for model-predicted infections\nTo better understand the distribution of predicted and confirmed cases, we have produced Figure 2. As expected, we see that the three data series represented (Real Caseload, Predicted Caseload, and Estimated $R_{0}$ over time) are clearly visually related, indicating that the model is well-fitted and sufficiently predictive of reality. One noteworthy reflection is that, due to the way the Government of Spain reports data, the time series of actual cases presents a series of \u0026ldquo;peaks\u0026rdquo; at regular intervals: this represent weekends, when fewer cases are reported than normal due to lower activity in hospitals and the system in general. This, logically, makes it difficult to fit the data linearly, since, although it is a variation whose shape is constant over time, it slightly \u0026ldquo;messes up\u0026rdquo; the shape of the series; therefore, we believe it is possible that the R² of our model is even larger than predicted.\nWe can delve a little deeper into the complexity of this model by showing the results broken down by province, as shown in Figure 3. We can see how, as expected, the most populated provinces, Madrid and Barcelona, are always the ones that take the most cases, thus paying the price for their high centrality and interconnectivity in the system. Moreover, we can see that the pandemic evolves in reality somewhat differently from how it does in the model: for example, the second wave is almost exclusively led by Madrid, while the third and fifth waves have more cases in Barcelona, a less populated province. This is where an estimate of the $R_{0}$ province-by-province would have been more useful, as it would have allowed us to discern the effectiveness and usefulness of the different containment measures at the regional level, as was done in the original paper for the whole of Spain. However, this work is too complex and difficult, and is therefore beyond the scope of this study, which only seeks to present the model.\nFinally, we can represent the three compartments that, I believe, are of most interest: Hospitalized, Critical and Deceased. The first two, at least at first glance, yield values that seem fairly close to reality, although further statistical analysis would have to be carried out to decide whether or not they do; unfortunately, I have not had time for that either. In the last compartment, however, the model fails much more: it predicts about 30,000 deaths by the end of 2021, when, in reality, the number exceeds 80,000. This may be due to several factors, including the fact that it only seems to predict deaths in the 4 most populated provinces, and that dealing with so many provinces, and thus dividing the total number of inhabitants, makes it more difficult for them to cross the compartments I \\rightarrow H \\rightarrow C \\rightarrow M. It is possible that an adjustment of the parameters defining these compartments, using real-world data, could improve the model\u0026rsquo;s outcomes on this front. \\newpage\nConclussions Building on the work of Alóndiga et al., a previous paper by our team, we have been able to develop an updated model to represent the CoViD-19 pandemic on a province-by-province basis. This model has a good fit with reality (R² = 0.73), a high level of granularity, and takes into account various phenomena such as waning immunity and interprovincial mobility, which gives the model a spatial sensitivity that most SIR-type models lack, which is often criticised of them. However, some limitations remain (mainly due to lack of time): the deaths predicted by the model do not fully match reality, and the emergence of a new wave, omicron, with unprecedented levels of contagiousness, could not be included in the analysis due to it emerging too closely to the time of the analysis. Another caveat is that the loss-of-inmunity parameter is based on current levels of Recovered individuals, instead on the ones recovered 180 days ago; again, this should be fixed when more time is devoted into such a model.\nNow, it is up to future researchers to use existing data series (on Hospitalizations, Critical patients and Deaths, for example) to adjust the parameters of this model even better, hopefully obtaining a new one with higher degrees of predictability and affinity with reality that not only allows us to extrapolate what the pandemic could look like in the short term, but also to analyse the effects of the restrictions on mobility adopted by the different governments on the evolution of the virus, allowing us to determine whether these were effective or not.\nReferences This document, and the accompanying code, is availaible under the CC By SA 4.0 License, and was generated using pandoc\nGarcía Rebollar P. XIII Congreso de Estudiantes de Ciencia, Tecnología e Ingeniería Agronómica [Internet]. Madrid. España: E.T.S. de Ingeniería Agronómica, Alimentaria y de Biosistemas (UPM); 2021 [cited 2022 Jan 14]. Available from: https://oa.upm.es/67410/\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nRoberts M, Andreasen V, Lloyd A, Pellis L. Nine challenges for deterministic epidemic models. Epidemics [Internet] 2015 [cited 2022 Jan 15];10:49–53. Available from: https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4996659/\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nInstituto de Salud Carlos III. Evolución de la Pandemia por CoViD 19 en España [Internet]. [cited 2022 Jan 14];Available from: https://cnecovid.isciii.es/covid19/#documentaci%C3%B3n-y-datos\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nMadrid cierra el último hotel medicalizado para pacientes con coronavirus [Internet]. ElDiario.es2021 [cited 2022 Jan 14];Available from: https://www.eldiario.es/sociedad/madrid-cierra-ultimo-hotel-medicalizado- pacientes-coronavirus_1_8003046.html\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nDíaz ayuso inaugura el hospital enfermera isabel zendal [Internet]. Comunidad de Madrid2020 [cited 2022 Jan 14];Available from: https://www.comunidad.madrid/noticias/2020/12/01/diaz-ayuso-inaugura-hospital-enfermera-isabel-zendal\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nEpyestim - A python package to estimate the time-varying effective reproduction number of an epidemic from reported case numbers [Internet]. [cited 2022 Jan 14];Available from: https://github.com/lo-hfk/epyestim\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nNetworkX — NetworkX documentation [Internet]. [cited 2022 Jan 15];Available from: https://networkx.org/\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nCreate Graph online and find shortest path or use other algorithm [Internet]. [cited 2022 Jan 14];Available from: https://graphonline.ru/en/\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nMapas de líneas de Renfe [Internet]. [cited 2022 Jan 15];Available from: https://www.renfe.com/es/es/viajar/informacion-util/mapas-y-lineas/ave-y-larga-distancia\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n. Datos mensuales de tráfico ministerio de transportes, movilidad y agenda urbana [Internet]. [cited 2022 Jan 15];Available from: https://www.mitma.es/carreteras/trafico-velocidades-y-accidentes-mapa-estimacion-y-evolucion/datos-mensuales-de-trafico/datos-mensuales-de-trafico-en-la-rce\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nPrincipales rutas desde aeropuertos españoles Ministerio de Transportes, Movilidad y Agenda Urbana [Internet]. [cited 2022 Jan 15]; Available from: https://www.mitma.gob.es\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","date":"January 12, 2022","hero":"/es/posts/master-en-biolog%C3%ADa-computacional/biolog%C3%ADa-programable/essay/hero.png","permalink":"https://www.pablomarcos.me/es/posts/master-en-biolog%C3%ADa-computacional/biolog%C3%ADa-programable/essay/","summary":"Absctract In Alóndiga et al, our team presented a modified SEIR model, which we called \u0026ldquo;SEIRHCM\u0026rdquo;, to model the effect of mobility restrictions on deaths during the first wave of the CoViD-19 pandemic. In this paper, we build on that model, extending it to cover the 2 years of the pandemic so far, and trying to add other interesting features, such as waning immunity after 3 months. We have also added a spatial component to the model, representing the 20 most populated provinces in Spain by means of a graph that indicates the interconnections between them.","tags":null,"title":"Studing the CoViD-19 pandemic using space-sensitive compartmental models "},{"categories":null,"contents":"This jupyter notebook shall serve as accompanying material to the paper titled \u0026ldquo;Using Computational Methods to Discover Novel Drugs for the Treatment of Androgenetic Alopecia\u0026rdquo;, a report for the \u0026ldquo;Computational Structural Biology for Lead Discovery\u0026rdquo; subject at UPM\u0026rsquo;s Master in Computational Biology\nImports First, we proceed to import the modules we will use:\nimport json # Lets us work with the json format import requests # Allows Python to make web requests import pandas as pd # Analysis of tabular data import numpy as np # Numerical library import matplotlib.pyplot as plt # Static, animated, and interactive visualizations import rdkit # Cheminformatics and ML package from rdkit.Chem import MACCSkeys # MACCS fingerprint calculation from rdkit.Chem import PandasTools # RDkit interaction with pandas from rdkit.Chem import rdFingerprintGenerator #Generate fingerprints from rdkit import Chem # The chemistry library from rdkit.Chem import Descriptors, Draw # Molecule descriptors for the Ro5 import rdkit.Chem.AllChem as AllChem # Import all RDKit chemistry modules Initial Analysis First, we query the compounds targeting CHEMBL1856 (3-oxo-5-alpha-steroid 4-dehydrogenase 2, a.k.a. 5-$\\alpha$-reductase type II\n# Ask for at least 150 compounds activity_url = \u0026#34;https://www.ebi.ac.uk/chembl/api/data/activity?target_chembl_id_exact=CHEMBL1856\u0026amp;offset=150\u0026amp;limit=150\u0026#34; # Get results as JSON activity_request = requests.get(activity_url, headers={\u0026#34;Accept\u0026#34;:\u0026#34;application/json\u0026#34;}).json() # Display as table activity_table = pd.DataFrame.from_dict(activity_request[\u0026#39;activities\u0026#39;])[[\u0026#39;molecule_chembl_id\u0026#39;, \u0026#39;type\u0026#39;, \u0026#39;standard_value\u0026#39;, \u0026#39;standard_units\u0026#39;]] We then process the data by Ki and standarize the values\n# Select only those that have Ki activity_table_filter = activity_table.loc[activity_table[\u0026#39;type\u0026#39;]==\u0026#34;Ki\u0026#34;].copy().dropna() # Transform the standard_value column to float to be able to work with it activity_table_filter[\u0026#39;standard_value\u0026#39;] = activity_table_filter[\u0026#39;standard_value\u0026#39;].astype(\u0026#34;float\u0026#34;) # Order the table by value and display first 5 values activity_table_filter.sort_values([\u0026#39;standard_value\u0026#39;]).head(5) molecule_chembl_id type standard_value standard_units 143 CHEMBL336532 Ki 7.6 nM 21 CHEMBL3350133 Ki 44.0 nM 140 CHEMBL340006 Ki 46.0 nM 25 CHEMBL3350133 Ki 110.0 nM 141 CHEMBL340006 Ki 265.0 nM We find the best compound to be CHEMBL296415. After checking CHEMBL, we find no information on selectivity; since we want it to be selective, we search for approved, selective drugs:\n# Specify approved drugs that target CHEMBL1856 mechanism_url = \u0026#34;https://www.ebi.ac.uk/chembl/api/data/mechanism?target_chembl_id__exact=CHEMBL1856\u0026#34; # Get data from the URL mechanism_components = requests.get(mechanism_url, headers={\u0026#34;Accept\u0026#34;:\u0026#34;application/json\u0026#34;}).json() # Format dataframe mechanism_table = pd.DataFrame.from_dict(mechanism_components[\u0026#39;mechanisms\u0026#39;])[[\u0026#39;molecule_chembl_id\u0026#39;, \u0026#39;max_phase\u0026#39;]] mechanism_table #And print molecule_chembl_id max_phase 0 CHEMBL710 4 We find there is only one approved compound, Finasteride (CHEMBL710). Our focus will thus be finding drugs similar to Finasteride, but more effective, more selective, and with less side effects.\nSimilarity Analysis We upload FNS\u0026rsquo;s SMILES TO SwissSimilarity, select ChEMBL (activity\u0026lt;10µM) as subset and combined as method, run the analysis and save the result as \u0026ldquo;ScreeningResultsFinasteride.csv\u0026rdquo;.\nLigand Based Virtual Screening We subset this data to get only the results most similar to FNS, using a series of metrics (DICE/Tanimoto, MACCS/Morgan) explained in the paper.\n# Define Query molecule smiles = \u0026#34;CC(C)(C)NC(=O)[C@H]1CC[C@H]2[C@@H]3CC[C@H]4NC(=O)C=C[C@]4(C)[C@H]3CC[C@]12C\u0026#34; Query = rdkit.Chem.MolFromSmiles(smiles) # And depict it rdkit.Chem.Draw.MolToImage(Query, includeAtomNumbers=True) raw_database = pd.read_csv(\u0026#39;./ScreeningResultsFinasteride.csv\u0026#39;, delimiter=\u0026#39;;\u0026#39;, names =(\u0026#39;ChemblID\u0026#39;,\u0026#39;Score\u0026#39;,\u0026#39;Smile\u0026#39;)) # Read the file and add titles print(f\u0026#34;The initial compounds database has {len(raw_database)} molecules\u0026#34;) PandasTools.AddMoleculeColumnToFrame(raw_database, smilesCol=\u0026#39;Smile\u0026#39;) # Generate molecule (RoMol) from SMILES The initial compounds database has 400 molecules Now, we will build databases for our fingerprints:\n# We build databases for the two methods MACCSDatabase = raw_database.ROMol.apply(MACCSkeys.GenMACCSKeys) MorganDatabase = rdFingerprintGenerator.GetFPs(raw_database[\u0026#34;ROMol\u0026#34;].tolist()) # And prepare the queries: MorganQuery = rdFingerprintGenerator.GetFPs([Query])[0] MACCQuery = MACCSkeys.GenMACCSKeys(Query) And we calculate the indices based on those fingerprints:\n# We calculate this indices both for the MACCS fingerprints raw_database[\u0026#34;Tanimoto (MACCS)\u0026#34;] = rdkit.DataStructs.BulkTanimotoSimilarity(MACCQuery, MACCSDatabase) raw_database[\u0026#34;Dice (MACCS)\u0026#34;] = rdkit.DataStructs.BulkDiceSimilarity(MACCQuery, MACCSDatabase) # And for the morgan fingerprint too raw_database[\u0026#34;Tanimoto (Morgan)\u0026#34;] = rdkit.DataStructs.BulkTanimotoSimilarity(MorganQuery, MorganDatabase) raw_database[\u0026#34;Dice (Morgan)\u0026#34;] = rdkit.DataStructs.BulkDiceSimilarity(MorganQuery, MorganDatabase) We can describe the new database:\nraw_database.describe() Score Tanimoto (MACCS) Dice (MACCS) Tanimoto (Morgan) Dice (Morgan) count 400.000000 400.000000 400.000000 400.000000 400.000000 mean 0.055978 0.482113 0.640505 0.157272 0.258770 std 0.099914 0.130162 0.115564 0.108950 0.140409 min 0.025000 0.205128 0.340426 0.035294 0.068182 25% 0.027750 0.388682 0.559785 0.093458 0.170940 50% 0.030000 0.450806 0.621456 0.113636 0.204082 75% 0.038000 0.600000 0.750000 0.172580 0.294359 max 0.998000 1.000000 1.000000 1.000000 1.000000 As we can see, the simmilarity scores are pretty small, but this is to be expected when selecting \u0026ldquo;Combined - ChEMBL (activity\u0026lt;10µM)\u0026rdquo; in ENSEMBL, as it takes two methods (electroshape and fingerprint) which are quite different. For our Tanimoto and Dice metrics, we can see that all metrics including MACCS yield way better values than those used with the Morgan fingerprint, and those with Dice show better results than those with Tanimoto. Thus, we use \u0026ldquo;dice_maccs\u0026rdquo; as our parameter, and set the cutoff to 0.621456 (with a length of ~200 elements and representing Quartile 2) and save the data.\nraw_databasefilter = raw_database.loc[raw_database[\u0026#39;Dice (MACCS)\u0026#39;].astype(\u0026#39;float\u0026#39;) \u0026gt; 0.621456] raw_databasefilter.drop(\u0026#39;ROMol\u0026#39;, axis=1).to_csv(\u0026#39;./SimilarityAnalysisFinasteride.csv\u0026#39;, index=False) print(f\u0026#34;The DICE(MACCS) filtered database has {len(raw_databasefilter)} molecules\u0026#34;) The DICE(MACCS) filtered database has 200 molecules Pharmacophore Based Virtual Screning We upload \u0026ldquo;SimilarityAnalysisFinasteride.csv\u0026rdquo; to Pharmit, and generate the \u0026ldquo;Finasteride-Similar 5ARIs KQMJ9Y\u0026rdquo; database. Since PharmIt requires the db format to be \u0026ldquo;.smi\u0026rdquo;, we make the necessary changes:\npharmit_input = raw_databasefilter[\u0026#39;Smile\u0026#39;] + \u0026#39; \u0026#39; + raw_databasefilter[\u0026#39;ChemblID\u0026#39;] pharmit_input.to_csv(\u0026#39;./pharmit_input.smi\u0026#39;, index=False, header=False) We use our created pharmacophore (see paper) to filter the database, and download the results as \u0026ldquo;pharmit_output.sdf\u0026rdquo;\nADMET Properties analysis As a proxy for ADMET properties, we are using Lipinski\u0026rsquo;s rule of five, adding TPSA as an additional method.\ndef calculate_ro5_properties(smiles): # Define the funtion to validate Ro5 molecule = Chem.MolFromSmiles(smiles) molecular_weight = Descriptors.ExactMolWt(molecule) n_hba = Descriptors.NumHAcceptors(molecule) n_hbd = Descriptors.NumHDonors(molecule) logp = Descriptors.MolLogP(molecule) TPSA = Descriptors.TPSA (molecule) conditions = [molecular_weight \u0026lt;= 500, n_hba \u0026lt;= 10, n_hbd \u0026lt;= 5, logp \u0026lt;= 5, TPSA \u0026lt; 140] ro5_fulfilled = sum(conditions) == 5 return pd.Series( [molecular_weight, n_hba, n_hbd, logp, TPSA, ro5_fulfilled], index=[\u0026#34;molecular_weight\u0026#34;, \u0026#34;n_hba\u0026#34;, \u0026#34;n_hbd\u0026#34;, \u0026#34;logp\u0026#34;, \u0026#34;TPSA\u0026#34;, \u0026#34;ro5_fulfilled\u0026#34;], ) Read the database we will process:\nMoleculeDatabase = PandasTools.LoadSDF(\u0026#39;./pharmit_output.sdf\u0026#39;, embedProps=True, molColName=None, smilesName=\u0026#39;smiles\u0026#39;) print(f\u0026#34;The pharmit database has {len(MoleculeDatabase)} molecules\u0026#34;) The pharmit database has 51 molecules And apply the Rule of Five\n# Calculate RO5 properties for all molecules ro5_properties = MoleculeDatabase[\u0026#34;smiles\u0026#34;].apply(calculate_ro5_properties) # Concat the properties dataframe with the pharmit output dataframe MoleculeDatabase_Concat = pd.concat([MoleculeDatabase, ro5_properties], axis=1) #And separate by valid and invalid drugs (~ negates boolean values) MoleculeDatabase_ro5_fulfilled = MoleculeDatabase_Concat[MoleculeDatabase_Concat[\u0026#34;ro5_fulfilled\u0026#34;]] MoleculeDatabase_ro5_violated = MoleculeDatabase_Concat[~MoleculeDatabase_Concat[\u0026#34;ro5_fulfilled\u0026#34;]] print(f\u0026#34;{len(MoleculeDatabase_ro5_fulfilled)} Ro5-following compounds found\u0026#34;) 30 Ro5-following compounds found We save Ro5-fulfilling compounds for refference:\nMoleculeDatabase_ro5_fulfilled.to_csv(\u0026#34;MolDB_Ro5.csv\u0026#34;, index=False) Docking using DockThor (Autodock Vina) For autodock vina, we want to screen the most similar molecules to our pharmacophore to see how these bind to our original targets. We can get the 4 best hits like so:\nMoleculeDatabase_ro5_fulfilled.sort_values([\u0026#39;rmsd\u0026#39;], ascending=False).head(4) rmsd ID smiles molecular_weight n_hba n_hbd logp TPSA ro5_fulfilled 33 0.60144037 CHEMBL2397139 CNC(=O)c1ccc(OC2CCC(NC(=O)NC34CC5CC(CC(C5)C3)C... 425.267842 3 3 4.00420 79.46 True 23 0.551425219 CHEMBL1668930 CC(=O)N1CCC(NC(=O)NC2CCC(C)(C)CC2)CC1 295.225977 2 2 2.26530 61.44 True 17 0.547150791 CHEMBL2282650 CC1=C2N(C)CC3C(CCC4(C)C(C(=O)NC(C)(C)C)CCC34)C... 400.308979 3 1 4.54840 49.41 True 46 0.510232747 CHEMBL1631395 CC(=O)Nc1cc2c3c(c1)n(C/C=C/Cn1cc(C)c(=O)[nH]c1... 409.175004 7 2 1.12242 110.89 True And we process them using Autodock Vina. Unfortunately, this are not valid molecules (see paper). So, we process the full database to get one in SDF format for DockThor\npd.options.mode.chained_assignment = None # Disable warnings # Subset only the eseential columns MoleculeDatabase_ro5_fulfilled.reset_index(inplace=True) MitDatabase = MoleculeDatabase_ro5_fulfilled[[\u0026#39;ID\u0026#39;,\u0026#39;smiles\u0026#39;]] # We add the molecule column using rdkit PandasTools.AddMoleculeColumnToFrame(MitDatabase,\u0026#39;smiles\u0026#39;,\u0026#39;Molecule\u0026#39;) # And add Hidrogens to said molecules MitDatabase[\u0026#39;MoleculeH\u0026#39;] = MitDatabase[\u0026#39;Molecule\u0026#39;].apply(Chem.AddHs) # Compute 3D coordinates MitDatabase[\u0026#39;MoleculeH\u0026#39;].map(AllChem.EmbedMolecule); And we save it:\nPandasTools.WriteSDF(MitDatabase, \u0026#39;Finasteride_Similar_Ro5.sdf\u0026#39;, molColName=\u0026#39;MoleculeH\u0026#39;, properties=list(MitDatabase.columns)) We can now read the results and process them:\n# Load dataframes bestranking_typeII = pd.read_csv(\u0026#39;./bestranking_typeII.csv\u0026#39;, delimiter=\u0026#39;;\u0026#39;, header=0) bestranking_typeI = pd.read_csv(\u0026#39;./bestranking_typeI.csv\u0026#39;, delimiter=\u0026#39;;\u0026#39;, header=0) # For each database, get the compound ID and select the Name and Score columns for i, row in bestranking_typeII.iterrows(): bestranking_typeII[\u0026#39;Name\u0026#39;][i] = bestranking_typeII[\u0026#39;Name\u0026#39;][i].split(\u0026#39;_\u0026#39;)[-1] bestranking_typeII = bestranking_typeII[[\u0026#39;Name\u0026#39;, \u0026#39;Score\u0026#39;]] # For each database, get the compound ID and select the Name and Score columns for i, row in bestranking_typeI.iterrows(): bestranking_typeI[\u0026#39;Name\u0026#39;][i] = bestranking_typeI[\u0026#39;Name\u0026#39;][i].split(\u0026#39;_\u0026#39;)[-1] bestranking_typeI = bestranking_typeI[[\u0026#39;Name\u0026#39;, \u0026#39;Score\u0026#39;]] # We rename the columns bestranking_typeII.rename(columns = {\u0026#39;Score\u0026#39;:\u0026#39;Score for type II\u0026#39;}, inplace = True) bestranking_typeI.rename(columns = {\u0026#39;Score\u0026#39;:\u0026#39;Score for type I\u0026#39;}, inplace = True) # Merge the two dataframes bestranking = pd.merge(bestranking_typeI, bestranking_typeII, how=\u0026#39;inner\u0026#39;, on = \u0026#39;Name\u0026#39;) # Add CHEMBLIDs for i, row in bestranking.iterrows(): bestranking[\u0026#39;Name\u0026#39;][i] = MoleculeDatabase_ro5_fulfilled.iloc[int(row[\u0026#34;Name\u0026#34;])][\u0026#39;ID\u0026#39;] # And convert scores to Ki bestranking[\u0026#34;KI for type I\u0026#34;] = np.exp(-bestranking[\u0026#34;Score for type I\u0026#34;]/(0.008314*(273+37))) bestranking[\u0026#34;KI for type II\u0026#34;] = np.exp(-bestranking[\u0026#34;Score for type II\u0026#34;]/(0.008314*(273+37))) Now, we select only those CHEMBLIDs that are more effective and selective than finasteride:\ninitial_ki = float(bestranking.loc[bestranking[\u0026#34;Name\u0026#34;]==\u0026#34;CHEMBL710\u0026#34;][\u0026#34;KI for type I\u0026#34;]) bestranking.loc[(bestranking[\u0026#34;KI for type I\u0026#34;] \u0026gt;= initial_ki) \u0026amp; (bestranking[\u0026#34;KI for type II\u0026#34;] \u0026lt;= initial_ki)] Name Score for type I Score for type II KI for type I KI for type II 0 CHEMBL2057291 -10.082 -8.657 49.988102 28.757235 2 CHEMBL1343539 -9.619 -8.498 41.768499 27.036773 3 CHEMBL2282650 -9.608 -8.746 41.590612 29.767617 4 CHEMBL2057296 -9.581 -8.565 41.157187 27.748830 5 CHEMBL76192 -9.507 -8.255 39.992294 24.604138 6 CHEMBL2282653 -9.351 -8.616 37.643461 28.303388 7 CHEMBL1762030 -9.271 -8.796 36.492965 30.350742 8 CHEMBL2282652 -9.252 -8.281 36.224930 24.853599 9 CHEMBL1668930 -9.204 -7.939 35.556526 21.765096 10 CHEMBL1631395 -9.186 -8.164 35.309066 23.750579 11 CHEMBL1825149 -9.137 -7.898 34.644117 21.421599 12 CHEMBL3221237 -9.109 -8.250 34.269783 24.556452 13 CHEMBL1800917 -9.108 -8.054 34.256489 22.758239 14 CHEMBL280155 -8.991 -8.473 32.736163 26.775786 15 CHEMBL2282654 -8.969 -8.318 32.457919 25.212967 16 CHEMBL2057293 -8.937 -8.796 32.057416 30.350742 17 CHEMBL2282779 -8.909 -8.314 31.711032 25.173867 18 CHEMBL1668929 -8.847 -8.067 30.957299 22.873321 19 CHEMBL710 -8.839 -8.467 30.861358 26.713525 Since there are a lot of compounds, we can select those that are better among equals:\nbestranking.loc[(bestranking[\u0026#34;KI for type I\u0026#34;] \u0026gt;= (initial_ki + 0.15*initial_ki )) \u0026amp; (bestranking[\u0026#34;KI for type II\u0026#34;] \u0026lt;= (initial_ki - 0.15*initial_ki))] Name Score for type I Score for type II KI for type I KI for type II 5 CHEMBL76192 -9.507 -8.255 39.992294 24.604138 8 CHEMBL2282652 -9.252 -8.281 36.224930 24.853599 9 CHEMBL1668930 -9.204 -7.939 35.556526 21.765096 The discussion can be checked in the paper! :P\nCode and Acknowledgements The header image for this post is CC-By-Sa 3.0 by Alberto Salguero on Wikimedia Commons\n","date":"January 10, 2022","hero":"/es/posts/master-en-biolog%C3%ADa-computacional/lead-discovery/annex-i-code/hero.png","permalink":"https://www.pablomarcos.me/es/posts/master-en-biolog%C3%ADa-computacional/lead-discovery/annex-i-code/","summary":"This jupyter notebook shall serve as accompanying material to the paper titled \u0026ldquo;Using Computational Methods to Discover Novel Drugs for the Treatment of Androgenetic Alopecia\u0026rdquo;, a report for the \u0026ldquo;Computational Structural Biology for Lead Discovery\u0026rdquo; subject at UPM\u0026rsquo;s Master in Computational Biology\nImports First, we proceed to import the modules we will use:\nimport json # Lets us work with the json format import requests # Allows Python to make web requests import pandas as pd # Analysis of tabular data import numpy as np # Numerical library import matplotlib.","tags":null,"title":"Annex I - Code"},{"categories":null,"contents":" Abstract The synchronisation of bioluminescence in Photinus carolinus, the rocky mountain firefly, has intrigued researchers and visitors alike for years, making it one of the most studied phenomena of its kind. Using parameters defined in previous literature, we have created a cellular automaton capable of representing, in a simple way, the complex synchronisation mechanisms behind this coleopteran species. Using descriptive statistical methods, we concluded that the most important parameter for defining the system is the size of the network, much more so than the initial proportion of each class of individual or the ratio of influenceability by neighbours, and we were able to test the robustness of our results using Machine Learning, a completely different method that nevertheless supports the same conclusions. Although the predictive power of the parameters studied remains low, it is up to future researchers to characterise other parameters, such as boundary conditions, which could allow us to define the system and predict its results even more accurately.\nIntroduction Lampyridae are a family of polyphagous coleopterans that includes the insects known as fireflies, characterized by their ability to emit light through bioluminescence. This phenomenon, which greatly exposes the insects to the presence of predators in the dark of night, has a lot of functions, mostly related to reproduction: by performing a flash of light, a male firefly can attract the attention of a female mating partner, although a female of an opposing species can also use this mechanism to attract a male and eat him.\nSpecial mention should be made of Photinus carolinus, whose synchronized bioluminescence displays have intrigued Rocky Mountain National Park visitors over the years. Due to its originality, this event has been the subject of many studies, which show that it presents a reproductive advantage: by synchronizing their flashes of light, males facilitate location recognition by females, which, in turn, increases their chances of transmitting their genes and improves the collective reproductive prowess of the species.\nDuring the breeding season in June, hundreds and even thousands of these synchronous fireflies emit between five to eight flashes of light all at once before allowing eight to 10 seconds of darkness. This light pattern can continue for hours, usually starting by the evening and ending around midnight. Previous literature suggests that these mechanisms are governed by a biological pacemaker, which has a waiting time in which it is not usually activated again once it has been previously activated, and an activation treshold at which bioluminescence appears. In this way, the fireflies avoid flashing too close together in time, as well as remaining in a permanent ON/OFF state. This pacemaker is usually reset every 1000 milliseconds, although this can vary depending on a series of external stimuli, which can be either an artificial light (in the case of researchers) or the luminescent signal from neighbors, causing synchronization to exist.\nTo better understand the functioning of this complex system, we have designed a cellular automaton that mimics the synchronization behavior of fireflies, taking into account the waiting and treshold periods described above, and we have studied its behavior to understand the importance of different factors, such as lattice size, initial frequency of each type of individuals and influenceability by the neighbors\u0026rsquo; consensus.\nMaterials and Methods Our simulator consists of three main functions:\ntime_step takes a python matrix, in the form of a np.ndarray, and recursively updates it over time by using our specified treshold parameter, which defaults to 0.25. This works only one step at a time, and a similar ndarray matrix is return with the results of a cycle of lights-turning.\nkeep_track tracks a series of values of study that, together with our parameters of interest, let us define the system and understand how it is behaving. We have decided this values to be time to stabilization, number of components and size of the largest component at a given time. keep_track itself only manages the time to stabilization part, by defining \u0026ldquo;stabilization\u0026rdquo; as no change in the size of the largest component (the parameter we thought would be most prone to change of the three, as it works as the \u0026ldquo;background\u0026rdquo; of sorts of the binary image) over three generations (or six ON/OFF cycles), and invokes count_elements to do the actual calculations.\ncount_elements uses ndimage, a SciPy package, to label and classify the number of islands in each binary image (our lattice), processing its size and returning largest component size, number of components, and average size at equilibrium.\nWith the main pieces of code defined, we can start our investigations:\nLattice Size Based on previous knowledge from Assignment 4 (the cellular automata), we would expect the complexity of our system to be highly correlated with the size of the network, since complex systems can only start to appear once the framework for them is large enough. This can be understood simply by reductio ad absurdum: if we have a grid of only 1 or 2 pixels, synchronisation will be almost immediate, making the system uninteresting to study; whereas, with a grid of 100 pixels, there may be local phenomena of interest, which is why we have chosen to study only grids of between 5 x 5 and 100 x 100 pixels (since bigger grids were deemed too computationally intensive).\nWith respect to the parameters, the direct consequences of this assumptions would be to expect the number of components to be directly correlated with the size of the lattice, since, as the lattice size increases, the chances of local phenomena of interest also increase. For the size of the largest component, something similar should happen: the larger the grid, the larger it will be, although it is possible that the appearance of local phenomena will make the correlation less intense than in the previous case. Finally, the time to stabilization should also correlate nicely with this parameter, since in smaller lattices, this is trivial, while, in larger lattices, \u0026ldquo;lazy fireflies\u0026rdquo; that take longer to synchronise may appear.\nFor the data visualization part, we have fitted the data to a linear model using SciPy\u0026rsquo;s linregress package, extracting the R² and the p-value parameters to mathematically see how well this correlate. To show the results, we defined the plot_graph function, which simply makes it easier to automate making subplots of linearly-fitted data.\nInfluenceability by the neighbors\u0026rsquo; consensus For this parameter, our basic time_step function has a qualifier, treshold, which represents the chance that, given a set of neighbours with equal values, a given firefly X with the opposite value and neighbouring them will reset to match the ON/OFF state of its neighbours. This value is defined as a threshold between 0 and 1, so that the higher the threshold, the more difficult it is for this to occur, and therefore the lower the influenceability, from which it follows that, the lower the threshold, the higher the influenceability. Influenceability can thus be expressed, in percentage terms, by the following formula:\n$$ influenceability_percentage = (1-threshold) \\cdot 100 \\qquad %$$\nIn principle, we would expect the size of the largest component to be highly influenced by the neighbor\u0026rsquo;s consensus, since, if fireflies tend to more easily respond to each other syncronizations, the should more easily become syncronized, forming larger groups; although, and this is important, the number of components might not be as influenced, since local pockets that surged after 100 generations mostly stayed that way because they were insensible to the default (and already quite high) treshold of 25%. Time to stabilization, might, however, be more correlated, since we have defined stabilization as \u0026ldquo;no changes in the largest component over three generations\u0026rdquo;, and a low influenceability treshold might cause an increase in the number of \u0026ldquo;lazy fireflies\u0026rdquo;, which are, in a biological sense, more autonomous; and, in a mathematical sense, more prone to remember the effects of the initial distribution (which is in this case, random but common to all simulations in this section).\nFor our analysis, we chose 100 linearly spaced treshold values between 0 and 1, covering all probability ranges, and drew the same linearly-fit graphs as in the previous section; more creative plots could have been provided for, but it is difficult to match the descriptive power of a good scatterplot with an R² inside. For the simulations, we chose a 25 x 25 grid, which is enough to be computationally affordable while allowing some interesting complexity to appear.\nInitial Frequency of each type of individuals The initial frequency of each type of individual (i.e., ON/OFF, 1/0) can be defined through a value $p$, which is equal to the proportion over 1 of initially ON individuals; whereas $1-p$ would represent the initial proportion of OFF individuals. Taking into account this probability, initial values were assigned at random through a series of 100, 25 x 25 grids, using the data to plot some more linear correlation graphs using the plot_graph function. Although common sense would suggest that, as happened in the previous section, extreme values (those closer to $p=0$ or $p=1$) should be excluded from our analysis (they represent extreme values that are difficult to find in nature), we have decided to include them and maybe cut them later on, so as to check if taking them out has the desired effect.\nIn general, I would expect all the values here to be weakly correlated (R² of around 0.5), specially parameters such as largest component size, because, although having a larger population of one type will obviously lead to a bigger group of individuals of that same type in the end, this should no big deal: initial conditions usually dont matter that much. I would expect time to stabilization to be less related, specially since, first of all, the time values already tend to be pretty low, so making it easier by modifying proportions shouldn\u0026rsquo;t make much of an impact; and since \u0026ldquo;lazy fireflies\u0026rdquo;, depending on the treshold, may make the process difficult to stabilize in any case. Finally, with regards to the number of components, I expect to see a low correlation, because I expect the data to be two-tailed: with low values of $p$, almost all will be OFF at the beginning, so they would be mostly synched; the reverse would happen for high $p$ values. However, for intermediate values, we might see lots of components appearing, since there are more posibilities to choose from. In this case, of course, the R² metric would be useless, but we will calculate it nonetheless.\nThe final question By now, we should have already characterized the system based on our designed parameters, concluding that, by importance, lattice size \u0026raquo; influenceability, with initial freq.\u0026rsquo;s influence being more difficult to determine. Nowing how the model works, we should be qualified to answer the following questions:\nWhat determines how quickly the system reaches equilibrium? What does the average cluster size at equilibrium depend mostly on?\nFor this section, we will build on the knowledge acquired in the \u0026ldquo;Big data\u0026rdquo; subject, doing something quite different from the traditional statistical analysis we have been using until now. Using a decision tree machine-learning algorithm, we will try to train out machine to predict the \u0026lsquo;Average cluster size\u0026rsquo; and \u0026lsquo;Time to Equilibrium\u0026rsquo; parameters, using the \u0026rsquo;largest component size\u0026rsquo;, \u0026rsquo;time to stabilization\u0026rsquo; and \u0026rsquo;number of components\u0026rsquo; values that we have studied. The interesting part here is that SPARK (the software we want to use), as part of its Machine Learning algorithms, presents a method, .featuresImportance, which shows us precisely what determines the different parameters and in what proportion, which is our subject of study!\nThus, if our analysis is correct, we should find that lattice size is much more important than influenceability, and that influenceability is somewhat more important than intial frequency.\nTo perform this analysis, we have run 8000 different experiments, in which we have played with the following parameters:\nThe lattice size, between 10 and 30: this way, we avoided very small sizes, with artefacts such as perfect synchronisations at the beginning, and very large ones, which would increase computing time. The proportions of initial values, with probabilities between 0.1 and 0.9, up to 20 values: this way, we avoid extreme 0/1 values, which we are not likely to find in nature. Tresholds between 0.1 and 0.9, up to 20 values, once again to get more \u0026ldquo;nature-like\u0026rdquo; values. The number of generations has been kept at a maximum of 80 ON/OFF cycles in all cases, since we have seen, by analysing the graphs of the previous simulations, that this number is almost never exceeded, even for lattices larger than 30x30, which we will not be using here. Results Lattice Size As we can see in Figure 1, with respect to the number of components, represented in in olive colour, we can see that, indeed, they seem to be highly correlated with the size of the lattice, with an R² of 0.90, especially between 0 and 80 pixels. However, from 90x90 pixels onwards we see that a cluster of elements appears that deviates greatly from the trend line, probably due, as we have mentioned, to the fact that, at such large sizes, rare local phenomena appear.\nRegarding the size of the largest component, in pink, we see that it presents an even higher R² of 0. 0.94, although with a trend that seems exponential (although it is not, as we have made the calculations and ressembles more clearly a linear extrapolation), with an inflection point around 50x50 pixels; below this, the size of the largest component does not quite take off, especially between 0 and 40, where almost all the larger sizes are very small due to the ease with which they all synchronise; however, from 50 onwards a new trend appears, in the form of an almost straight line with a much greater slope.\nWhere our predictions seem to have been less accurate is with respect to time to stabilization, in cyan, which has a low R² of only 0.66, with a large dispersion of values around a diffuse increasing trend. This may be due, as mentioned above, to the occurrence of \u0026ldquo;lazy fireflies\u0026rdquo;, which disrupt systems, making the total time to synchronisation less predictable. It is noteworthy, however, how curious it is that, despite this large variability in the time to stabilisation, the rest of the parameters show such high correlation parameters.\nInfluenceability by the neighbors\u0026rsquo; consensus As we can see in Figure 2, the correlation between variables is pretty bad, with lots of outliers appearing when the treshold gets close to 1; this makes sense, since a treshold of 1 means that the fireflies are completely insensible to their neighbors, and, thus, that they will never syncronize, keeping lots of small-sized components. This was at first surprising to me, and I didnt understood it quite well until I used the animate_firefly function (see below) to evaluate the system graphically.\nTo make sense of the remaining, non-outlier data, we can slice it to keep them out and re-make the graphs:\nIn Figure 3, correlations dont get much better than in Figure 2 for the number of components, and increase moderately, to ~0.3, for time to stabilization. For largest component size, however, R² skyrockets to 0.70: still not super high, but way, way better than before. This actually makes sense from an evolutionary perspective: if fireflies are able to eventually sync, with not much of a delay independently of how bad they are at that (measured by treshold), that means that we can have really \u0026ldquo;messy\u0026rdquo;, biological, and real-life scenarios in which fireflies will, eventually, sync, no matter how bad the visibility conditions are, for example. In a system so important to reproduction, an essential function of any living being, it is precisely the robustness shown here that it is the most important; if good coupling would have been limited to just a few treshold values, that would have made finding suitable mates much more difficult.\nOf course, in such a system, treshold values close to 0 or 1 dont make much sense: a treshold of 1 would imply almost-blind fireflies which would have trouble reproducing; and a treshold of 0 would see hyper-aware fireflies, so bent on their surroundings and on keeping everything as-is, that no sync would take place.\nInitial Frequency of each type of individuals In Figure 4, we can see that, in fact, our prediction for number of components was right! There seems to be a two-tailed distribution, although, after trying different parameters, we couldn\u0026rsquo;t fit it neither to a $\\chi²$, nor to a normal or a polynomial distribution, so, being quite unsure what how this data are fitted exactly, we removed the trend lines (the poor R² can still be seen). With regards to largest component size and time to stabilization, however, we were less in the right: they too seem to follow a \u0026ldquo;triangular\u0026rdquo; distribution, although opposite to what was previously theorized. As we explained in the \u0026ldquo;Materials and Methods\u0026rdquo; section, this makes sense! It is difficult to find complexity if the initial conditions are all 0s or 1s.\nRespecting the biological interpretation of the data, this means that, at our default treshold of 0.25, fireflies would already be quite good at synching, forming homogeneous groups unless the proportions are ~50/50. This makes sense, since a ~50/50 distribution is the most likely to appear in nature, but if this is good or bad could only be said through further studies of nature: it could very well be that a homogeneous, big group is better at attracting females, or maybe the existance of \u0026ldquo;spots\u0026rdquo; is what makes the light shows attractive to mates and different to sunlight, which would make sense given that they are most likely to appear in nature.\nThe final question In Table 1, we can see that our predictions were, in fact, more or less correct! For \u0026lsquo;Average Cluster Size\u0026rsquo;, the first component, Lattice Size, has an outsized influence of 60.49%, with initial frequency assigned an influence of 27.02%, and the influenceability by neighbors’ consensus assigned 12.47%. That we find an R² of 0.791 (quite high when considered that we only studied three values, and that probability has a role in our system) only reinforces our previous ideas: the system has, in fact, good predictive power, which means it is well characterized and useful for our purpouses. This also solves the question of initial frequency influence: although not linearly fittable, it seems like, for determining \u0026lsquo;Average Cluster Size\u0026rsquo;, it is actually better than a poorly fitted influenceability.\nWith regards to \u0026rsquo; Time to Equilibrium\u0026rsquo;, however, we find that the component with the most influence, with 47.35%, is actually influenceability. This makes sense both theoretically, where our \u0026ldquo;lazy firefly\u0026rdquo; theory would mean that, the less influenceable the fireflies are, the more they take to sync; and graphically: time to equilibrium, though never the one with the best descriptive R², seems way more \u0026ldquo;structured\u0026rdquo; than any other parameter in Figure 4, which precisely studies this interaction, and may skew the worse R² found in Figures 1-3. In any case, this feature has a way worse R², which means that our predictive power is way smaller than in the last case; this, as we said, makes sense, since we are only studying 3 values and trying to predict a whole system from that, so we in any case consider 0.472 to be high enough to consider our results significant and, therefore, to settle the issue in question as successfully resolved.\nRMSE R² Features Importance Average Cluster Size 28.82 0.791 [0.6049,0.2702,0.1247] Time to Equilibrium 11.12 0.472 [0.2102,0.3162,0.4735] Table 1: Results of our Machine Learning Analysis\nConclussions Using descriptive statistical methods, we have been able to effectively describe our model as a function of parameters such as lattice size, initial frequency of each type of individuals and influenceability by the neighbours\u0026rsquo; consensus, showing that, of these, only the former appears to be more statistically significant. To analyse and validate these results, we have evaluated the effects of these parameters on the average cluster size at equilibrium and how quickly the system reaches equilibrium itself, finding that, indeed, the size of the lattice is the most important parameter. That we reached similar conclussions using both Machine Learning algorithms (in our case, a decision tree) and traditional statistics endows our analysis with even greater robustness, cementing the basis of our results.\nThe fact that, in the case of Machine Learning, our predictive ability is low may be due to many things: among them, the low number of simulations with which the algorithm has been trained (only 70% of 8000, which is not as high as it seems in Machine Learning); it remains for future researchers with more powerful computers than ours to repeat this analysis taking more variations of data. Another possible factor to study could be the boundary conditions, which, honestly, I did not know how to modify in the code I worked with, but which could greatly change the result: for example, in a \u0026ldquo;sphere world\u0026rdquo;, there are many more contacts than in the flat 2D world we have proposed, although the one we studied seems more representative of reality. Another possibility would be to introduce 3D matrices, although this seemed too complicated to me.\nWith regards to the code itself, some possible changes would be making the plot_graph function more moddable, adding the posibility of polynomial fitting for Figure 4. We could also create a function/modify the form in which we run the simulations, since now they often overwrite each other, meaning that, to get Figure n, you have to re-run all the code if you have already run Figure n+1\nIn general, however, I believe to be proven that we have a robust and useful code and analysis system that models reality well (because of its agreement with previous literature) and that can be of great interest: it is curious to see how the simplest cellular automata can model biological processes as complex as these.\nReferences Using the knowledge from Task 6, here is a nice animation function that will allow you to visualise the evolution of the fireflies depending on the initial conditions you choose. An equivalent mp4 video has been sent along with the answer in case you don\u0026rsquo;t feel like running the code yourself.\nFaust, L. F. (2004). Fireflies as a catalyst for science education. Integrative and Comparative Biology, 44(3), 264-265. https://doi.org/10.1093/icb/44.3.264\nFaust, L. F., \u0026amp; Weston, P. A. (2009). Degree-day prediction of adult emergence of Photinus carolinus (Coleoptera: Lampyridae). Environmental Entomology, 38(5), 1505-1512. https://doi.org/10.1603/022.038.0519\nRamírez-Ávila, G. M., Kurths, J., \u0026amp; Deneubourg, J. L. (2018). Fireflies: A paradigm in synchronization. En M. Edelman, E. E. N. Macau, \u0026amp; M. A. F. Sanjuan (Eds.), Chaotic, Fractional, and Complex Dynamics: New Insights and Perspectives (pp. 35-64). Springer International Publishing. https://doi.org/10.1007/978-3-319-68109-2_3\nSarfati, R., Hayes, J. C., \u0026amp; Peleg, O. (2021). Self-organization in natural swarms of Photinus carolinus synchronous fireflies. Science Advances. https://doi.org/10.1126/sciadv.abg9259\nThis document, and the accompanying code, is availaible under the CC By SA 4.0 License\nHeader image by Evan Leith on Unsplash\nExample Simulations ","date":"January 10, 2022","hero":"/es/posts/master-en-biolog%C3%ADa-computacional/modelizaci%C3%B3n-de-sistemas-biol%C3%B3gicos/synchronization-of-fireflies/hero.png","permalink":"https://www.pablomarcos.me/es/posts/master-en-biolog%C3%ADa-computacional/modelizaci%C3%B3n-de-sistemas-biol%C3%B3gicos/synchronization-of-fireflies/","summary":"Abstract The synchronisation of bioluminescence in Photinus carolinus, the rocky mountain firefly, has intrigued researchers and visitors alike for years, making it one of the most studied phenomena of its kind. Using parameters defined in previous literature, we have created a cellular automaton capable of representing, in a simple way, the complex synchronisation mechanisms behind this coleopteran species. Using descriptive statistical methods, we concluded that the most important parameter for defining the system is the size of the network, much more so than the initial proportion of each class of individual or the ratio of influenceability by neighbours, and we were able to test the robustness of our results using Machine Learning, a completely different method that nevertheless supports the same conclusions.","tags":null,"title":"Modelling the syncronization pattern of Photinus carolinus in silico using cellular automata"},{"categories":null,"contents":"Abstract Androgenetic alopecia is a genetic variation that causes hair loss on the scalp due to hypersensitivity of hair follicles to didydrotestosterone (DHT). Using only computational methods, we were able to find a drug candidate to replace the most commonly used drug, Finasteride (FNS), which acts by blocking the generation of DHT in the scalp by type II 5-α-reductase, the most active isoform there. Not only did we improve Ki (from 26 nM in FNS to 21 nM), but our candidate remains 63% more selective to type II than to type I, which we hope will allow it to avoid FNS’s side effects.\nKeywords: Male Pattern Hair Loss, Alopecia, Finasteride, Drug discovery\nIntroduction Male pattern Hair Loss, baldness or, more technically, androgenetic alopecia, is a genetic variation present in about 40% of Spanish men 1 that causes hair loss in the scalp due to hypersensitivity of hair follicles to didydrotestosterone (DHT), a potent androgen derived from testosterone. 2\nWith the exception of hair transplants, which treat the condition by repositioning testosterone-insensitive follicles from the sides and back of the head to the DHT-sensitive area on top, there are only two drugs approved for the treatment of androgenetic alopecia: minoxidil and finasteride. Unlike minoxidil, which works by increasing the blood supply to the area, nourishing the hair follicles and temporarily increasing their resistance to DHT, finasteride treats the root of the problem. It competitively and non-suicidally 3 inhibits 5α-reductase type II, the enzyme that converts testosterone to DHT in the scalp, and has been shown to slow hair loss in up to 60% of patients, and to increase hair density in 35% of them. 4 Moreover, due to its selectivity (it acts mainly against 5αR type II, which is mainly expressed in the scalp, and only slightly against 5αR type I, which is expressed in the liver and the rest of the skin), 3 it has fewer side effects than comparable drugs such as dutasteride, even though these may be slightly more effective.\nBoth drugs were discovered accidentally, when it was found that patients being treated for benign prostatic hyperplasia (in the case of FNS) and hypertension (in the case of minoxidil) presented excessive hair growth. 2 In this work, we aim to find a drug with greater efficacy (measured by its Ki) than FNS, with comparable side effects and which maintains selecitvity against 5αR type II (so that DHT does not disappear completely from the body, limiting its generation to the area not affected by alopecia). We hope that, in this way, side effects (such as decreased libido and sperm levels, erectile dysfunction and depression in FNS) will remain limited, without sacrificing efficacy.\nMaterials and Methods Initial Analysis and Ligand Based Virtual Screning To start with the drug search process we entered CHEMBL and searched for type II 5α-Reductase, sorting the interacting molecules by increasing order of Ki (idecreasing order of inhibitory activity). CHEMBL296415, the best hit, presents an Ki of 7.6 nM, which will be used for comparison as a maximum target value at the end of the study. However, as we are looking for a molecule selective only against type II 5αR, and, as we know that this one is not, we will rather use finasteride, which is already characterized, as a base molecule on which to improve its Ki of ~ 26 nM. 5\nThus, obtained its SMILES from CHEMBL, we uploaded it to SwissSimilarity, selecting \u0026ldquo;Combined - ChEMBL (activity\u0026lt;10µM)\u0026rdquo; to obtain molecules with a high degree of similarity to FNS. We processed that list of molecules to find the most similar ones using both MACCS and Morgan Fingerprints and Tanimoto and Dice indexes (see Annex I - Code for more details on processing), selecting those with a combined Dice-MACCS value greater than 0.75. This metric was selected because it yielded the smallest values among the four MACCS/Morgan and Tanimoto/Dice combinations, although all values were quite small due to method selection (Combined - ChEMBL uses electroshape and fingerprint indexing, which are different and make the conditions stringent).\nPharmacophore Based Virtual Screning Having obtained the results of the LBVS, we will now analyze the pharmacophoric properties of our molecules, to find out how and which parts of the molecule carry biological activity\nTo do this, we first built a pharmacophoric model:\nAs a guide molecule, we used Finasteride itself, downloading the crystallographic structure of the FNS-5aR type II complex from the Protein Data Bank 6 and separating the ligand both from the protein and from the NADPH that acts as a cofactor. To get the original mollecule, we recreated the double bond that was destroyed when the NADPH joined it. This helps our pharmacophore account for the biological conformation of the ligand in vivo.\nTo improve diversity and make the structure less force, and in the absence of more crystallographic structures in PDB, we used 4 additional molecules, the 4 best hits by Dice - MACCS of the Ligand Based Virtual Screening, to generate the pharmacophore. These are, CHEMBL2282655, CHEMBL2282779, CHEMBL2282654 and CHEMBL2282782, all come from the same original paper, 7 and have the same mechanism of action and target the same protein pocket as finasteride, so we can be sure that their position with respect to the enzyme will be similar, and, therefore, that they will be valid to form the pharmacophore.\nOnce we have added the four compounds and the cleaved Finasteride to Chimera, we added Hydrogens and Charge and minimized the structure five times, with 500 steepest descent steps and 50 conjugate gradient steps each, and saved as five different mol2 files, which we merged into one. Then, we uploaded that file to PharmaGist, a webserver for ligand-based pharmacophore detection which works via multiple flexible alignment of drug-like molecules. 8 To account for finasteride’s high lipophilicity, we changed the “Hydrophobic” value from 0.3 to 0.5.\nFrom PharmaGist, we downloaded the best match, and screened it against a Pharmit 9 database made out of our Dice-MACCS filtered compounds. To make the screen less stringent, we deactivated some of the less important features, as shown in Figure 1.\nFigure 1. Selected and discarded features for our pharmacophoric model. In green, kept hydrophobic features generated by PharmaGist; in blue, discarded ones. A Hydrogren Bond donor and an aceptor, in white and orange respectively and on the down left, where also kept in the final model.\nADMET Properties analysis The term ADME, which stands for Absorption, Distribution, Metabolism, and Excretion, describes how a given drug interacts with a target organism, since the levels and kinetics of drug exposure to different tissues influence drug’s degradation and processing, and, thus, activity and efficiency. 10\nUsing the code present in Annex I, we evaluated whether our selected molecules follow Lipinski’s rule of five. The rule, developed at Pfizer in 1997, evaluates how likely a drug candidate is to be an effective oral drug in humans, based on the idea that most orally-active compounds are small, lipofillic molecules, and can be used as a proxy for ADMET properties. 11 It demands that the mollecular weight is \u0026lt; 500 daltons, that there are no more than 10 hydrogen bond acceptors and 5 donors, and that its n-octanol-water partition coefficient (a measure of the relationship between lipophilicity and hydrophilicity) does not exceed 5 (high lipophilicity, but not too high). 11\nIf all 5 rules are met, and if the Topological Polar Surface Area is less than 140 (since polar mollecules are bad at permeating cell membranes), the molecules are considered valid.\nDocking using Dock Thor Finally, having selected some drug candidates with good pharmacophoric and chemical properties, we performed a virtual molecular docking, to try and predict the Ki of our compounds and see if they are indeed more effective and selective than approved compounds.\nSince the docking process is very intensive in both time and computational resources, we have used Dock Thor, an online docking service provided by the Laboratório Nacional de Computação Científica do Brazil and powered by the SDumont Supercomputer. 12 With its help, we performed two dockings: one against 5aR type II, and the other against type I, both using as cofactor the NADPH cleaved from the PDB structure obtained in Section 2; and, as a list of potential ligands, a series of 30 compounds obtained in the previous section and converted to SDF using the code in Annex I. To define a suitable docking box, we have performed a reference docking, comparing Finasteride’s 3D structure from PDB with the results of a virtual docking in Chimera using Autodock Vina, 13 obtaining an RMSD of just 0.453 Å for a docking box with the values in Figure 2.\nFigure 2. Docking box size and coordinates using USCF Chimera. In green, the NADPH-coupled finasteride, with the NADPH part left out of the docking box as much as possible. In blue, the docking-ready finasteride, together with 5aR typeII, in beige. In red, the docking result, with an RMSD of 0.453 Å to the original, blue finasteride.\nGiven that 5aR type I’s structure was not availaible in PDB, we kept the same box when docking it, expecting it to be similar to that of 5aR type II. Its structure was obtained from alphafold, which predicts its most likely conformation based on the aminoacidic chain.\nResults Over the successive rounds of selection detailed in the “Materials and Methods” page, we were able to select only those compounds with better similarity, affinity and ADMET properties than finasteride, our original molecule. Figure 3 explains the process in a more detailed way: of the 400 SwissSimilarity results, 200 dissappeared when we applied a further Dice-MACCS combined simmilarity treshold of 0.621, representing Q2; of those, only 51 presented good pharmacophoric properties (as presented in Figure 1), which was reduced to just 30 Rule-of-Five following compounds, with good enough ADMET properties to be sure that they will work in the human body.\nThe results for the docking of those 30 molecules are shown in Table 1, which filters only those with 15% more affinity towards 5aR type II and 15% less towards 5aR type I than FNS, making for suitable drug candidates. As one can see, the estimated Ki values for FNS (CHEMBL710) are 26.713525 for type II and 30.861358, pretty close to literature values of 17–29 nM for type II, 5 though not much for type I, which has a real Ki of 330 nM. 14 This might be because, in vivo, type 2 has higher affinity for testosterone than type 1, 15 and because its structure here was predicted by alphafold, but, given selectivity was overall preserved, we may consider the docking valid.\nFigure 3. Flowchart for compound selection\nAs we can see, the results are not impressive, and most compounds have only a small variation in Ki, never getting close to the 7.6 nM value for which we set off at the start. This, however, is to be expected: the molecule that presented it, CHEMBL336532, was itself an outlier on its table, and may not have good ADMET, similarity or docking qualities. With regards to the compounds that we ended up with, all of them might be good drug candidates, although CHEMBL1668930 seems to be the best: its Ki for 5aR type I might not be as high as for, for example, CHEMBL76192, but it makes up for that with a much lower Ki for 5aR type I, meaning that, overall, it is the most selective of the 3. Furthermore, given that Finasteride is already approved and found to have minimal side effects, we would suggest choosing the one with the lowest Ki for 5αR type II as a proposed drug, since what seems to matter most is efficacy, not side effects, which would in any case already be reduced given the higher Ki for 5αR type I in all our molecules.\nChembl ID Ki for 5αR type II Ki for 5αR type I Selectivity Ratio CHEMBL710 26.7135 30.8614 15.53 CHEMBL2282652 24.8536 36.2249 45.75 CHEMBL76192 24.6041 39.9923 62.54 CHEMBL1668930 21.7651 35.5565 63.36 Figure 4. Table of proposed compounds. In blue, Finasteride, the original drug, and its predicted Kis. In lime, CHEMBL1668930, our proposed drug, with its predicted Kis. Selectivity ratio is 100*(Ki for type I-Ki for type II)\\Ki for type II\nConclussions Using computational methods only, we were able to determine a good candidate for the treatment of Androgenetic Alopecia in CHEMBL1668930. Its high selectivity towards 5αR type II, its better affinity than already approved drugs and its fulfillment of Lipinski’s Rule of Five, makes us believe that this drug holds potential in improving the state of the art with regards to the treatment of both AA and, potentially, Benign Prostate Hyperplasia, which is caused by the same molecular mechanisms. It remains for future scientists to build on our discovery, analyzing the in vivo activity of our molecule and understanding whether this drug is, indeed, body safe, useful and effective.\nReferences 1\tA. Goren et al., “A preliminary observation: Male pattern hair loss among hospitalized COVID-19 patients in Spain – A potential clue to the role of androgens in COVID-19 severity,” Journal of Cosmetic Dermatology, vol. 19, no. 7, pp. 1545–1547, 2020, doi: 10.1111/jocd.13443.\n2\tV. A. Randall, “Molecular Basis of Androgenetic Alopecia,” in Aging Hair, R. M. Trüeb and D. J. Tobin, Eds. Berlin, Heidelberg: Springer, 2010, pp. 9–24. doi: 10.1007/978-3-642-02636-2_2.\n3\tS. L. Hulin-Curtis, D. Petit, W. D. Figg, A. W. Hsing, and J. K. Reichardt, “Finasteride metabolism and pharmacogenetics: new approaches to personalized prevention of prostate cancer,” Future Oncol, vol. 6, no. 12, pp. 1897–1913, Dec. 2010, doi: 10.2217/fon.10.149.\n4\tJ. Leyden et al., “Finasteride in the treatment of men with frontal male pattern hair loss,” Journal of the American Academy of Dermatology, vol. 40, no. 6, pp. 930–937, Jun. 1999, doi: 10.1016/S0190-9622(99)70081-2.\n5\t“Pharmacogenetic analysis of human steroid 5α reductase type II: comparison of finasteride and dutasteride in: Journal of Molecular Endocrinology Volume 34 Issue 3 (2005).” https://jme.bioscientifica.com/view/ journals/jme/34/3/0340617.xml (accessed Jan. 10, 2022).\n6\tR. P. D. Bank, “RCSB PDB - 7BW1: Crystal structure of Steroid 5-alpha-reductase 2 in complex with Finasteride.” https://www.rcsb.org/structure/7BW1 (accessed Jan. 10, 2022).\n7\t“3D-QSAR CoMFA and CoMSIA studies for design of potent human steroid 5α-reductase inhibitors | SpringerLink.” https://link.springer.com/article/10.1007%2Fs00044-012-0006-1 (accessed Dec. 12, 2021).\n8\tD. Schneidman-Duhovny, O. Dror, Y. Inbar, R. Nussinov, and H. J. Wolfson, “PharmaGist: a webserver for ligand-based pharmacophore detection,” Nucleic Acids Research, vol. 36, no. suppl_2, pp. W223–W228, Jul. 2008, doi: 10.1093/nar/gkn187.\n9\tJ. Sunseri and D. R. Koes, “Pharmit: interactive exploration of chemical space,” Nucleic Acids Research, vol. 44, no. W1, pp. W442–W448, Jul. 2016, doi: 10.1093/nar/gkw287.\n10\tL. L. G. Ferreira and A. D. Andricopulo, “ADMET modeling approaches in drug discovery,” Drug Discov Today, vol. 24, no. 5, pp. 1157–1165, May 2019, doi: 10.1016/j.drudis.2019.03.015.\n11\tC. A. Lipinski, “Lead- and drug-like compounds: the rule-of-five revolution,” Drug Discovery Today: Technologies, vol. 1, no. 4, pp. 337–341, Diciembre 2004, doi: 10.1016/j.ddtec.2004.11.007.\n12\tI. A. Guedes et al., “New machine learning and physics-based scoring functions for drug discovery,” Sci Rep, vol. 11, no. 1, p. 3198, Feb. 2021, doi: 10.1038/s41598-021-82410-1.\n13\tO. Trott and A. J. Olson, “AutoDock Vina: Improving the speed and accuracy of docking with a new scoring function, efficient optimization, and multithreading,” Journal of Computational Chemistry, vol. 31, no. 2, pp. 455–461, 2010, doi: 10.1002/jcc.21334.\n14\tS. Délos, C. Iehlé, P.-M. Martin, and J.-P. Raynaud, “Inhibition of the activity of ‘basic’ 5α-reductase (type 1) detected in DU 145 cells and expressed in insect cells,” The Journal of Steroid Biochemistry and Molecular Biology, vol. 48, no. 4, pp. 347–352, Mar. 1994, doi: 10.1016/0960-0760(94)90074-4.\n15\tC. Iehlé, S. Délos, O. Guirou, R. Tate, J. P. Raynaud, and P. M. Martin, “Human prostatic steroid 5 alpha-reductase isoforms\u0026ndash;a comparative study of selective inhibitors,” J Steroid Biochem Mol Biol, vol. 54, no. 5–6, pp. 273–279, Sep. 1995, doi: 10.1016/0960-0760(95)00134-l.\nCode and Acknowledgements Source code for the assignment can be found here(./Annex_I_Code.ipynb)\nThis document, and the accompanying code, is availaible under the CC-By SA 4.0 License(https://creativecommons.org/licenses/by-sa/4.0/).\n","date":"January 10, 2022","hero":"/es/posts/master-en-biolog%C3%ADa-computacional/lead-discovery/assignment-3/hero.png","permalink":"https://www.pablomarcos.me/es/posts/master-en-biolog%C3%ADa-computacional/lead-discovery/assignment-3/","summary":"Abstract Androgenetic alopecia is a genetic variation that causes hair loss on the scalp due to hypersensitivity of hair follicles to didydrotestosterone (DHT). Using only computational methods, we were able to find a drug candidate to replace the most commonly used drug, Finasteride (FNS), which acts by blocking the generation of DHT in the scalp by type II 5-α-reductase, the most active isoform there. Not only did we improve Ki (from 26 nM in FNS to 21 nM), but our candidate remains 63% more selective to type II than to type I, which we hope will allow it to avoid FNS’s side effects.","tags":null,"title":"Using Computational Methods to discover novel drugs for the treatment of Androgenetic Alopecia"},{"categories":null,"contents":"SPARQL is a query language and protocol for searching, adding, modifying or deleting RDF (Resource Description Framework) graph data available on the Internet. Its name is a recursive acronym that stands for SPARQL Protocol and RDF Query Language. Its syntax and functionality is quite simmilar to that of SQL, given that both are designed to query large databases; the main difference is that SQL does this by accessing tables in relational databases, and SPARQL works with a web of Linked Data\nFor this assignment, we will use SPARQL\u0026rsquo;s Jupyter Notebook Kernel to answer some questions regarding a series of online databases:\nUniProt SPARQL Endpoint First, we will use uniprot\u0026rsquo;s SPARQL to learn some things about the UniProt DB. For this, we first have to set up the endpoint to uniprot, and we will set the format to JSON since it is easier to process using Jupyter and will automatically generate some nice tables!\nWe need to use some kernel magic instructions for that!\n%endpoint https://sparql.uniprot.org/sparql %format JSON And, now, we can begin the problem solving!\nHow many protein records are in UniProt? PREFIX core:\u0026lt;http://purl.uniprot.org/core/\u0026gt; SELECT (COUNT(?protein) AS ?Total) # I first tried COUNT(DISTINCT ?protein), but it took ages to run WHERE{ ?protein a core:Protein . } Total360157660Total: 1, Shown: 1 How many Arabidopsis thaliana protein records are there in UniProt? PREFIX core:\u0026lt;http://purl.uniprot.org/core/\u0026gt; PREFIX taxon:\u0026lt;http://purl.uniprot.org/taxonomy/\u0026gt; SELECT (COUNT(DISTINCT ?protein) AS ?Total) WHERE{ ?protein a core:Protein . # Select proteins only ?protein core:organism taxon:3702 . # Arabidopsis thaliana has taxa id 3702 } Total136782Total: 1, Shown: 1 Retrieve pictures of Arabidopsis thaliana from UniProt PREFIX foaf: \u0026lt;http://xmlns.com/foaf/0.1/\u0026gt; # We use the FOAF vocabulary to learn what an image is PREFIX core: \u0026lt;http://purl.uniprot.org/core/\u0026gt; SELECT ?name ?image WHERE { ?taxon foaf:depiction ?image . # We select images ?taxon core:scientificName ?name . # And get their associated names FILTER regex(?name, \u0026#39;^Arabidopsis thaliana$\u0026#39;, \u0026#39;i\u0026#39;) . #We keep only those exactly named \u0026#34;Arabidopsis thaliana\u0026#34; } name imageArabidopsis thaliana https://upload.wikimedia.org/wikipedia/commons/3/39/Arabidopsis.jpgArabidopsis thaliana https://upload.wikimedia.org/wikipedia/commons/thumb/6/60/Arabidopsis_thaliana_inflorescencias.jpg/800px-Arabidopsis_thaliana_inflorescencias.jpgTotal: 2, Shown: 2 What is the description of the enzyme activity of UniProt Protein Q9SZZ8 PREFIX core:\u0026lt;http://purl.uniprot.org/core/\u0026gt; PREFIX uniprot:\u0026lt;http://purl.uniprot.org/uniprot/\u0026gt; # Hint: rdfs and label are already prefixed! SELECT ?description WHERE { uniprot:Q9SZZ8 a core:Protein ; # We select the uniprot protein #Q9SZZ8 core:enzyme ?enzyme . # Only if it is an enzyme we save the ?enzyme variable ?enzyme core:activity ?activity . # For the enzime, we get its activity ?activity rdfs:label ?description # And for its activity, we get the description label } descriptionBeta-carotene + 4 reduced ferredoxin [iron-sulfur] cluster + 2 H(+) + 2 O(2) = zeaxanthin + 4 oxidized ferredoxin [iron-sulfur] cluster + 2 H(2)O.Total: 1, Shown: 1 Retrieve the proteins IDs, and date of submission, for proteins that have been added to UniProt this year PREFIX core:\u0026lt;http://purl.uniprot.org/core/\u0026gt; # Without a limit, jupyter will not load the results. A script-query works fine! :p # Also, I cant place this comment alongside \u0026#34;LIMIT\u0026#34;. Dont know why SELECT ?id ?date WHERE{ ?protein a core:Protein . # Select all instances of protein from uniprot ?protein core:mnemonic ?id . # and from them, retrieve the mnemonic as ?id ?protein core:created ?date . # also, get the time of creation as ?date FILTER (contains(STR(?date), \u0026#34;2021\u0026#34;)) # We only want those submitted in 2021 } LIMIT 10 id dateA0A1H7ADE3_PAEPO 2021-06-02A0A1V1AIL4_ACIBA 2021-06-02A0A2Z0L603_ACIBA 2021-06-02A0A4J5GG53_STREE 2021-04-07A0A6G8SU52_AERHY 2021-02-10A0A6G8SU69_AERHY 2021-02-10A0A7C9JLR7_9BACT 2021-02-10A0A7C9JMZ7_9BACT 2021-02-10A0A7C9KUQ4_9RHIZ 2021-02-10A0A7D4HP61_NEIMU 2021-02-10Total: 10, Shown: 10 How many species are in the UniProt taxonomy? PREFIX core:\u0026lt;http://purl.uniprot.org/core/\u0026gt; SELECT (COUNT(DISTINCT ?taxon) AS ?Total) WHERE{ ?taxon a core:Taxon . # Select all instances of taxon from uniprot ?taxon core:rank core:Species # and from them, all taxons with level = species } Total2029846Total: 1, Shown: 1 How many species have at least one protein record? # This WILL take a long time to execute. Please, pick a coffee, sit back, and enjoy the flight! PREFIX core: \u0026lt;http://purl.uniprot.org/core/\u0026gt; SELECT (COUNT(DISTINCT ?species) AS ?Total) WHERE { ?protein a core:Protein . # Select all protein records from uniprot ?protein core:organism ?species . # Select all the species present on those proteins ?species a core:Taxon . # (species are a taxon) ?species core:rank core:Species . # a taxon with level = species } Total1057158Total: 1, Shown: 1 Find the AGI codes and gene names for all Arabidopsis thaliana proteins that have a protein function annotation description that mentions “pattern formation” # The SKOS (Simple Knowledge Organization System) vocabulary is a common data model for sharing and linking #knowledge organization systems via the Semantic Web. We will use it to get description labels PREFIX skos:\u0026lt;http://www.w3.org/2004/02/skos/core#\u0026gt; PREFIX core:\u0026lt;http://purl.uniprot.org/core/\u0026gt; PREFIX taxon:\u0026lt;http://purl.uniprot.org/taxonomy/\u0026gt; SELECT ?agi_code ?gene_name WHERE{ ?protein a core:Protein . # Select all instances of protein from uniprot ?protein core:organism taxon:3702 . # From those proteins, keep those from arabidopsis only ?protein core:annotation ?annotation . ?annotation a core:Function_Annotation . # I mean, their functional annotations! ?annotation rdfs:comment ?description . # And select their description (save for later) ?protein core:encodedBy ?gene . # See which gene encodes the protein ?gene core:locusName ?agi_code . # and get the AGI codes ?gene skos:prefLabel ?gene_name . # and the name # Now that we have everything, we filter our answers by description # Could have placed this earlier to improve computer time; it loks cleaner this way though FILTER regex( ?description, \u0026#39;pattern formation\u0026#39;,\u0026#39;i\u0026#39;) . } LIMIT 10 agi_code gene_nameAt3g54220 SCRAt4g21750 ATML1At1g13980 GNAt5g40260 SWEET8At1g69670 CUL3BAt1g63700 YDAAt2g46710 ROPGAP3At1g26830 CUL3AAt3g09090 DEX1At4g37650 SHRTotal: 10, Shown: 10 MetaNetX SPARQL Endpoint MetaNetX.org is an online platform for accessing, analyzing and manipulating genome-scale metabolic networks (GSM) as well as biochemical pathways. To this end, it integrates a great variety of data sources and tools. In this assignment, we would like to find what is the MetaNetX Reaction identifier for the UniProt Protein Q18A79\n# Of course, we need to reset the endpoint %endpoint https://rdf.metanetx.org/sparql Endpoint set to: https://rdf.metanetx.org/sparql PREFIX meta: \u0026lt;https://rdf.metanetx.org/schema/\u0026gt; PREFIX uniprot: \u0026lt;http://purl.uniprot.org/uniprot/\u0026gt; SELECT DISTINCT ?reaction_identifier # This is HUGE. If we dont DISTINCT, we get 128 repetitions!! WHERE{ ?peptide meta:peptXref uniprot:Q18A79 . # First, we get all molecules in metanetx that correspond to UNiprot\u0026#39;s Q18A79 ?catalyzes meta:pept ?peptide . # We extract the peptides from all of those molecules ?gpr meta:cata ?catalyzes ; # Get the reactions catalyzed by said peptide meta:reac ?reaction . # We get the associated reactions ?reaction rdfs:label ?reaction_identifier . # And we use rdfs to get the ID label #Not 100% necessary, but can be used to filter out invalid identifiers FILTER regex( ?reaction_identifier, \u0026#39;^mnx*\u0026#39;,\u0026#39;i\u0026#39;) . } reaction_identifiermnxr165934mnxr145046c3Total: 2, Shown: 2 Federated Query To finish up, we will learn how to do federated queries, i.e. those that implement more than one database, making them talk among themselves. Here, we ask, what is the “mnemonic” Gene ID and the MetaNetX Reaction identifier for the protein that has “Starch synthase” catalytic activity in Clostridium difficile (taxon 272563)?\nFirst, we need to decide the endpoint. Since we are doing a \u0026ldquo;composite\u0026rdquo; search, ¿which endpoint should be use? ¿The one from UniProt or the one from Metanext?\nThe answer is, any of those! The endpoint is just our \u0026ldquo;point of entry\u0026rdquo; to the semantic web, and, while it makes it easier to run some \u0026ldquo;local\u0026rdquo; queries, with the correct syntax we can navigate all databases!\nI will thus use UniProt, since its interface is cuter; but its up to you!\n%endpoint https://sparql.uniprot.org/sparql Endpoint set to: https://sparql.uniprot.org/sparql Now, lets divide the question by parts: first, what is the mnemonic ID of the protein that has “Starch synthase” catalytic activity in Clostridium difficile? We can almost re-use some code from exercise 8:\nPREFIX core: \u0026lt;http://purl.uniprot.org/core/\u0026gt; PREFIX taxon: \u0026lt;http://purl.uniprot.org/taxonomy/\u0026gt; SELECT ?protein WHERE { ?protein a core:Protein . ?protein core:organism taxon:272563 . ?protein core:mnemonic ?mnemonic . ?protein core:classifiedWith ?goTerm . ?goTerm rdfs:label ?activity . FILTER regex( ?activity, \u0026#39;starch synthase\u0026#39;,\u0026#39;i\u0026#39;) . } proteinhttp://purl.uniprot.org/uniprot/Q18A79Total: 1, Shown: 1 But\u0026hellip; wait!! I have already seen this before! This is exactly the same protein from exercise 9!! I know how to solve this!\n%endpoint https://rdf.metanetx.org/sparql PREFIX meta: \u0026lt;https://rdf.metanetx.org/schema/\u0026gt; PREFIX uniprot: \u0026lt;http://purl.uniprot.org/uniprot/\u0026gt; SELECT DISTINCT ?reaction_identifier WHERE{ ?peptide meta:peptXref uniprot:Q18A79 . ?cata meta:pept ?peptide . ?gpr meta:cata ?cata ; meta:reac ?reaction . ?reaction rdfs:label ?reaction_identifier . FILTER regex( ?reaction_identifier, \u0026#39;^mnx*\u0026#39;,\u0026#39;i\u0026#39;) . } Endpoint set to: https://rdf.metanetx.org/sparql reaction_identifiermnxr165934mnxr145046c3Total: 2, Shown: 2 Ok, that was great! We can solve the exercise step-by-step. But, ¿can we do it all in one go, without changing endpoints as promised and getting all the results in a neat, simple table! Yes we can! We just need to use some sub-stringing and some binding to get Metanetx to understand what we want from it. It would work like this:\n# Set endpoint to UniProt (I prefer it) %endpoint https://sparql.uniprot.org/sparql # Do some prefixes PREFIX meta: \u0026lt;https://rdf.metanetx.org/schema/\u0026gt; PREFIX core: \u0026lt;http://purl.uniprot.org/core/\u0026gt; PREFIX taxon: \u0026lt;http://purl.uniprot.org/taxonomy/\u0026gt; #Prepare the selects SELECT DISTINCT ?mnemonic ?reaction_identifier ?protein WHERE { # The SERVICE function lets me use servers independently of the enpoint; great! service \u0026lt;http://sparql.uniprot.org/sparql\u0026gt; { # Code derived from exercise 8 ?protein a core:Protein . ?protein core:organism taxon:272563 . ?protein core:mnemonic ?mnemonic . ?protein core:classifiedWith ?goTerm . ?goTerm rdfs:label ?activity . FILTER regex( ?activity, \u0026#39;starch synthase\u0026#39;,\u0026#39;i\u0026#39;) . } service \u0026lt;https://rdf.metanetx.org/sparql\u0026gt; { # Code derived from exercise 9 ?peptide meta:peptXref ?protein . # ?protein is already on up:up endpoint! No need to BIND anything haha ?catalyzes meta:pept ?peptide . ?gpr meta:cata ?catalyzes ; meta:reac ?reac . ?reac rdfs:label ?reaction_identifier . FILTER regex( ?reaction_identifier, \u0026#39;^mnx*\u0026#39;,\u0026#39;i\u0026#39;) . } } Endpoint set to: https://sparql.uniprot.org/sparql mnemonic reaction_identifier proteinGLGA_CLOD6 mnxr165934 http://purl.uniprot.org/uniprot/Q18A79GLGA_CLOD6 mnxr145046c3 http://purl.uniprot.org/uniprot/Q18A79Total: 2, Shown: 2 And, ¡that would be it!!\n","date":"December 30, 2021","hero":"/es/posts/master-en-biolog%C3%ADa-computacional/bioinfo-programming-challenges/assignment-5/hero.png","permalink":"https://www.pablomarcos.me/es/posts/master-en-biolog%C3%ADa-computacional/bioinfo-programming-challenges/assignment-5/","summary":"SPARQL is a query language and protocol for searching, adding, modifying or deleting RDF (Resource Description Framework) graph data available on the Internet. Its name is a recursive acronym that stands for SPARQL Protocol and RDF Query Language. Its syntax and functionality is quite simmilar to that of SQL, given that both are designed to query large databases; the main difference is that SQL does this by accessing tables in relational databases, and SPARQL works with a web of Linked Data","tags":null,"title":"SPARQL Queries"},{"categories":null,"contents":"by Yaiza ARNÁIZ ALCACER (251), Pablo MARCOS LOPEZ (269), Alexandre VERGNAUD (178) and Lexane LOUIS (179)\nIntroduction Climate change is a term that refers to a lasting change in the statistical parameters of the Earth\u0026rsquo;s global climate or its various regional climates. These changes may be due to processes intrinsic to the Earth, to external influences or, more recently, to human activities. The Industrial Revolution has led to a change in the composition of the Earth\u0026rsquo;s atmosphere as a result of greenhouse gas emissions from human activities (and in which natural variations can also play a role). The climate emergency is the defining issue of our time and we are at a tipping point: from changes in weather conditions affecting agricultural and food production to rising sea levels increasing the risk of flooding, the consequences of climate change are global in impact and scale. Without immediate action, it will be much more difficult and costly to adapt to the future consequences of these changes. 1\nHowever, despite the existence of a clear scientific consensus that the planet is warming due to human action, [^noauthor_scientific_nodate] 2 many continue to doubt the existence of a human connection, or of global warming itself. In an attempt to clarify the doubts that may exist on the subject, and to demonstrate conclusively the existence of warming and its association with the concentration of CO$_{2}$ in the atmosphere, we have decided to perform this study.\nAdditionally, to confirm that these changes are mainly due to human activities, we have decided to investigate the correlation of these two variables with a historical series of estimated GDP data. The correlation between them and with GDP may serve as indirect evidence of industrial activity and thus of human culpability in global warming. Finally, to analyze the changes that await our planet in the future, we analyzed sea level rise data from 1850 to the present, since the changes in sea levels are precisely one of the best known and most dangerous consequences of climate change. Our hypothesis is that all of this variables will be connected, showing that climate change is an urgent threat of which we must take care. This could be expressed also as an exploratory question, such as:\nAre CO$_{2}$ concentration and human activities linked to global warming ? Is the rise of the sea level a consequence of the aforementioned factors ?\nThe datasets In order to proceed with this analysis, we have chosen four datasets :\nTo investigate and analyze temperature variation over time, we downloaded a dataset from a Kaggle competition,3 that includes data from the Berkeley Earth Surface Temperature Survey. This source combines 1.6 billion temperature reports from 16 pre-existing archives and accounts for both the mean temperature over land (including data since 1750) and the mean temperature over the sea (since 1850). These data have been processed and normalized following the criteria provided by Berkeley Earth, to take into account various factors such as the different types of thermometers used (mercury thermometers are more inaccurate), the displacement of the location of measuring stations or the use of new measurement techniques. It contains several sub-datasets:\nGlobal Land and Ocean-and-Land Temperatures (GlobalTemperatures.csv ) Global Average Land Temperature by Country (GlobalLandTemperaturesByCountry.csv ) Global Average Land Temperature by State (GlobalLandTemperaturesByState.csv ) Global Land Temperatures By Major City (GlobalLandTemperaturesByMajorCity.csv ) Global Land Temperatures By City (GlobalLandTemperaturesByCity.csv ) To understand the growth in atmospheric CO$_{2}$ concentration, and how it correlates with the previous dataset, we used a database from the Institute for Atmospheric and Climate Science in Zurich (Switzerland) 4, which includes data from year 0 to the present. In order to perform a correlation analysis, we have trimmed the data to include only those values after 1750.\nAs an indirect measure of human activity, one of the variables whose correlation with the previous variables we wish to measure, we have used a database compiled by the Madison Project of the Potsdam Institute for Climate Impact Research 5 which compiles estimated GDP data from 1850 to 1944 (when the GDP metric itself was introduced by the Bretton-Woods conference) and self-reported, country-by-country data from then to 2010.\nTo see whether sea levels are rising, and how this rise correlates with other variables, we used a database from the United States\u0026rsquo;Environmental Protection Agency 6, including data collected from 1880 to 2014 using a combination of long-term tide gauge measurements and more recent satellite measurements, such as those made using NASA\u0026rsquo;s Jason-3. This data contains cumulative changes in sea level for the world\u0026rsquo;s oceans , showing average absolute sea level change (the height of the ocean surface), regardless of whether nearby land is rising or falling. The data was corrected to account for sea floow sinkage since the last Ice Age peak 20,000 years ago.\nWith the use of these datasets, we will try to understand whether or not the increase of CO$_{2}$ in the atmosphere and the rise in sea levels and GDP are related to climate change. After pre-processing the data, and after having made a series of graphs that allow us to intuitively understand the relationship between the variables, we carried out student-t and linear tests of correlation, which allowed us to observe that, indeed, all four datasets are highly correlated, that climate change is a real risk and that, if we don\u0026rsquo;t do something about it soon, the earth\u0026rsquo;s most prosperous cities, which are, almost all, close to the sea, are at clear danger 7.\nData Preparation and Description To understand the content of our selection of data sets, we must first analyze and briefly represent each of them, preparing them for subsequent correlation analyses. Thus, we have eliminated null values,normalized the format of dates and trimmed the time periods to be studied from the following sets:\nTemperature Data The first dataset concerns the measurement of temperature over time and across the Earth. It consists of 5 files, one of which shows global mean temperatures and the remaining 4 show aggregated data by country,region, major cities and cities. To prepare the data, we have removed the null values, noting that there are no values for measurements that include the sea before 1850. Using the describe function, we can see that the uncertainty of the set is very low (0.938); and, to visualize the data set, we have decided to use the dispersion matrix in Figure[[dispersion_matrix]\u0026quot; src=\u0026quot;#dispersion_matrix){reference-type=\u0026ldquo;ref\u0026quot;reference=\u0026ldquo;dispersion_matrix\u0026rdquo;}\nIn this representation, we see that the distributions of the variables related to uncertainty are close to the left of the histograms. This means that the uncertainty is low for most of the values. On another hand, the uncertainty related to \u0026quot;Land and Ocean Average Temperature\u0026quot;is higher than the others and it seems not relevant to study this feature. Regarding the correlation between variables, we cannot conclude anything great except that the minimum and maximum land temperature are correlated : when one increase, the other increase also.\nFor the other files of this dataset, we decided to represent the distribution of temperature in cities and countries of the world in all years, from 1743 to 2013. Figure[[distribution_temp]\u0026rdquo; src=\u0026quot;#distribution_temp){reference-type=\u0026ldquo;ref\u0026quot;reference=\u0026ldquo;distribution_temp\u0026rdquo;} shows us that the average temperature over time is between 20 and 30 degrees.\nFinally, we decided to show the global distribution of the temperature in the entire world in a country-by-country basis, generating an interactive graph available on google colab.The resulting image, Figure[[map_temp]\u0026rdquo; src=\u0026quot;#map_temp){reference-type=\u0026ldquo;ref\u0026rdquo; reference=\u0026ldquo;map_temp\u0026rdquo;},shows that, globally, Africa, South America and Oceania are the hottest continents and that the countries near the northern poles are the coldest, as was to be expected.\nCO2 Data The CO${2}$ dataset is composed of one file which includes a monthly measurement of the quantity of CO${2}$ (in ppm) in the atmosphere from year 0 to the present. The first step is to filter out all the dates and times that came before 1750, since none of our datasets extends so far,far away in time. To process this dataset, we selected the columns of interest and visualized the data by representing the evolution of the amount of CO$_{2}$ in the atmosphere over time in Figure[[dist_co2]\u0026quot; src=\u0026quot;#dist_co2){reference-type=\u0026ldquo;ref\u0026rdquo; reference=\u0026ldquo;dist_co2\u0026rdquo;}.\nWe can see that the PPM concentration of CO$_{2}$ increases the years and that, since 1950, the increase is on an exponential slope.Today we are probably at double the first recorded levels, in 1750.\nGDP Data The GDP dataset is composed of one file which includes the GDP of 164 countries from 1850 to 2010. As always, this dataset contains missing values, which we removed, eliminating those countries with missing values, since their existence, especially in the 2000s, distorted the data tremendously, making it look like there was a global GDP collapse in 2007 that brought us back to 1850 levels (we know there was a drop in GDP, but it was not that big). We summed up the country-by-country data to obtain a global time series, and we saved thproperly formatted database for the next steps. To check that the data makes sense, we plot it. If we are right, we should get an almost exponential growth curve in Figure [[dist_GDP]\u0026quot; src=\u0026quot;#dist_GDP){reference-type=\u0026ldquo;ref\u0026quot;reference=\u0026ldquo;dist_GDP\u0026rdquo;}.\nWe can in fact observe that global GDP grows over the years and that, since 1950, the increase is on an exponential slope. It is curious how,in such a simple graph, so many things can be seen: around 1970, we finda first bump, probably related to the oil crisis8 that reached its peak in those years; and,around 1990, we find another bump, which, according to our team theorizes, could be related to the fall of the USSR.9\nSea level Data The Sea levels dataset is composed of one file which includes data from to 2014 as explained before. We decided to visualize the data by representing the evolution of the sea levels over time. In Figure[[dist_Sea]\u0026rdquo; src=\u0026quot;#dist_Sea){reference-type=\u0026ldquo;ref\u0026rdquo; reference=\u0026ldquo;dist_Sea\u0026rdquo;},we see that the sea level increases over the years and that since 2000,the slope gets a bit steeper.\nAs previously explained, our hypothesis was that climate change is real,and that all the variables would correlate easily; so, after this initial data exploration, we will try to see whether that hypothesis validates in the next segment.\nValidation of the hypothesis Initial exploration To study the increase in temperatures, we plotted on interactive map by year, also available in our Google Colabtear. As we can see in Figure [[temp]\u0026quot; src=\u0026quot;#temp){reference-type=\u0026ldquo;ref\u0026quot;reference=\u0026ldquo;temp\u0026rdquo;}, there is a clear, global increase in temperatures across the world.\nTo study whether our hypothesis is correct or not, the first thing we need to do is to group the temperature data by year, as it are currently grouped by month, an excessively granular measure for our analysis. With the CO$_{2}$ data, since it is already grouped by year only, we don\u0026rsquo;t need to further slice it ; however, we can get its mean and standard deviation (Figure [1\u0026rdquo; src=\u0026quot;#std){reference-type=\u0026ldquo;ref\u0026rdquo; reference=\u0026ldquo;std\u0026rdquo;}),which will be useful later on:\nAn intuitive way to see if there is indeed a correlation or not between thee data, is to make a couple of graphs showing, in the same figure(Figure [2\u0026quot; src=\u0026quot;#CO2){reference-type=\u0026ldquo;ref\u0026rdquo; reference=\u0026ldquo;CO2\u0026rdquo;}), theevolution off atmospheric CO$_{2}$ and land and land and ocean temperatures,respectively, so that we can guess whether they might be correlated or not.\nAs we can see, there is a fairly clear correlation between the increase in temperature and the atmospheric concentration of CO${2}$, especially in the second case, which measures the average temperature over both land and sea. This makes sense for two reasons: the first is that the first data set, which includes only land temperature, starts much earlier, in 1750 (note how the red line in fact starts earlier than the orange one), when measurements were made by hand, at different times of the day and with inaccurate mercury thermometers, so the deviation in the data, and hence the deviation in the correlation, is much larger,despite best efforts to correct this by the data providers. On the other hand, it is quite credible that the temperature has increased more in the oceans than on land : it is known that the oceans, which are acidifying, are large reservoirs of CO${2}$, which absorption could increase their average temperature. Also that the effect of evaporation of water, one of the most potent greenhouse gases (yes, water is really a greenhouse gas,[^deemer_greenhouse_2016] even if it does not stay long in the atmosphere and makes it less risky than methane or CO$_{2}$), is much greater there than on land, so one would expect local temperatures to be much higher. Also note that 75% of the Earth\u0026rsquo;s surface is covered by oceans, so they would be expected to play an excessive role in global temperatures, determining the correlation.\nOn the other hand, it is interesting to note, in the land temperature data, a huge drop in temperatures between about 1810 and 1840. Although might partially be due, as mentioned above, to the poor quality of the measurements, such a sustained decline in time reminds our team that the Little Ice Age, which had its minima between 1770 and 1850, occurred around that time. 10\nCorrelation However, all the things done until now consist of a mere visual analysis and, to give solidity to our work, we must perform one or more tests of statistical significance. In our case, we have decided to make a linear fit and calculate the coefficient of determination of the sample, so that we can affirm with total solidity that our data is correlated, and,therefore, that climate change is real. In Figure[[reg_sea_temp]\u0026quot; src=\u0026quot;#reg_sea_temp){reference-type=\u0026ldquo;ref\u0026quot;reference=\u0026ldquo;reg_sea_temp\u0026rdquo;}:\nAs we can see in Figure[[reg_CO2_temp]\u0026rdquo; src=\u0026quot;#reg_CO2_temp){reference-type=\u0026ldquo;ref\u0026quot;reference=\u0026ldquo;reg_CO2_temp\u0026rdquo;}, our suspicions are confirmed, and, indeed,the combined mean temperature in the sea and ocean has a very good correlation (of 0.8, being datasets over hundreds of years with different measurement methods and variabilities) with the atmospheric concentration of CO${2}$, indicating that, indeed, climate change exists and is due to the increase of CO${2}$ on land. However, we can see that land temperature alone is barely correlated with CO$_{2}$concentration, presenting a very low coefficient of 0.51.\nThis, which at first might seem to be a problem as it contradicts our hypothesis, is not, for two reasons. The first is that, as we previously explained 75% of Earth\u0026rsquo;s surface is occupied by ocean mass, so it is understandable that data that include the sea have a disproportionate influence on the relationship with a global variable such as the atmospheric concentration of CO$_{2}$. Second, and no less important, is that this phenomenon has already been observed in previous literature :water masses have an inordinate effect in accepting heat, acting as a buffer that has so far protected us from the worst effects of climate change, taking as a price, among others, the lives of thousands of corals off the Australian coast. 11\nWith these analyses, we therefore consider the following axioms to be proven : that the earth is warming, and that this warming is due to an increase in the amount of CO$_{2}$ in the atmosphere.\nThe human factor Country\u0026rsquo;s GDP The most skeptical, however, will still say : this is not true ! Yes, it is possible that CO$_{2}$ causes climate change ; but it is not caused by human activity, but by natural processes of the earth ! To try to address these concerns, and to evaluate, through statistics, whether climate change is caused by mankind or not, we have decided to perform the following analysis.\nIt has been widely proven that, at least until a few years ago, the growth in a country\u0026rsquo;s GDP, which used to be associated with industrial growth, is associated with a growth in the country\u0026rsquo;s greenhouse gas emissions (and therefore CO${2}$) 12 ; thus, if we were to see that there is an association between global GDP, the observed increase in temperature, and the amount of CO${2}$ in the atmosphere, we could prove that global warming exists, and that it is caused by an increase in human CO$_{2}$ emissions. Thus, we will perform the same analysis as in the previous section, a linear adjustment of the data, taking this time only the data of combined temperature between sea and land, since we have observed in the previous section that they are the most appropriate.\nAs we can see in the figure[[reg_CO2_GDP]\u0026rdquo; src=\u0026quot;#reg_CO2_GDP){reference-type=\u0026ldquo;ref\u0026quot;reference=\u0026ldquo;reg_CO2_GDP\u0026rdquo;}, global temperature over time and global GDP have a correlation of 0.72, which, although weak, is not negligible:indeed, it seems that, in order to grow, humans overheat the planet, as we had indicated in our hypothesis.\nWhat is surprising, because of its high correlation of 95%, is the link between CO$_{2}$ and global GDP : indeed, it seems that, in order togrow, humans are dependent on fossil fuels on a scale that, at least, we were not aware of until now. This very high correlation is more surprising in that these are datasets that we have downloaded from completely different sites on the internet, and that we have preprocessed as we saw fit without ever looking for prior correlation.This is a good indicator of the serious trouble we are in, a notion that is further reinforced by the connection expressed above between temperature and GDP : it seems that the three variables have a lot to do with each other.\nIndustrial revolution Finally, we have performed a small analysis to see how statistically significant the change in temperature on land and ocean before and after the industrial revolution, a period widely recognised as one with huge spikes in both CO${2}$ production (through the burning of coal) and GDP(through industrial re-conversion) is. We expected the correlation between the two values to be high, since we saw that GDP is already highly correlated with temperature and CO${2}$ emissions.\nAs we can see in Figure [3\u0026rdquo; src=\u0026quot;#industrial){reference-type=\u0026ldquo;ref\u0026quot;reference=\u0026ldquo;industrial\u0026rdquo;}, there is a very high correlation, as measured by the student t-test, between mean, minimum and maximum temperature,both in the ocean and in global terms (ocean + sea) before and after the industrial revolution. This confirms the results of the previous analysis, adding even more robustness to our conclusions and showing that the First Industrial Revolution, the period where massive fossil fuel burning began, is also the starting point for our climate-related troubles\nThe future that awaits us One of the most worrying consequences of climate change is the rise of sea level. It is well known that the most prosperous regions of the earth, the richest cities, and the majority of the population, are distributed along the shores of the sea, where trade, fishing and exchange of content is easier than in land. This presents us with a dilemma : what will happen when, due to global warming, sea level will rise ? As we can see from the graphs below on Figure[[sea_temp]\u0026rdquo; src=\u0026quot;#sea_temp){reference-type=\u0026ldquo;ref\u0026rdquo; reference=\u0026ldquo;sea_temp\u0026rdquo;},there is a visually clear relationship between temperature increase and sea level rise; and since, as we can see in Figure[3\u0026quot; src=\u0026quot;#industrial){reference-type=\u0026ldquo;ref\u0026rdquo; reference=\u0026ldquo;industrial\u0026rdquo;} above,temperature and industrial progress are highly correlated, it is a foregone conclusion that industrial growth inevitable gives rise to huge climate change problems. If, as we have predicted in the previous parts of this analysis, global temperatures are predestined to rise withGDP and industry, we can only think with concern about the future of these cities, and thus of humanity itself.\nConclusion Through a thorough analysis of existing data on GDP, CO$_{2}$ emissions and temperature over time, we have been able not only to understand how our datasets work, test their consistency and explore their content, but also to demonstrate their use in showing the existence of global warming and proving that it is caused by human industrial activity. While haters may point out that correlation does not imply causation (which,generally speaking, is true), the reality is that, both from the previous literature available in the References section, and from the arguments presented here, the authors of this paper believe that the case for human responsibility for global warming, with all the negative things that go with it, is proven. It is now up to us, then, to remedy the situation, avoiding dangerous consequences such as excessive sea level rise caused by the melting of the poles, which threatens the coastal regions where most of humanity lives today.13\nBibliography This document, and the accompanying code, is availaible under the CC BySA 4.0 License\nM. Abou Chakra, S. Bumann, H. Schenk, A. Oschlies, and A. Traulsen. Immediate action is the best strategy when facing uncertain climate change. 9(1) :2566. Nature Publishing Group\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nExamining the scientific consensus on climate change - doran - 2009 - eos, transactions american geophysical union - wiley online library.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nClimate change : Earth surface temperature data.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nInstitute for Atmospheric and Climate Science. Atmospheric CO2 concentration database from the year 0 to the present, in PPM.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nT. Geiger and K. Frieler. Continuous national gross domestic product (GDP) time series for 195 countries : past observations (1850-2005) harmonized with future projections according the shared socio-economic pathways (2006-2100). Artwork Size : 806905 Bytes, 2 Files Medium : application/x-zip-compressed,application/pdf Pages : 806905 Bytes, 2 Files Type : dataset.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nO. US EPA. Climate change indicators : Sea level.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nB. R. Deemer, J. A. Harrison, S. Li, J. J. Beaulieu, T. DelSontro, N. Barros, J. F. Bezerra-Neto, S. M. Powers, M. A. dos Santos, and J. A. Vonk. Greenhouse gas emissions from reservoir water surfaces : A new global synthesis. 66(11) :949–964.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nGoverning europe in a globalizing world : Neoliberalism and its alternatives following the 1973 oil crisis.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nY. Wang, S. Hong, Y. Wang, X. Gong, C. He, Z. Lu, and F. B. Zhan. What is the difference in global research on central asia before and after the collapse of the USSR : a bibliometric analysis. 119(2) :909–930.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nG. Gebbie and P. Huybers. The little ice age and 20th-century deep pacific cooling. 363(6422) :70–74.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nO. Hoegh-Guldberg. Climate change, coral bleaching and the future of the world’s coral reefs. 50(8) :839–866. Publisher : CSIRO PUBLISHING.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nL. H. Martinez. POST INDUSTRIAL REVOLUTION HUMAN ACTIVITY AND CLIMATE CHANGE : WHY THE UNITED STATES MUST IMPLEMENT MANDATORY LIMITS ON INDUSTRIAL GREENHOUSE GAS EMMISSIONS. 20(2) :403–421. Publisher: Florida State University College of Law.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nB. Neumann, A. T. Vafeidis, J. Zimmermann, and R. J. Nicholls. Future coastal population growth and exposure to sea-level rise and coastal flooding–a global assessment. 10(3) :e0118571.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","date":"December 29, 2021","hero":"/es/posts/master-en-biolog%C3%ADa-computacional/estad%C3%ADstica/assignment-2/hero.jpg","permalink":"https://www.pablomarcos.me/es/posts/master-en-biolog%C3%ADa-computacional/estad%C3%ADstica/assignment-2/","summary":"by Yaiza ARNÁIZ ALCACER (251), Pablo MARCOS LOPEZ (269), Alexandre VERGNAUD (178) and Lexane LOUIS (179)\nIntroduction Climate change is a term that refers to a lasting change in the statistical parameters of the Earth\u0026rsquo;s global climate or its various regional climates. These changes may be due to processes intrinsic to the Earth, to external influences or, more recently, to human activities. The Industrial Revolution has led to a change in the composition of the Earth\u0026rsquo;s atmosphere as a result of greenhouse gas emissions from human activities (and in which natural variations can also play a role).","tags":null,"title":"Practical Assignment 2"},{"categories":null,"contents":"For our presentation, we told the class about ELIXIO, an iGEM 2021 awarded project on developing sustainaible alternatives to the fragances world. It was developed by a team from INSA Toulouse, France. Their work, including this post\u0026rsquo;s header, falls under the Creative Commons Attribution 4.0 License.\nYou can download the PDF here.\nPrevious Next \u0026nbsp; \u0026nbsp; Page: / ","date":"December 17, 2021","hero":"/es/posts/master-en-biolog%C3%ADa-computacional/biolog%C3%ADa-programable/presentation/hero.png","permalink":"https://www.pablomarcos.me/es/posts/master-en-biolog%C3%ADa-computacional/biolog%C3%ADa-programable/presentation/","summary":"For our presentation, we told the class about ELIXIO, an iGEM 2021 awarded project on developing sustainaible alternatives to the fragances world. It was developed by a team from INSA Toulouse, France. Their work, including this post\u0026rsquo;s header, falls under the Creative Commons Attribution 4.0 License.\nYou can download the PDF here.\nPrevious Next \u0026nbsp; \u0026nbsp; Page: / ","tags":null,"title":"ELIXIO : a synthetic microbial consortium for sustainable violet fragrances"},{"categories":null,"contents":"Accompanying the paper in this website, we delivered the following presentation on our project. The PDF can be downloaded here, and it should be noted that the presentation\u0026rsquo;s theme is NOT open source.\nPrevious Next \u0026nbsp; \u0026nbsp; Page: / ","date":"December 16, 2021","hero":"/es/posts/master-en-biolog%C3%ADa-computacional/knowledge-representation/presentation/hero.png","permalink":"https://www.pablomarcos.me/es/posts/master-en-biolog%C3%ADa-computacional/knowledge-representation/presentation/","summary":"Accompanying the paper in this website, we delivered the following presentation on our project. The PDF can be downloaded here, and it should be noted that the presentation\u0026rsquo;s theme is NOT open source.\nPrevious Next \u0026nbsp; \u0026nbsp; Page: / ","tags":null,"title":"The Link Between Brain and Machine!"},{"categories":null,"contents":"by Arnáiz, Yaiza and Marcos, Pablo Ignacio\nAbstract Knowledge representation is an area of artificial intelligence whose fundamental goal is to represent knowledge in a way that facilitates the inference of new data. This discipline studies how to think formally - how to use a system of symbols to represent a domain of discourse, along with functions that allow formal reasoning about objects.\nIn this paper, we will analyze the inner workings of knowledge representation, providing a simplified approach to the discipline and a practical example on how it can be applied to improve a Machine Learning Model. We will also explore how the different knowledge representation systems can be applied to neuroscience and artificial intelligence, in order to better understand how humans learn and how to use this knowledge to improve the design of machine learning algorithms.\nIntroduction: Knowledge representation for machines and humans When we represent knowledge, what we seek is not only to want to explain how the world around us works (be it nature itself or a ML algorithm), but, above all, to be able to make inferences from existing knowledge to derive new data. Therefore, it is necessary to find a balance between systems that are very expressive and easy to understand for humans, such as propositional logic, and others that are less intuitive but make the derivation of new data much easier, such as autoepistemic logic. To understand how these systems work, we will look at machines, which, due to being built by humans, are the ones most in need of this systems to order data.\nKnowledge representation for machines There are two main types of models for studying how machines acquire knowledge:\n• In connectionist models, we assume that knowledge is embedded in networks of relationships between different elements, allowing basic stimuli to be used to give rise to large processing networks. These models are based on the internal behavior of living beings, and give rise to structures such as neural networks, but are less explicable, since, in them, it is not possible to find an association between knowledge and representation.\n• In symbolic models, knowledge is represented as a series of declarative sentences, usually based on logic, describing the attributes of a series of \u0026ldquo;symbols\u0026rdquo;, mental models modifiable by rules that hold cognizable properties. These models are easier to explain, and it is on this type of models that we will deal with in our study.\nKnowledge representation for humans In view of this, we may then ask ourselves, how do humans learn? In Legenyel et al., it is theorized that humans learn on the basis of symbols, which, in their case, are each of the movements that make up the repertoire of motor memories appropriate for the multitude of tasks we perform. But didn\u0026rsquo;t we say that the connectionist models, and not the symbolic ones, were the ones based on real evolutionary phenomena?\nThe truth is, in reality, most of the biological systems are hybrid, mixing symbolic representations with connectionist models; the brain is probably one of them.\nThe Legenyel experiment To understand this in more detail, we would like to take a look into a theoretical model that explains how human senses work to produce and store knowledge. As part of a 2019 study, Legenyel et al did precisely this, studying how the sense of sight, a complex set of receptors and neural paths, works to convert complex features into simple, brain-processable pieces of information, which is what in the introduction we called symbols.\nTheir idea was simple: humans are good at recognizing symbols, but, what about chimeras? (symbols made up by remixing already existing symbols) Can humans easily distinguish those, or, on the contrary, would humans get even more lost when two different things are mixed together? How can we help humans visually remember different patterns?\nTo solve this question, they showed a group of 20 individuals groups of 6 elements (the train data) that were pairwise related. Then, the test subjects were shown groups of pairs, some “true” pairs as shown in the train data, and some “chimeric”, meaning mixing elements from different groups or different scenes, and asked them to tell whether those groups were familiar or not. In line with previous results, they found that observation of the training scenes helped subjects perform above chance in the visual familiarity test, judging “true pairs” more familiar than “pseudo pairs” despite having seen all the shapes an equal number of times.\nTo improve learning outcomes, a phisical (haptic) stimuli was provided: when shown the train data, subjects were told to “brea them appart”, being forced to push onto a bimanual robotic interface until it told them to stop. Critically, the force to break any of the groups of 6 elemets depended on their proportion of “true pairs” with those composed only of true pairs being the most difficult to break and those with only chimeras the easiest to tear appart. This improved learning significantly, as it provided humans with another sense to experiment.\nMaterials and Methods Our experiment: Using ML to classify chimeras So, by now, it should be clear that humans are really good at distinguishing between objects, and even at guessing how to classify new objects when they emerge. However, machines are notably bad at the same task, with classification of emerging patterns one of the clear challenges of machine learning approaches.\nTo illustrate this, we have built a simple, yet powerful, machine learning algorithm that classifies some images based on the pattern (circles, triangles or squares) that appear on them. To train and test it, we have built a custom database of shapes, including some chimeras, which, if we follow the ideas exposed on Legenyel, and if machines do indeed learn in a way similar to how humans do, should be the most difficult to analyze.\nThe source code and the dataset for this work can be found on GitHub, and are availaible freely for anyone to reuse or reproduce at will.\nA fourth class: Random Pixels To improve our model, we have to provide an external input, as Legenyel did with the haptic impulse when separating the components. In our case, we have created a new class of \u0026ldquo;random pixels\u0026rdquo;, which we hope will help the network to discover that a fourth class exists and be able to better classify chimeras.\nThis is because we believe that, by teaching the machine that there is a possibility of incomprehensible pixels appearing, it will learn to classify the chimeras as part of this new class of \u0026ldquo;random pixels\u0026rdquo;, separating them from the different shapes.\nThe final comparison: training with chimeras To compare the results of our model with the best version of itself, we have created a final neural network, which, in this case, we have trained using chimeras as a fourth class; in this way, we expect its accuracy to be the best of all, being able to perfectly classify shapes into circles, squares or triangles.\nResults As expected, the first neural network, which we trained using shape data but not chimera data, has fairly high accuracy rates in predicting the shape present in simple images (see Figure 1); however, in complex images, those containing a chimera, the program fails, telling us which shape is present in greater proportion but not being able to indicate that what it is seeing is neither one thing nor the other.\nFigure 1: Compared accuracy of different shapes, global accuracy and performance over time for our first neural network, trained using only shape data. Please note how accuracy for shape detection is really high, although it cant detect chimeras,\nThis makes sense: as we understand it, this machine would learn by grouping pixels, and deciding with what percentage confidence those pixels belong to one shape or another. The problem with this way of learning is that, given a chimera, the program does not quite understand how it works, since different groups of pixels belong, in different percentages, to different classes, and cannot classify it.\nThus, in order to be able to distinguish chimeras, we created our fourth class of random pixels. We theorized this would help the machine learn better, since it would have a fourth class of “uncomprehensible images” where to put the chimeras; however, contrary to expectations, we have seen that the accuracy not only does not increase, but actually decreases (see Figure 2): it seems that the random pixels only \u0026ldquo;confuse\u0026rdquo; the machine, which does not know how to classify them.\nFigure 2: Compared accuracy of different shapes, global accuracy and performance over time for our second neural network, trained using shape data and random pixels. Please note how accuracy for shape detection has decreased with regards to Figure 1, while there are no improvements in chimera detection\nWhat is more, the machine is not even good at predicting chimeras, so it seems like adding a random pixel category was definetely not a good idea. Therefore, in order to understand what are the best results the machine would be capable of, we decided to create a third neural network, this time training it to know about the existence of the chimeras, and see what happens. Indeed, as we can see in Figure 3, this new neural network is not only better than the second one at classifying simple figures, but, unlike the first one, it is highly efficient at classifying chimeras, although these, due to their more complex nature, are still the least accurate (just like with humans!).\nFigure 3: Compared accuracy of different shapes, global accuracy and performance over time for our third neural network, trained using shape data and chimera data. Please note how accuracy, although it takes a bit to take over, is better than in the second network, and how it can efficiently classify chimeras.\nConclusions Throughout this paper, we have studied how humans learn, using the knowledge learned in class and Legenyel\u0026rsquo;s paper. By creating our own neural networks, we have tried to simulate the learning process \u0026ldquo;in silico\u0026rdquo;, providing our \u0026ldquo;virtual brains\u0026rdquo; with a series of external inputs (in this case, random pixels) to try to increase their recognition efficiency. However, we found that this approach did not work: possibly due to poor stimulus design.\nIn any case, what our results do agree with Legenyel\u0026rsquo;s experiment is that, given a series of simple shapes, both humans and machines are worse at differentiating chimeras from real shapes, demonstrating that, in the end, machines are not so different from the humans who created them.\nThis is why knowledge representation remains a topical field, whose applications go beyond the philosophical field, and allow us to make practical decisions that improve decision-making algorithms. Although in this case we have not managed to improve the model, this experiment can serve as an example of how to try to make machine learning algorithms more understandable to humans, allowing interpretable and understandable decisions to be made in a world that will increasingly be governed by this type of algorithm.\nReferences [1]: Cho, Y.-R., \u0026amp; Kang, M. (2020). Interpretable machine learning in bioinformatics. Methods, 179, 1-2. https://doi.org/10.1016/j.ymeth.2020.05.024\n[2]: Doherty, P., Szalas, A., Skowron, A., \u0026amp; Lukaszewicz, W. (2006). Knowledge representation techniques: A rough set approach. Springer.\n[3]:Lengyel, G., Žalalytė, G., Pantelides, A., Ingram, J. N., Fiser, J., Lengyel, M., \u0026amp; Wolpert, D. M. (2019). Unimodal statistical learning produces multimodal object-like representations. ELife, 8, e43942. https://doi.org/10.7554/eLife.43942\n[4]: Tanwar, P., Prasad, \u0026amp; Mahendra. (s. f.). Comparative Study of Three Declarative Knowledge Representation Techniques. nternational Journal on Computer Science and Engineering.\nLicense Text is available under the Creative Commons Attribution Share Alike 4.0 License. Figures and source code availaible under the GNU General Public License, versions 3 or later.\n","date":"December 16, 2021","hero":"/es/posts/master-en-biolog%C3%ADa-computacional/knowledge-representation/paper/hero.png","permalink":"https://www.pablomarcos.me/es/posts/master-en-biolog%C3%ADa-computacional/knowledge-representation/paper/","summary":"by Arnáiz, Yaiza and Marcos, Pablo Ignacio\nAbstract Knowledge representation is an area of artificial intelligence whose fundamental goal is to represent knowledge in a way that facilitates the inference of new data. This discipline studies how to think formally - how to use a system of symbols to represent a domain of discourse, along with functions that allow formal reasoning about objects.\nIn this paper, we will analyze the inner workings of knowledge representation, providing a simplified approach to the discipline and a practical example on how it can be applied to improve a Machine Learning Model.","tags":null,"title":"Using Knowledge Representation to Improve Machine Learning Processes"},{"categories":null,"contents":"Abstract Biological clocks are molecular mechanisms present in living organisms that allow scientists to estimate the passage of time. In this task, we will use a computer model to simulate the evolution over time of a neutral mutation appearing in the genome of a living being in a population N, analyzing its fixation capacity and the average time it takes for fixation to occur. It is precisely through these mechanisms that it is possible to analyze the passage of time since the appearance of a mutation, as has recently happened in the Juan Maeso case.\nFor this work, we have used part of a code provided in class by Professor Alejandro Couce, which has been heavily modified to take into account only one mutated gene with no mutation rate over time. We will take into account various parameters such as population size or the occurrence of \u0026ldquo;bottleneck\u0026rdquo; phenomena. The code used in class can be found in the attached jupyter notebook, and both it and the text of this article are available under the CC-by-SA 4.0 license.\nIntroduction Biological clocks are mechanisms present in living beings that allow us to follow and measure the passage of time. In the case of molecular mechanisms, we can speak of the existence of a time clock dependent on evolutionary drift, where random DNA substitutions caused, for example, by enzyme failures, which are usually neutral, accumulate in protein sequences in a constant manner, allowing us to predict approximately the time elapsed since their appearance, provided that the selective or mutational processes do not vary over time.\nAlthough at first the biological clock hypothesis may seem contradictory with the stasis of the fossil record, which shows that species originate abruptly (the Cambrian explosion being a paradigmatic example of a large number of speciation events in a short time), when it is taken into account that most polymorphisms in the DNA chain are random, everything starts to make sense. Biological clocks are thus simply a consequence of drift processes that generate variation so that, when a bottleneck phenomenon occurs, some individuals can survive. Evolution is not, therefore, a constant process, but takes advantage of moments of scarcity to build on pre-existing genetic diversity.\nIn this article, we will analyze the evolution of these neutral mutations from their appearance to their disappearance or population fixation, using R, Jupyter Notebooks and Google Colab.\nMaterials and Methods To replicate the process of mutation generation, we will start from a population of individuals, in which, in a completely random manner, we force the appearance of a mutation in one and only one of its individuals on day 1. This mutation is represented in the code by a 1 when present and by a 0 when absent. Over a series of timesteps (days), we will simulate reproduction by randomly dividing the population into a number rate of groups (with rate = $\\frac{1}{survival rate}$ to the bottleneck event we are attending) and repeating these groups N times until the original population size is reached.\nIf the gene disappears, the function we have used for this process (see: iterate_daily) aborts (returning the population prevalence over time until then), which not only simplifies the process, but also brings exponential improvements in processing time. On the other hand, if the function reaches as many iterations as days we had originally defined, the function also returns a list with the population prevalence over time.\nIn the introduction, we explained that one of our main objectives was to study the effect of population size and the different possible bottleneck events in the population. Therefore, to facilitate this, we will perform the following variants of our experiment (read by grouping rows two-by-two):\nIndividuals Bottleneck Individuals Bottleneck Individuals Bottleneck 100 No 100 0.5 100 0.25 300 No 300 0.5 300 0.25 1000 No 1000 0.5 1000 0.25 3000 No 3000 0.5 3000 0.25 For all twelve repetitions of the experiment, we will perform 10000 time iterations (days) and 10000 replicates, a more than sufficient number to overcome the low probability of transmission of a mutation in a single gene in such large populations.\nNote: Though as efficient as possible, the code still takes ages to run in my computer. To make it more reproducible, I have uploaded the code to Google Colab, where you can more easily interact with it.\nAfter running the iterate_daily function for all simulations and possible variations, we visualize the distribution of either fixation or mutation times by using histograms, generated by counting the length of the population prevalence over time vectors using generate_histdata. To make them comparable, we forced the limits of all x axes to be between 0 and 10000, and, to make data more easily interpretable, we cut the graphs at y = 70 and grouped the data to form 500 bars.\nTo search for fixation, we developed two functions:\ncalculate_fixation checks if a given repetition of the experiment reached the maximum number of days (a requisite without which it is impossible to speak of mutation fixation, since the gene would have become prematurely extinct). If the list is of the desired length, it calls fixed_since to figure out when the mutation became fixed, and then calculates fixation_probability (the proportion of repetitions which present fixation over the total repetitions in a given variant of our experiment) and avg_fixation_time, the average time it took for the gene to fixate in the repetitions where it dead. It returns both. fixed_since figures out when a gene became fixed. Two possible approaches can be taken here. The more simple one assumes that a gene becomes fixed when all the individuals on the population have it; this, although simplistic and easy to program, feels unreal to me: In the real world, genes are not fixed in a binary fashion, but can appear in varying proportions. Take the example of blood groups: the most common in Catalonia, 0+, is not dominant, but has two recessive alleles, so its incidence should be better: this is a clear case of fixation by genetic drift. Therefore, to take these factors into account, I have developed the second method, which consists of, for each element of the time series, comparing it with the next 2 (or the next one if we are at the end of the list) and the last element of the list; if the relative variance is less than 5% (which is already quite small) I will estimate that the gene has started or reached the fixation process; otherwise, I will keep looking. Thus, if I reach the end of the list without having found fixation, I will assume that, since the fixation process is still active (and we cannot know if it will be fixed or not) we decide to exclude it, returning 0 as value and not adding a new fixed gene to the counter. Once we have calculated the necessary parameters, we place them in a table for display, and perform both a pearson correlation test and a linear fit to find out the relationship of the data to each other, producing a plot for the average fixation time and another for the probability of fixation.\nResults The distribution of the proportion of individuals with the mutation, which can be seen in the histograms in Figure 1, shows the following groups:\nThe early extinctions, with lengths between 1 and 200, represent those genes that, though sheer probability, fail to take over the whole population, ending their presence early on. It is because of this group that we have trimmed the height of the histograms to $y = 70$ (since there are so many concentrated in this group that it would otherwise be very difficult to make inferences about the data), and why we must perform so many repetitions: it takes many, many simulations to overcome the imposing force of chance, which tends to make such small changes (in just one individual) disappear.\nThe fading trend: loosely defined, this group would include all the bars between 200 and 999, and represent genes that, although managing to survive for some time, lack long-term impact, disappearing without becoming fixed (either in the total population or in a part of it) after several generations.\nThe survivers: the members of this group, represented by the histogram bar above $x = 1000$, are those who have been able to survive the 1000 days after which they have been observed, becoming permanently fixed (if all individuals acquire the gene, in which case we could call the individuals with this gene colonizers) or semi-permanently fixed (if only a certain proportion is fixed, as is the case for group 0 in the blood groups.\nAs we can see in the histogram, most of the repeats belong to the early extinctions group, with the number progressively decreasing as we move from the first group to the second and third. However, a reflection is pertinent: we can clearly see how, for variants with only 100 individuals, whether there is a bottleneck or not, no fixed genes will appear; however, as we increase the population, the distribution gradually shifts to the right, leaving room for the appearance of survivers. It is surprising to see that the group that seems to obtain the highest number of fixations is the one that presents a bottleneck with 50% survival, something we did not expect when we started the experiment. Initially, it would make sense that those groups with the lowest bottleneck would have the easiest time seeing a gene fix, since it would only have to compete with the others through its survival fitness (the number of times it is selected by offspring in the iterate_daily function), and not also with bottleneck selection. However, it seems that this precisely provides a suitable environment, a balance between the difficulty to be selected among the ~25% of the genes that reproduce and the possibility to multiply by 2 (instead of only by 1 as happens without bottleneck).\nWith respect to the fixation parameters, we obtain the following table:\nPopulation Bottleneck Fixation Probability AVG Fixation Time 100 1 0.0123 157.869918699187 1000 1 0.0011 996.454545454545 300 1 0.0049 440.204081632653 3000 1 0,0004 5418.5 100 0.5 0.0222 76.0855855855856 1000 0.5 0.0024 698.25 300 0.5 0.0066 238.181818181818 3000 0.5 0,0007 2501.71428571429 100 0.25 0.0375 44496 1000 0.25 0.0034 426.794117647059 300 0.25 0.0141 118.234042553191 3000 0.25 0.0013 1229.69230769231 Pearson (Population) -0.5883012 0.8024554 Pearson (Bottleneck) -0.3393287 0.4053819 As we can see, as far as the connection between population structure and fixation probability is concerned, the results are rather inconclusive: due to the negative sign, it could be said that they are inversely correlated, but the low pearson-coefficient of only 0.58 gives very little robustness to this hypothesis. However, as far as the relationship between average fixation time and population values is concerned, we obtain much more promising results: a pearson-coefficient of 0.802, which, although not super high, still represents a degree of moderate linear dependency; thus we can conclude that average fixation time is moderately related to population characteristics.\nAnother way to investigate the results, as we have explained in the \u0026ldquo;Materials and Methods\u0026rdquo; section, is to use a linear fit to see what the fit between the two populations is; such an analysis is available in Figure 2.\nHere, we can see that the degrees of $R^2$ (= pearson correlation$^2$), as was previously discussed, are bad for Fixation Probability, and neither really good nor really bad for Average Fixation Time. Part of this has to do with the population distribution: as one can see in the graphs, and as was specified by our code, the population coalesces around brackets of 100, 300, 1000 and 3000, which makes it more difficult for the data to fit a linear model.\nThis analysis could also be applied to the bottleneck level, but, as can be seen in the table, the correlation seems to be pretty weak, so I left it out for this time.\nConclussions According to the graphs shown above, we can conclude that, while population size is really important for the fixation of traits, especially for the time it takes for them to fixate once it is known that they are going to do so, we can say that the presence of bottlenecks is not nearly as relevant. This result is certainly surprising: one would expect that the larger the bottleneck, the more difficult it would be for the gene to bind, but this does not seem to be the case. Perhaps this is precisely because we have defined the gene as neutral, and therefore not subject to the selective pressure that these events usually represent.\nRegarding possible extensions and improvements to the code, some of these would be:\nModify the code so that it breaks once 100% of the members of the population contain the gene: this is a small change that could improve computing time significantly. Realize the graphs of the trajectories seen in class: the data are present, available under lists such as sim_100_none, so it would be a matter of plotting it; however, precisely because this type of graphs have already been explored in class, I preferred to try new representations (through, for example, histograms). More repetitions could be added to the code, until reaching the 100,000 suggested by the statement: however, in colab this would increase the computing time, probably due both to point 1 of this list and to the fact that I have performed 12 variants per simulation; moreover, as with 10,000 replications I have already managed to find interesting complexity, I have considered it better to leave this as it is. Another possible improvement, although changing the objective of the code, would be to make the presence of the mutation in the genome modify the reproductive fitness, since bottleneck phenomena are usually not neutral. However, since the goal of this program was to simulate a cellular clock, it is better to keep the mutations neutral. As was discussed in the \u0026ldquo;Results\u0026rdquo; section, we could randomize population values, in order to make the results more representative. Regarding the improvement of the computational approah itself, we could say that one of its major shortcomings is probably in the way it simulates reproduction. By taking random individuals by lot and multiplying the sample several times, not only do we avoid facts such as recombination (an individual does not always have to pass the mutation) or diploidy (a copy or two may pass\u0026hellip;). The code, as it stands, is simplistic, and, although useful for the simulation of the events we want to show, it could be extended to take into account these factors, as well as, for example, the effect of space: currently, it is assumed that all individuals reproduce with all at the same rate, something we know that in reality does not happen. Dominance/recessivity relationships, on the other hand, are less relevant, since we consider this mutation as neutral.\nReferences Fossil Record Stasis - blavatsky.net. https://www.blavatsky.net/index.php/39-sciences/biology/evolution/24-fossil-record-stasis. Accedido 14 de diciembre de 2021.\nPascual-García, Alberto, et al. «The Molecular Clock in the Evolution of Protein Structures». Systematic Biology, vol. 68, n.º 6, noviembre de 2019, pp. 987-1002. PubMed, https://doi.org/10.1093/sysbio/syz022.\n«Descubre cuáles son los grupos sanguíneos más frecuentes». Blog Banc de Sang i Teixits, 22 de septiembre de 2021, https://www.bancsang.net/blog/es/descubre-cuales-son-los-grupos-sanguineos-mas-frecuentes/.\nAnd lots upon lots of Stack Overflow :D\nThis document was generated using pandoc.\n","date":"December 15, 2021","hero":"/es/posts/master-en-biolog%C3%ADa-computacional/biolog%C3%ADa-evolutiva/exercise-1/hero.png","permalink":"https://www.pablomarcos.me/es/posts/master-en-biolog%C3%ADa-computacional/biolog%C3%ADa-evolutiva/exercise-1/","summary":"Abstract Biological clocks are molecular mechanisms present in living organisms that allow scientists to estimate the passage of time. In this task, we will use a computer model to simulate the evolution over time of a neutral mutation appearing in the genome of a living being in a population N, analyzing its fixation capacity and the average time it takes for fixation to occur. It is precisely through these mechanisms that it is possible to analyze the passage of time since the appearance of a mutation, as has recently happened in the Juan Maeso case.","tags":null,"title":"Analyzing the probability and time of fixation of random neutral mutations in silico"},{"categories":null,"contents":" In the last exercise, we used an Erdös-Rényi random attachment model to analyze how interconnections influence the spread of illness over space. Erdös-Rényi, however, is not the only model that exists, and others, such as Preferential Attachment Networks, have the advantage of being more true to reality and an even closer representation of reality.\nIn these models, unlike in the random models, there is a preference when forming links between nodes: normally, nodes tend to cluster around the nodes with the largest number of linked nodes, promoting centralization and improving the connection between nodes; in this way, for example, the diameter of the network tends to decrease, since most nodes can be connected by traveling to the \u0026ldquo;mother node\u0026rdquo;.\nThis model, theorized by Barabási and Albert in 2000, works with an initial grid full of vertices, which are connected either to each other or to a larger node according to a series of possibilities.\nTo begin with, we will import the necessary modules:\nimport numpy as np #Supports common numeric operations import matplotlib.pyplot as plt #For graphs import networkx as nx #For the creation, manipulation, and study of complex networks. import time, datetime #To check how long the script takes to run To make the task easier to work with, we will also import some of the functions from the previous section:\ndef erdos_renyi(nodes, link_prob): x = np.zeros((nodes, nodes),int) #start with n vertices for i in range(nodes): for j in range(nodes): if np.random.uniform(0, 1) \u0026lt; link_prob: x[i,j] = x[j,i] = 1 return(x) def plot_adjacency(adjacency_matrix): gr = nx.from_numpy_matrix(adjacency_matrix) mylabels = dict(zip(range(len(adjacency_matrix)), range(len(adjacency_matrix)))) nx.draw(gr, node_size=200, edge_cmap = \u0026#34;Greys\u0026#34;, labels=mylabels, with_labels=True) return nx.diameter(gr) And now, we can define the functions that will help us generate the preferential attachment network:\ndef preferential_attachment(nodes, links_per_newnode): x = np.zeros((nodes, nodes),int) #start with n vertices bonds = np.zeros(nodes,int) for i in range(1, links_per_newnode): for j in range(1, nodes-1): x[i,j] = x[j,i] = 1 bonds[i] += 1; bonds[j] += 1 for i in range(links_per_newnode + 1, nodes): prob = bonds / np.sum(bonds) for n in range(1, links_per_newnode): r = np.random.uniform(0, 1) cum_p = 0; j = 0 while cum_p \u0026lt; r: j += 1; cum_p += prob[j] x[i,j] = x[j,i] = 1 bonds[i] += 1; bonds[j] += 1 prob[j] = 0 prob = prob / np.sum(prob + 1e-7) #+ 1e-7 so that it doesnt produce an error return(x) And a 2D-lattice network; this I have done using only networkx, since I am so short on time I cant afford to think how to do it.\nTo see how our graphs went, we would like to plot them; but, first, we need to select only the biggest component:\ndef purge(adjacency_matrix): graph = nx.from_numpy_matrix( adjacency_matrix ) sub_graph = graph.subgraph( (sorted(nx.connected_components(graph), key=len, reverse=True))[0] ) return nx.to_numpy_matrix( sub_graph ) And now, we can plot:\nlattice_graph = nx.grid_2d_graph(10,10) plt.subplots(figsize=(20, 5)) plt.subplot(1, 3, 1) d = plot_adjacency( purge(erdos_renyi(35, 0.1)) ); plt.title(f\u0026#34;Network diameter: {d}\u0026#34;) plt.subplot(1, 3, 2) d = plot_adjacency( purge(preferential_attachment(35, 2)) ); plt.title(f\u0026#34;Network diameter: {d}\u0026#34;) plt.subplot(1, 3, 3) d = plot_adjacency( purge(nx.to_numpy_matrix( lattice_graph )) ); plt.title(f\u0026#34;Network diameter: {d}\u0026#34;) plt.show() In the above graphs, one can clearly see the difference betweeen the three topologies: one has completely random edges, with most nodes exhibiting a similar number of conections and none clearly predominating. The network diameter is 4, which is small but not as small as in the preferential attachment network (diameter 2), where most nodes can be easily connected through the middle one. Finally, the 2D-lattice has the biggest diameter (18) as there are no shortcuts and one must traverse lots of nodes to get from one place to another.\nTo better understand how the structure affects outcome, lets carry out a numerical simulation of a rumor propagation process that starts with a single individual carrying a hot piece of news. At each time step, individuals will pass the rumour to their neighbors with probability 𝛽 (per neighbor), and they will stop propagating the network with probability 𝜇. We will compare these three structures with the mean-field solution, which is propagation at a constant speed given by the average number of contacts.\nWe define the propagating function:\ndef propagate_rumour(adjacency_matrix, pass_it = 0.3, ignore = 0.001, iterations = 1000): my_graph = nx.from_numpy_matrix(adjacency_matrix) new_infected = dict(zip(range(len(my_graph)), np.zeros(len(my_graph)))) infected_overtime = [1]; new_infected[np.random.randint(len(my_graph))] = 1 for t in range(iterations-1): infected = new_infected for element in range(len(my_graph)): #For each of the elemets of the graph for neighbor in my_graph.neighbors(element): #Find its neighbors p = np.random.uniform(0,1) if p \u0026gt;= ignore: if infected[element] == 1 and p \u0026lt; pass_it and infected[neighbor] != None: new_infected[neighbor] = 1 else: new_infected[element] = None infected[element] = None counter = [1 if x == 1 else 0 for x in new_infected.values()] #Cannot sum \u0026#34;None\u0026#34; infected_overtime.append(sum(counter)) if sum(counter) == 0: return infected_overtime, t return infected_overtime And we run the simulations 1000 times for up to 1000 steps. Please note that this cell can take suuuuuuper long to run, since it is doing 1000x1000 repetitions. Please, bear in mind that, in my computer, it took 1 full hour to run.\nerdos_endpoint = preferential_endpoint = lattice_endpoint = [] print(datetime.datetime.now()) for repetition in range(1000): print(f\u0026#34;Repetition #{repetition}\u0026#34;, end = \u0026#34;\\r\u0026#34;, flush=True) erdos_endpoint.append(propagate_rumour(purge(erdos_renyi(100, 0.1)))[1]) preferential_endpoint.append(propagate_rumour(purge(preferential_attachment(100, 3)))[1]) lattice_endpoint.append(propagate_rumour(nx.to_numpy_matrix(nx.grid_2d_graph(10,10)))[1]) print(datetime.datetime.now()) 2021-12-14 16:22:29.960008 2021-12-14 17:12:37.746489 And we plot some histograms. Please note that I reduced the number of bins from 1000 to 100, since what I want to look at is not the precise distribution by exact time of ending, but rather a general idea about whether the simulation ends prematurely, towards the middle, at the end, or does not finish at all by the time of reporting.\nplt.subplots(figsize=(20, 5)) plt.subplot(1, 3, 1); plt.ylim(0, 100) plt.hist(erdos_endpoint, bins = 100); plt.title(\u0026#34;Time of ending for 1000 runs on Erdös-Rényi\u0026#34;); plt.subplot(1, 3, 2); plt.ylim(0, 100) plt.hist(preferential_endpoint, bins = 100); plt.title(\u0026#34;Time of ending for 1000 runs on Preferential Endoint\u0026#34;); plt.subplot(1, 3, 3); plt.ylim(0, 100) plt.hist(lattice_endpoint, bins = 100); plt.title(\u0026#34;Time of ending for 1000 runs on a 2D-Lattice\u0026#34;); plt.show() What we can see is that, in fact, the three gaphs are quite similar, if not identical. Part of this has to do with the reduction in the number of bins (grouping in groups of 10 necessarily reduces resolution), but part of this must also have to do with the underlying mechanisms of the data. This has greatly surprised me: I would have expected the Preferentially-attached network to be at least more rumour-prone than the erdos-renyi one, and I would have expected the 2D-lattice to provide a middle ground between them, maybe with rumours spreading for longer given that the network diameter is so large. That this didn\u0026rsquo;t happen seems to contradict my hypothesis from the last exercise, that network topology is determinist towards what will happen; of course, this could also be just an artifact due to the different enunciados. I dont know.\nAnyways, with regards to the found results, we can clearly identify three groups:\nEarly finishers: Actually, most of the trajectories in all networks end at some point between 1 and ~10 repetitions, so much so that I have been forced to cut the y-axis so that the rest of the endpoints could be seen. This has surprised me, but it actually makes sense: I selected such an small transmission value 0.3, that it is actually quite difficult for local cotillas to spread the word, specially since most of the nodes have an average of ~3 joined nodes. This was made on purpouse, no make the networks comparable, but, since $3 \\cdot 0.3 = 1$, it is actually difficult to spread things over. The fading fashion: For those rumours that manage to break out, there seems to be a normal-like distribution centered around $t = 300$: The rumour spreads for some time, until all individuals become inmune, and then it dies out. The enduring cult: Some rumours manage to fo even further, trespassing a \u0026ldquo;barrier\u0026rdquo; around 800 to live for longer, and maybe even perdure over time: around 20 simulations hadn\u0026rsquo;t actually ended by the time of reporting. Since the initial individual was selected at random, we can for sure say that this is not a relevant factor on the outcome: what, then, decides whether a rumour spreads out or not? What defines, then, wether an individual survives or not? With the graphs and the information that I have, I am sadly forced to say that this must be pure chance: if it was network topology, some differences would have been observed between the three histograms.\nAnother option is that network topology is relevant on the small scale, but that, with such a big number of repetitions, that variability becomes less relevant, forcing the model to \u0026ldquo;collapse\u0026rdquo; around the explained groups.\n","date":"December 15, 2021","hero":"/es/posts/master-en-biolog%C3%ADa-computacional/modelizaci%C3%B3n-de-sistemas-biol%C3%B3gicos/beyond-erdos-renyi/hero.png","permalink":"https://www.pablomarcos.me/es/posts/master-en-biolog%C3%ADa-computacional/modelizaci%C3%B3n-de-sistemas-biol%C3%B3gicos/beyond-erdos-renyi/","summary":"In the last exercise, we used an Erdös-Rényi random attachment model to analyze how interconnections influence the spread of illness over space. Erdös-Rényi, however, is not the only model that exists, and others, such as Preferential Attachment Networks, have the advantage of being more true to reality and an even closer representation of reality.\nIn these models, unlike in the random models, there is a preference when forming links between nodes: normally, nodes tend to cluster around the nodes with the largest number of linked nodes, promoting centralization and improving the connection between nodes; in this way, for example, the diameter of the network tends to decrease, since most nodes can be connected by traveling to the \u0026ldquo;mother node\u0026rdquo;.","tags":null,"title":"Preferential Attachment Networks"},{"categories":null,"contents":" In the last assignment, we explored the role that spacial distribution played in the evolution of different functions over time, but playing with Conway\u0026rsquo;s Game of Life. This approach, though really powerful for its level of simplicity, has some issues, the most obvious being that real human beings dont live in 3x3 lattices where they only interact with their closes neighbors; rather, as we can see just by looking around at human society, some people are more connected than other, some are connected to people far away and closer home, and some are not connected at all.\nTo model this, a mathematical approach exists too: networks, with their differnt topologies, are able to represent different kinds of interactions between human beings, making for useful tools in the study of how space affects phase transitions.\nIn this notebook, I will explore the power of the Erdös-Rényi model, a methodology for generating random networks which, when represented as graphs, allow us to precisely model the evolution of systems as diverse as epidemics or social networks, although, due to its random nature, this approximations tend not to be really precise.\nFirst, we get some necessary imports:\nimport numpy as np #Supports common numeric operations import matplotlib.pyplot as plt #For graphs import networkx as nx #For the creation, manipulation, and study of complex networks. import csrgraph as cg #For read-only graphs; we will use it for random walking from collections import Counter #Used to count elements in lists from scipy.stats import chi2_contingency #For statistical tests And, now, we generate the adjacency matrix that will be used to define the topology of our Erdös-Rényi network. This model is very simple: we start with n vertices, and, for each vertex pair, we draw a link with probability p under random conditions. Thus, following the pseudocode given by the teacher:\ndef erdos_renyi(nodes, link_prob): x = np.zeros((nodes, nodes),int) #start with n vertices for i in range(nodes): for j in range(nodes): if np.random.uniform(0, 1) \u0026lt; link_prob: x[i,j] = x[j,i] = 1 return(x) And now, we generate the matrix:\nmy_matrix = erdos_renyi(100, 0.9) But, wait\u0026hellip; ¿is that it? How do we understand how our network works? How to we interpret the physical significancy of a simple adjacency matrix? Here, I propose to use python\u0026rsquo;s networkx library to plot it, and see how it looks like:\ndef plot_adjacency(adjacency_matrix): gr = nx.from_numpy_matrix(adjacency_matrix) mylabels = dict(zip(range(len(adjacency_matrix)), range(len(adjacency_matrix)))) nx.draw(gr, node_size=200, edge_cmap = \u0026#34;Greys\u0026#34;, labels=mylabels, with_labels=True) return nx.diameter(gr) plt.subplots(figsize=(20, 5)) plt.subplot(1, 2, 1) d = plot_adjacency( erdos_renyi(100, 0.9) ); plt.title(f\u0026#34;Network diameter: {d}\u0026#34;) plt.subplot(1, 2, 2) d = plot_adjacency( erdos_renyi(100, 0.1) ); plt.title(f\u0026#34;Network diameter: {d}\u0026#34;) plt.show() As we can see, there are problems with the pre-defined values we chose! 100 elements are so many elements that understanding the network\u0026rsquo;s topology just by looking at it becomes almost impossible; thus, we have decided to use completely different starting values: by using only 35 members and a probability of linkage of 0.1, we can easily understand what is happening:\narray = np.loadtxt(\u0026#34;./adjacency_matrix.csv\u0026#34;, delimiter=\u0026#34;,\u0026#34;) d = plot_adjacency( array ); plt.title(f\u0026#34;Network diameter: {d}\u0026#34;); plt.show() (To make the results more reproducible, I have provided with a pre-cooked adjacency matrix; nothing special, but it keeps things consistent)\nAnother advantage with working with these parameter lies within the concept of diameter, the shortest path between the two most distant nodes. In networks with a big number of nodes or of interconnectedness, the diameter rapidly becomes 1 or really close to 1, given how related all nodes are. This can be seen in our first graphs: (depending on how your random number generators behaved), the network diameter either remains 2 or increases to 3; with our approach, the network diameter gets to 4, which is not super high but is more interesting than just 2.\nAnother potential problem is disconnected networks: ¿what happens if some points appear disconnected from the rest of the matrix? This could be problematic when doing a random walk, so we will only take the biggest component from our graph:\ngraph = nx.from_numpy_matrix( array ) #Create graph sub_graph = graph.subgraph( (sorted(nx.connected_components(graph), key=len, reverse=True))[0] ) #Subset after ordering subarray = nx.to_numpy_matrix( sub_graph ) #Save np.ndarray d = plot_adjacency( subarray ); plt.title(f\u0026#34;Network diameter: {d}\u0026#34;); plt.show() (The graph might look a bit different, but its the same as the one before (there were no subnetworks to eliminate); you can, for example, look at the 4 and only 4 auto-connected networks)\nNow, to understand how network topology affects information transmission, we will perform a random walk. In this though experiment, we will imagine a traveler, Mariano, that goes through the network one step at a time, seeing what he finds an then moving on. We will record Mariano\u0026rsquo;s position every step on the way, and, when he gets tired (after 100000 steps-phew!), we will save his position and use it to plot an histogram of the frequency (and thus, the probability) of him ending up in a given location. For this, we use the csrgraph package:\nsubgraph_cg = cg.csrgraph(sub_graph, threads=12) walks = subgraph_cg.random_walks(walklen= nx.diameter(sub_graph) + 100, #𝑡 timesteps, with 𝑡 ≫ 𝑑 epochs=100000, #A very large number of repetitions start_nodes=np.random.randint(len(subarray)), #Random initial conditions return_weight=1, #Probability of going back neighbor_weight=1) #Probability of moving to a new node And, we extract the last value from the np.ndarray that csrgraph has generated for us, creating the histogram:\nplt.subplots(figsize=(20, 5)) plt.subplot(1, 2, 1) plt.hist(walks[:, -1], bins = 35); plt.title(\u0026#34;Histogram of position after 100000-step random walk\u0026#34;); plt.subplot(1, 2, 2) d = plot_adjacency( subarray ); plt.title(f\u0026#34;Network diameter: {d}\u0026#34;) plt.show() As we can see, the distribution is not at all random; some nodes, such as 25, concentrate lots of final positions, while you will hardly see Mariano in node 18 or, notably, in node 6. This is clearly a more \u0026ldquo;human\u0026rdquo; way of representing networks: maybe node 25 is a capital city, concentrating lots of infrastructure to connect it to other dots; and maybe 6 is an isolated village in the north pole, where you would hardly find any human.\nFor the next part of the assignment, we wish to simulate the spreading of an infection across towns.We will start with all towns, save by one (chosen at random), free of the disease, and one of them with a prevalence of 0.1. Lets see:\ndisease_conditions = new_disease_conditions = dict(zip(range(len(sub_graph)), np.zeros(len(sub_graph)))) disease_conditions[np.random.randint(len(sub_graph))] = 0.1 incrementer = 1.01; movility = 1; counter = 0 Now, assuming that each node represents a town and that edges represent mobility patterns across towns, we will proceed to recursively update the incidence among towns, such that any neighbour of an existing town will see its incidence increase by 1% multiplied by a movility factor $m$, in this case, 1 for simplicity. We will keep doing this until all towns reach a prevalence of 1000, and, then, we will stop. Thus:\nwhile any(values \u0026lt; 1000 for values in disease_conditions.values() ): for element in range(len(sub_graph)): #For each of the elemets of the graph for neighbor in sub_graph.neighbors(element): #Find its neighbors new_disease_conditions[neighbor] = (disease_conditions[neighbor] + disease_conditions[element] * movility * incrementer) disease_conditions = new_disease_conditions counter += 1 print(f\u0026#34;Treshold reached in {counter} generations\u0026#34;) Treshold reached in 3 generations As we can see, ¡the treshold is reached quite quickly, after less than 5 generations! This seems crazy to me, and is way less than I expected, but I guess it has to do with the high level of interconectedness of the network (remember, its diameter was 4) and with the fact that there are some self-linked nodes. This self-linked nodes are not an error, but rather represent cities with high levels of inner-connectedness; maybe they have a really good public transportation grid, or maybe they represent a small town were everyone knows each other and that just emmerged from some good ol\u0026rsquo; \u0026ldquo;fiestas del pueblo\u0026rdquo;. Now, we would like to see how the distribution is going:\nplt.subplots(figsize=(20, 5)) plt.title(\u0026#34;Histogram for disease prevalence\u0026#34;); plt.bar(disease_conditions.keys(), disease_conditions.values()); As you can see, the results are staggering! Some nodes, such as #10, present a skyrocket-high incidence rate, while others, such as #7, have just barely reached the treshold, with #27 the most likely culprit as the last node to reach it (and thus, the more overall isolated one). It should be noted that, indeed, #27 is always the culprit, at least in the 10 times I have run the simulation; this tells us aa lot about how deterministic the network topology is in the outcome, in this case, of the epidemic.\nNote: you can see that the bin #35 is empty. This is not an error, but rather a consequence of the programming language chosen: python starts indexing at 0, and, if you look at the initial graphs, there has never been a node #35\nNow, we would like to see how this parameters change if we modify the movility factor from 1 to 0.1, and, later, to 0.01. Lets thus define a helper function:\ndef helper_function(movility, incrementer, seed): disease_conditions = new_disease_conditions = dict(zip(range(len(sub_graph)), np.zeros(len(sub_graph)))) disease_conditions[np.random.randint(len(sub_graph))] = seed; counter = 0 while any(values \u0026lt; 1000 for values in disease_conditions.values() ): for element in range(len(sub_graph)): #For each of the elemets of the graph for neighbor in sub_graph.neighbors(element): #Find its neighbors new_disease_conditions[neighbor] = (disease_conditions[neighbor] + disease_conditions[element] * movility * incrementer) disease_conditions = new_disease_conditions counter += 1 plt.title(f\u0026#34;Histogram for disease prevalence after {counter} generations, with movility = {movility}\u0026#34;); plt.bar(disease_conditions.keys(), disease_conditions.values()); And we plot:\nplt.subplots(figsize=(20, 5)) plt.subplot(1, 2, 1) helper_function(0.1, 1.01, 0.1) plt.subplot(1, 2, 2) helper_function(0.01, 1.01, 0.1) plt.show() As we can see, there is really no big difference between the first and the second graphs, other that one takes way more steps to reach our desired treshold than the other. This seems to support the theory I developed earlier (and that, to be quite frank, I am not sure if its the case or not) that it is the network topology that is deterministic towards a result, and not the initial conditions (up to a certain extent, obviously).\nFinally, we are interested in which measure of network centrality (degree or eigenvector centrality) correlates the best with the distribution of probability (in the RW) or prevalence (in the disease spreading) across nodes at the end of the simulation. This should be easy to calculate!\na = dict(Counter(walks[:, -1])) #Some work is needed to count and order the list of values in walks[:, -1] a_list = sorted(dict(Counter(walks[:, -1]))) #Thanks to stack overflow for the code :D probability = list(dict(sorted(a.items(), key=lambda pair: a_list.index(pair[0]))).values()) #For eigenvector centrality and prevalence s, p, f, d = chi2_contingency([list(nx.eigenvector_centrality(sub_graph).values()), list(disease_conditions.values())]) print(f\u0026#34;The Chi Squared test found a p-value of {p} under {f} degrees of freedom [E-Pre]\u0026#34;) #For eigenvector centrality and probability s, p, f, d = chi2_contingency([list(nx.eigenvector_centrality(sub_graph).values()), probability]) print(f\u0026#34;The Chi Squared test found a p-value of {p} under {f} degrees of freedom [E-Pro]\u0026#34;) #For degree and prevalence s, p, f, d = chi2_contingency([list(dict(sub_graph.degree).values()), list(disease_conditions.values())]) print(f\u0026#34;The Chi Squared test found a p-value of {p} under {f} degrees of freedom [D-Pre]\u0026#34;) #For degree and probability s, p, f, d = chi2_contingency([list(dict(sub_graph.degree).values()), probability]) print(f\u0026#34;The Chi Squared test found a p-value of {p} under {f} degrees of freedom [D-Pro]\u0026#34;) The Chi Squared test found a p-value of 0.9999999997346068 under 34 degrees of freedom [E-Pre] The Chi Squared test found a p-value of 1.0 under 34 degrees of freedom [E-Pro] The Chi Squared test found a p-value of 3.867403696778631e-22 under 34 degrees of freedom [D-Pre] The Chi Squared test found a p-value of 1.0 under 34 degrees of freedom [D-Pro] Curiously enough, it seems like eigenvector centrality is a really, really good predictor of outcomes both in prevalence and in probability (although here I used raw occurrence instead of probability since, for a chi-squared test, they are interchangeable). This makes sense: eigenvector centrality is a measure of the influence of a node in a network, and, the more influential a node is, the more likely it will have travellers (@Mariano), or, if an epidemic breaks out, of getting infected the first and suffering it the most.\nFor degree, a weird thing happens: it is a really good predictor of probability, but a really, really bad predictor of prevalence. This, I am guessing, will most likely have to do with self-attached nodes: since they are autoconnected, they will have an artificially high degree value, which might bode well for probability of appearance, since it would just mean Mariano spends more time in the node; but might mess up with probability, as these networks, I theorized, act as \u0026ldquo;concentrators\u0026rdquo; and \u0026ldquo;seeders\u0026rdquo; of the virus, just as Wuhan\u0026rsquo;s market did with CoViD.\nIn any case, these super close correlations support my hypothesis that it is network topology, and not initial conditions, that determine evolution of events in the network. This, of course, should be further explored, but, if confirmed, would provide a new way to look at issues such as racism and segregation see this relatedwebpage, which, if network configuration is so relevant, would proof much more difficult to fix.\n","date":"December 15, 2021","hero":"/es/posts/master-en-biolog%C3%ADa-computacional/modelizaci%C3%B3n-de-sistemas-biol%C3%B3gicos/networks/hero.png","permalink":"https://www.pablomarcos.me/es/posts/master-en-biolog%C3%ADa-computacional/modelizaci%C3%B3n-de-sistemas-biol%C3%B3gicos/networks/","summary":"In the last assignment, we explored the role that spacial distribution played in the evolution of different functions over time, but playing with Conway\u0026rsquo;s Game of Life. This approach, though really powerful for its level of simplicity, has some issues, the most obvious being that real human beings dont live in 3x3 lattices where they only interact with their closes neighbors; rather, as we can see just by looking around at human society, some people are more connected than other, some are connected to people far away and closer home, and some are not connected at all.","tags":null,"title":"The Erdös-Rényi model"},{"categories":null,"contents":" In the previous assignment, we managed, using ligand Based Virtual Screening, to obtain a list of those molecules that we considered to have \u0026quot;high affinity\u0026quot; with finasteride, a selective inhibitor of 5α-reductase type II, the enzyme that converts testosterone into dihydrotestosterone in the scalp and, therefore, that causes baldness. The idea was thus to find a molecule similar to finasteride and with the same properties, but even more selective and with greater affinity, in order to improve its therapeutic properties.\nIn this assignment, the idea is to perform a pharmacophore-based virtual screening. In our case, the pharmacophore we design will already be selective, as it is based on the properties of finasteride; therefore, what we are looking for is to improve as much as possible the affinity, which we described in the initial assignment through its IC~50~ of 16 nM.\nTo do this, the first step is to enter PDB and search for structures of our target protein (5α-reductase) associated with the selective inhibitor we have previously chosen (finasteride). In this case, we have only managed to find only one result,1 which is coupled with NADPH as a reaction intermediary.\nFigure 1. The Finasteride-coupled-to-NADPH complex I found in PDB. In yellow, with the joining Carbon atom highlighted in green, the bond between FNS (right, front) and NADPH (back). Nottice some SO4-2 ions on the back: I believe they act as coadyuvants.\nThis complex structure has raised several serious issues for me. Is it appropriate to cut the NADPH bond, and recreate the double bond in order to have a \u0026quot;guide molecule\u0026quot; that has at least the right structure to bind to the 5-aR? Or, on the contrary, is this a bad idea since, although the structure of the complex may be adequate to interact with the enzyme, it may not be, for example, adequate to access the active center? Does it make sense to cut the molecule in this way, when surely the binding of NADPH itself modifies its spatial conformation?\nIt is because of all these doubts that I have decided to dispense with this molecule. I am aware that, in doing so, I give up the possibility of having a guide molecule with the appropriate conformational structure to bind to the enzyme in question, but, in exchange, I make sure that I am not touching parameters that I do not fully understand and that may end up messing up the pharmacophore even more. In addition, PDB as a format does not support the double bonds2 that Finasteride presents, and, as much as I have tried with Chimera, I have not found a way to simulate them.\nTherefore, to make up for this lack of valid crystallographic structures, I will take the 4 best hits from the file 'SimilarityAnalysisFinasteride.csv', measured by Dice (MACCS), which recall that in the previous assignment we had defined as the best similarity measurement method. Excluding CHEMBL710 (which has an score of 1.0, since it is our initial drug, finasteride), we get the following list:\nCHEMBL2282655 CHEMBL2282779 CHEMBL2282654 CHEMBL2282782 These four compounds all come from the same paper,3 have the same mechanism as Finasteride (competitive inhibition) and use the same \u0026ldquo;bolsillo protéico\u0026rdquo;, which means they will be quite similar, and thus good at predicting a pharmacophore.3\nOnce we have added the three compounds + the Finasteride to Chimera, we add Hydrogens, add Charges and minimize the structure five times, in order to provide as accurate a result as possible, and save as mol2.\nFigure 2. Molecule’s structure before (L) and after (R) minimization. Notice how the minimized structure is much more “loose”, letting molecules have their way.\nFor the next step, we would like to create a pharmacophore out of these molecules. For this, we merge the 4 individual mol2 files into one, and we upload it to PharmaGist.4 To account for its hight lipophilicity, the \u0026ldquo;Hydrophobic\u0026rdquo; advanced option could have been upped a bit, but, since I didn\u0026rsquo;t know by how much, I prefered to leave the default values. Once we get the results, we select the target with the best score from the alignments that include the 4 molecules, and download the JMOL file, which includes our pharmacophore.\nNow that we have created our pharmacophore, we can proceed with the part of interest in our analysis: the pharmacophore based virtual screening. For this, we will use Pharmit: first, we create a new database from the compounds discovered in Assignment 1 (SimilarityAnalysisFinasteride.csv), which we convert to .msi format by removing all columns except for SMILES and CHEMBLID and concatenating both of them with an space in between. Once we are sure the database has been correctly created (in our case, Finasteride-Similar 5ARIs KQMJ9Y, with 2,832 conformers and192 compounds), we can proceed with the scanning: we enter pharmit search, search for our database from the \u0026ldquo;contributed libraries\u0026rdquo; panel and add our pharmacores under the \u0026ldquo;Load features\u0026hellip;\u0026rdquo; button.\nUnfortunately, there are no hits for our chosen pharmacophore: this most likely means its structure is too restrictive, and thus that pharmit has problems finding matches. No problem! We can continue searching for matches using PharmaGist\u0026rsquo;s list of suitable pharmacophores, descending one-by-one in order of Score: for #6, we found 6 matches, and 9 matches for #13; still a bit too little for our analysis. However, when we go for #10, ¡bingo! 65 matches are found, which reduce to 25 when we select 1 hit per configuration and per molecule. After ordering the matches in order of decreasing RMSD, we can save the .sdf results file.\nAnd, just like this, ¡we have some drug candidates! Now, to make sure this drugs are good enough for clinical use, we would like to try and predict some of their ADME properties. The term ADME, which stands for Absorption, Distribution, Metabolism, and Excretion, describes how the drug interacts with the target organism, since the levels and kinetics of drug exposure to different tissues influence the degradation and processing of the drug, and thus its activity and efficiency. For this, we will use the accompanying Jupyter Notebook, which was kindly provided to us by the teacher.\nThis notebook takes the .sdf file and transforms it to CSV, which is more manageable, and evaluates whether the compounds fulfill Lipinski\u0026rsquo;s rule of five. That rule, developed at Pfizer in 1997, evaluates how likely a drug candidate is to be an effective oral drug in humans, based on the idea that most orally-active compounds are small, lipofillic molecules.5 It thus demands that the mollecular weight is \u0026lt; 500, that there are no more than 10 Hydrogen Bond acceptors and no more than 5 HB donors, and that its n-octanol-water partition coefficient (a measure of the relationship between lipophilicity and hydrophilicity) does not exceed 5 (high lipophilicity, but not too high).\nIf all 5 rules (or 4, since Lipinski originally merged the HB rules in one) are met, and if the Topological Polar Surface Area is less than 140 (since polar mollecules are bad at permeating cell membranes), the data is saved into a new file, \u0026lsquo;MolDB_Ro5.csv\u0026rsquo;, which you can find attached. We could also have allowed a rule to be broken without exclusion.; however, I found that too permissive, as it left out only 2 molecules (full rule-following left 13 in and 12 out, a ~50/50 split).\nTo better understand the statistical distribution of our drugs, we made the following graphs:\nA radar plot of the values of our selected \u0026ldquo;drug candidates\u0026rdquo;: here, we can see that none exceed the \u0026ldquo;blue area\u0026rdquo; of accepted values under the rule of five, although the more \u0026ldquo;dangerous\u0026rdquo; values (i.e. those that risk causing an exclusion the most) are molecular weight and n-octanol-water partition coefficient (here specified as LogP in log scale) Figure 3. Radar plot of the Ro5 (and therefore ADME) classification of those drugs that previously met the Rule of Five.Note how the characteristics with the highest deviation are molecular weight and logP\nSome histograms of the Ro5 properties of our drugs. Since we only found 13, it is a bit more difficult to extract conclussions from a histogram, but we can still see that the parameters with the greatest variance are in the number of hidrogen bond donors and acceptors, which can also be seen in the previous graphs. Molecular weight is actually pretty similar, which must mean that those molecules with more HBD/A are smaller, since hidrogen-bond formating atoms (usually N and O) are bigger than Carbon Figure 4. Histogram of the Ro5 properties of our compounds. As can be seen, the greatest variance is in the number of hydrogen bond acceptors and donors, which indicates, together with the low variance in molecular mass, that the molecules must vary in size.\nAnd, ¡that would be it! With the rule-of-five acting as a proxi for ADME properties acting as a proxy to ensure compliance with ADMET properties, these would be possible drugs to study as a substitute for finasteride. It remains for future researchers both to analyze the in vivo inhibitory capacity, side effects and viability of the drug, and to find more crystallographic structures for finasteride itself, which currently, to my surprise, is not as extensively studied as one would expect for one of the most important enzymes in the human (or at least in the male) body.\nReferences Bank, RCSB Protein Data. RCSB PDB - 7BW1: Crystal Structure of Steroid 5-Alpha-Reductase 2 in Complex with Finasteride. https://www.rcsb.org/structure/7BW1. Accessed 12 december 2021.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n«PDB File Format and Double Bonds». Avogadro Discussion, 19 de febrero de 2016, https://discuss.avogadro.cc/t/pdb-file-format-and-double-bonds/2293.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nKumar, Rajnish, y Manoj Kumar. «3D-QSAR CoMFA and CoMSIA Studies for Design of Potent Human Steroid 5α-Reductase Inhibitors». Medicinal Chemistry Research, vol. 22, n.o 1, enero de 2013, pp. 105-14. Springer Link, https://doi.org/10.1007/s00044-012-0006-1.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nSchneidman-Duhovny, D., et al. «PharmaGist: A Webserver for Ligand-Based Pharmacophore Detection». Nucleic Acids Research, vol. 36, n.o Web Server, mayo de 2008, pp. W223-28. DOI.org (Crossref), https://doi.org/10.1093/nar/gkn187.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nLipinski, Christopher A., et al. «Experimental and Computational Approaches to Estimate Solubility and Permeability in Drug Discovery and Development Settings1PII of Original Article: S0169-409X(96)00423-1. The Article Was Originally Published in Advanced Drug Delivery Reviews 23 (1997) 3\u0026ndash;25.1». Advanced Drug Delivery Reviews, vol. 46, n.o 1, marzo de 2001, pp. 3-26. ScienceDirect, https://doi.org/10.1016/S0169-409X(00)00129-0.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","date":"December 12, 2021","hero":"/es/posts/master-en-biolog%C3%ADa-computacional/lead-discovery/assignment-2/hero.png","permalink":"https://www.pablomarcos.me/es/posts/master-en-biolog%C3%ADa-computacional/lead-discovery/assignment-2/","summary":"In the previous assignment, we managed, using ligand Based Virtual Screening, to obtain a list of those molecules that we considered to have \u0026quot;high affinity\u0026quot; with finasteride, a selective inhibitor of 5α-reductase type II, the enzyme that converts testosterone into dihydrotestosterone in the scalp and, therefore, that causes baldness. The idea was thus to find a molecule similar to finasteride and with the same properties, but even more selective and with greater affinity, in order to improve its therapeutic properties.","tags":null,"title":"Designing a pharmacophore and ensuring ADMET properties"},{"categories":null,"contents":"The existence or not of free will is one of the problems that have most tormented mankind over the centuries, eliciting reactions from fields as varied as psychology, biology and physics.\nThe field of neurology does not seem to offer conclusive answers to this question. The Lisbet experiment, for example, demonstrated that the actions we take are previously and unconsciously decided by the brain, and that it is then that our consciousness, our \u0026ldquo;inner voice\u0026rdquo;, feels the desire to perform these actions, thus ruling out the possibility of a thinking entity outside the constraints of the physical world. We must, therefore, go deeper into the matter.\nIn the field of mathematics, the advent of Conway\u0026rsquo;s Game of Life (GoL) has meant, according to Wolfram 2002, \u0026ldquo;the single most surprising scientific discovery I have ever made\u0026rdquo;. The possibility of simulating terribly complex interactions from a really small set of rules makes us wonder about the very nature of the universe: is it possible that, just as life arose from simple molecules, we could obtain constantly evolving intelligences and thinking beings if we had a large enough GoL? Is it possible, then, that we are nothing more than mere toys of the universe, particles subject to the immutable rules of physics whose path is predefined?\nAt the beginning of the research conducted for this article, I had the impression, which I think is quite common, that the rules of quantum physics (our universe\u0026rsquo;s equivalent of Conway\u0026rsquo;s rules in the GoL) left, due to their probabilistic nature, a space of hope for free will; however, it is true that this theory is as deterministic as Newtonian physics, and, given that decisions, however difficult to predict, cannot be free if they are enclosed by the laws of probability, this does not seem an interesting path to follow for the defenders of free will.\nHowever, there is still some work to be done in this area, since we do not yet know whether our brain is a decoherent system (i.e. one in which the probabilities of the quantum wave function tend to collapse on a deterministic path) or a chaotic (i.e. unpredictable) one, which would allow us to glimpse, among the randomness of chaos, gaps for free will.\nEven more: recent developments, such as Dennett 2003, have shown that, even if a system is deterministic, its consequences need not be inevitable: although in the microscopic world particles follow predefined trajectories, in the abstract world in which we humans move, based on concepts, it is possible to avoid the consequences of laws based on such concepts. One can establish, for example, a law such that \u0026ldquo;in the GoL, Eaters eat Gliders\u0026rdquo;; or \u0026ldquo;on planet Earth, lynxes eat rabbits\u0026rdquo;; however, just as a lynx can stumble and the rabbit escape, a glider can, in certain cases, escape from an Eater, and so can, on occasion, a human being make unexpected decisions of his own, at least in the abstract layer that is the one that really matters to us.\nIt is because of this (besides, let\u0026rsquo;s not fool ourselves, because of my pre-existing personal convictions) that I prefer to opt for the side that defends the existence of free will, even if not consciously, but internally, because it leaves a free space for our ability to act to improve the world as much as possible. In any case, the debate is still open, as there is no conclusive evidence either way; and while it is impossible to know, at least for the moment, how much space the universe leaves for human action, what we can say with certainty is that our perpetual interest in the subject, which spans multiple disciplines over the centuries, says a lot about our desires as a species.\nBased on content from the Stanford Encyclopedia of Philosophy\nReferences Baldwin, John. \u0026ldquo;Stephen Wolfram . A New Kind of Science, Wolfram Media, Inc, Champaign, IL, 2002, Xiv + 1197 Pp.\u0026rdquo; Bulletin of Symbolic Logic, vol. 10, no. 1, March 2004, pp. 112-14. Cambridge University Press, https://doi.org/10.1017/S1079898600004200.\nBerto, Francesco, and Jacopo Tagliabue. \u0026ldquo;Cellular Automata.\u0026rdquo; The Stanford Encyclopedia of Philosophy, edited by Edward N. Zalta, Spring 2021, Metaphysics Research Lab, Stanford University, 2021. Stanford Encyclopedia of Philosophy, https://plato.stanford.edu/archives/spr2021/entries/cellular-automata/.\nDennett, Daniel Clement. Freedom Evolves. New York : Viking, 2003. Internet Archive, http://archive.org/details/freedomevolves00denn.\nEL EXPERIMENTO DE LIBET | El experimento que dice que no existe el libre albedrío. www.youtube.com, https://www.youtube.com/watch?v=wESoyC5Y9lU. Accessed December 9, 2021.\n¿Existe el Destino? www.youtube.com, https://www.youtube.com/watch?v=NvuCZJyoing. Accessed December 9, 2021.\n¿Somos realmente LIBRES de decidir nuestro futuro? www.youtube.com, https://www.youtube.com/watch?v=hXZiJ6TlFh8. Accessed December 9, 2021.\nNOTE: I have decided to make the essay a bit longer due to the text outlay of LaTeX, which shows much less text on screen than, for example, LibreOffice. This document can be easily converted to PDF using:\n````pandoc \u0026ndash;pdf-engine=xelatex \u0026ldquo;Essay.md\u0026rdquo; -o \u0026ldquo;Essay on Free Will.pdf\u0026rdquo;```.\n","date":"December 9, 2021","hero":"/es/posts/master-en-biolog%C3%ADa-computacional/modelizaci%C3%B3n-de-sistemas-biol%C3%B3gicos/essay-on-free-will/hero.jpg","permalink":"https://www.pablomarcos.me/es/posts/master-en-biolog%C3%ADa-computacional/modelizaci%C3%B3n-de-sistemas-biol%C3%B3gicos/essay-on-free-will/","summary":"The existence or not of free will is one of the problems that have most tormented mankind over the centuries, eliciting reactions from fields as varied as psychology, biology and physics.\nThe field of neurology does not seem to offer conclusive answers to this question. The Lisbet experiment, for example, demonstrated that the actions we take are previously and unconsciously decided by the brain, and that it is then that our consciousness, our \u0026ldquo;inner voice\u0026rdquo;, feels the desire to perform these actions, thus ruling out the possibility of a thinking entity outside the constraints of the physical world.","tags":null,"title":"Is free will or its counterpart, determinism, just a delusion?"},{"categories":null,"contents":" A cellular automaton is a discrete model of computation consisting on a regular grid of cells, each in one of a finite number of states, such as on and off. For each cell, a set of cells called its neighborhood is defined relative to the specified cell. An initial state is selected by assigning a state for each cell. A new generation is created according to some fixed rule that determines the new state of each cell in terms of the current state of the cell and the states of the cells in its neighborhood. Typically, the rule for updating the state of cells is the same for each cell and does not change over time, and is applied to the whole grid simultaneously.\nThe concept was discovered in the 1940s by Stanislaw Ulam and John von Neumann, and it was popularized in the 80s by John Horton Conway\u0026rsquo;s Game of Life, a two-dimensional cellular automaton in which all possible evolutions depend on the initial state\nFirst task: The elementary cellular automata The first task for this assignment is to use a 1D cellular automata to analyse the effect of a series of \u0026ldquo;rules\u0026rdquo;, which are mathematic functions that determine the new state of each cell in terms of the current state of the cell and the states of the cells in its neighborhood. We are presented with the following three rules, which tell us the new state for the center cell as a function of the current pattern:\nRule Number 111 110 101 100 011 010 001 000 Current Pattern 45 0 0 1 0 1 1 0 1 New state for center cell 48 0 0 1 1 0 0 0 0 New state for center cell 110 0 1 1 0 1 1 1 0 New state for center cell To do this easily, I reused some code found on this blog post from Towards Data Science, a magnificent blog I trully and sincerely recommend. I adapted the code so that it is callable as a function, with a set of initial conditions that can be specified by the user, and so that it is more simple, readable and understandable. We start by importing some necessary modules:\nimport numpy as np #Supports common numeric operations from scipy.signal import convolve2d #For step updating from matplotlib import animation #Creates animations (which we can then display as HTML, gif, etc) import matplotlib.pyplot as plt #For graphs from IPython import display #To display things on Jupyter; in this case, the animations We then define the function that updates the steps in our linear world:\ndef step_linear(x, rule_binary): x_shift_right = np.roll(x, 1); x_shift_left = np.roll(x, -1) y = np.vstack((x_shift_right, x, x_shift_left)).astype(int) z = np.sum(np.array([[4], [2], [1]]) * y, axis=0).astype(int) return rule_binary[7 - z] And the animation function:\ndef animate_linear(initial_conditions, rule_number, steps = 200, figtype = \u0026#34;animate\u0026#34;, noaxis = True, save = False): assert 0 \u0026lt;= rule_number \u0026lt;= 255 #The binary rule\u0026#39;s format cell_numb = len(initial_conditions) rule_binary_str = np.binary_repr(rule_number, width=8) #Convert decimal to binary rule_binary = np.array([int(ch) for ch in rule_binary_str], dtype=int) x = np.zeros((steps, cell_numb), dtype=int) x[0, :] = initial_conditions #The first row is the initial conditions vector for i in range(steps - 1): #Populate the animated matrix x[i + 1, :] = step_linear(x[i, :], rule_binary) global ax; global fig #Make external axis accesible if noaxis == True: #Redundant, could remove \u0026#39;\u0026#39;== True\u0026#39; fig = plt.figure() #Set axis where we will animate ax = plt.axes() def animate_lin(i): ax.clear(); ax.set_axis_off() #Restart for each iteration Y = np.zeros((cell_numb, cell_numb), dtype=int) upper_boundary = (i + 1) #Show 1 iteration per frame lower_boundary = 0 if upper_boundary \u0026lt;= cell_numb else upper_boundary - cell_numb for t in range(lower_boundary, upper_boundary): Y[t - lower_boundary, :] = x[t, :] img = ax.imshow(Y, interpolation=\u0026#39;none\u0026#39;,cmap=\u0026#39;gray_r\u0026#39;) return [img] if figtype == \u0026#34;animate\u0026#34;: anim = animation.FuncAnimation(fig, animate_lin, frames=cell_numb, interval=50, blit=True) if save == True : anim.save(f\u0026#39;Linear_cell_automata_rule_{rule_number}.gif\u0026#39;) video = anim.to_html5_video(); html = display.HTML(video) display.display(html); else: animate_lin(steps-1) ax.set_title(f\u0026#34;Rule {rule_number}\u0026#34;, fontsize=15) And, ¡that\u0026rsquo;s it! We now have a simple system that can tell us how a cellular automaton works in a smart, inrteresting and interactive way. To further study this system, the enunciado tells us to test 3 different rules in 5 different scenarios, to see what happens. Lets go!\nFirst, I will try and see what happens if we put \u0026ldquo;A single black (1) cell in the middle of the array\u0026rdquo;. To make the data visualization easier on the eyes, more manageable and more understandable, I will just plot the static map which shows the full evolution of the cellular automata, with each rown being a point in time, starting at the top and ending at the bottom; an animated version can be accessed by changing type to \u0026ldquo;animated\u0026rdquo;.\ninitial_conditions = np.zeros(100); initial_conditions[50] = 1 fig = plt.figure(figsize=(20, 30)) ax = fig.add_subplot(131) animate_linear(initial_conditions, 45, figtype = \u0026#34;static\u0026#34;, noaxis = False); ax = fig.add_subplot(132) animate_linear(initial_conditions, 48, figtype = \u0026#34;static\u0026#34;, noaxis = False); ax = fig.add_subplot(133) animate_linear(initial_conditions, 110, figtype = \u0026#34;static\u0026#34;, noaxis = False); Then, we can see what happens when the initial conditions are \u0026ldquo;Half black (1) cells and half white (0) cells at randomly chosen positions\u0026rdquo;\ninitial_conditions = np.random.choice([0, 1], size=(100,), p=[0.5, 0.5]) fig = plt.figure(figsize=(20, 30)) ax = fig.add_subplot(131) animate_linear(initial_conditions, 45, figtype = \u0026#34;static\u0026#34;, noaxis = False); ax = fig.add_subplot(132) animate_linear(initial_conditions, 48, figtype = \u0026#34;static\u0026#34;, noaxis = False); ax = fig.add_subplot(133) animate_linear(initial_conditions, 110, figtype = \u0026#34;static\u0026#34;, noaxis = False); When there are \u0026ldquo;25% of black (1) cells and 75% of white (0) cells (approximately)\u0026rdquo;\ninitial_conditions = np.random.choice([0, 1], size=(100,), p=[0.75, 0.25]) fig = plt.figure(figsize=(20, 30)) ax = fig.add_subplot(131) animate_linear(initial_conditions, 45, figtype = \u0026#34;static\u0026#34;, noaxis = False); ax = fig.add_subplot(132) animate_linear(initial_conditions, 48, figtype = \u0026#34;static\u0026#34;, noaxis = False); ax = fig.add_subplot(133) animate_linear(initial_conditions, 110, figtype = \u0026#34;static\u0026#34;, noaxis = False); And when there are \u0026ldquo;90% of black (1) cells and 10% of white (0) cells (approximately)\u0026rdquo;\ninitial_conditions = np.random.choice([0, 1], size=(100,), p=[0.1, 0.9]) fig = plt.figure(figsize=(20, 30)) ax = fig.add_subplot(131) animate_linear(initial_conditions, 45, figtype = \u0026#34;static\u0026#34;, noaxis = False); ax = fig.add_subplot(132) animate_linear(initial_conditions, 48, figtype = \u0026#34;static\u0026#34;, noaxis = False); ax = fig.add_subplot(133) animate_linear(initial_conditions, 110, figtype = \u0026#34;static\u0026#34;, noaxis = False); I must say I am really, really really proud of this code. It is simple, customizable, good, made by me (with some help from the internet of course) and serves a wide variety of use cases! It really deserves a good grade :D\nNow, al meollo: can we see repeated patterns? How long does it take for a pattern to repeat?. Although this visualization makes it easy for the human eye to detect $ 2D$ patterns, this is misleading: our colony is lineal only, and pattern repetition should be analyzed line by line, not in two dimensions.\nTaking this into account, we can still say that there are some repeated patterns, which ar the most obvious for rule 110. Though not regular, the colonies of cells that follow this rule seem to follow a kind of pattern, which, in 2D, ends up looking like stairs and steps with some isolated boxes in between. Rule 110 has indeed proven to be in the boundaries between chaos and stability, and might actually be the smallest nown Turing-complete machine (i.e., one which is able to recognize or decide on other data-manipulation rule sets).\nFor rule 48, no interesting patterns are observed: just simple lines of alive cells, maybe like unicellular kelp growing undisturbed on a lake. For rule 45, however, an interesting sort of \u0026ldquo;pattern\u0026rdquo; emmerges, although, just by myself, I cannot be quite sure whether it is chaotic or just random. Some have suggested, however, that at least some periodicy can be found on its diagonals, so this system might lean more on the chaotic side than on the random one.\nWe were also asked to figure out wether the array length influences the time it takes for a pattern to repeat. To analyze this, I have chosen rule 110, which seems to exhibit a simple and clearly recognizable pattern. I have knowingly regenerated the random vector on each iteration, and chosen the first 25, 50 and 75 values, with the full 100 values availaible in the last point.\ninitial_conditions = np.random.choice([0, 1], size=(100,), p=[0.1, 0.9]) fig = plt.figure(figsize=(20, 30)) ax = fig.add_subplot(131) animate_linear(initial_conditions[0:25], 110, figtype = \u0026#34;static\u0026#34;, noaxis = False) ax.set_title(\u0026#34;Rule 110 with 25 random initial values\u0026#34;, fontsize=15); initial_conditions = np.random.choice([0, 1], size=(100,), p=[0.1, 0.9]) ax = fig.add_subplot(132) animate_linear(initial_conditions[0:50], 110, figtype = \u0026#34;static\u0026#34;, noaxis = False) ax.set_title(\u0026#34;Rule 110 with 50 random initial values\u0026#34;, fontsize=15); initial_conditions = np.random.choice([0, 1], size=(100,), p=[0.1, 0.9]) ax = fig.add_subplot(133) animate_linear(initial_conditions[0:75], 110, figtype = \u0026#34;static\u0026#34;, noaxis = False) ax.set_title(\u0026#34;Rule 110 with 75 random initial values\u0026#34;, fontsize=15); The squared design I have chosen for my grids means that, the smaller the number of cells, the less resolution there is; but, to be honest, there is no need for more: we can already see that, no matter the length of the initial conditions array, the pattern starts appearing almost inmediately, without delay, and regardless of the size of the initial array. This reminds me of fractals, although I am clearly not an expert on the matter.\nThe last question tells us to cathegorize our three cellular automatas (the ones that stem from our three rules) using Wolfram\u0026rsquo;s classification (¡created by the guy behind wolfram alpha!! UwU), which creates 4 classes, starting from a random initial state:\nEvolution leads to a homogeneous state. Evolution leads to a set of separated simple stable or periodic structures. Evolution leads to a chaotic pattern. Evolution leads to complex localized structures, sometimes long-lived. I would say the first structures, those generated by rule 45, belong to the third wolfram class; the pattern seems to be completely chaotic, and, although some laberyth-like structures do seem to appear, I dont believe one can count them as non-random.\nTalking about rule 110, I would say the patterns it creates belong to the fourth class, since it seems to lead to complex structures that propagate locally in the lattice. In fact, as we have already discussed, rule 110 has found to be one of the smallest turing-complete machines ever discovered, and it is of great interest to Cellular Automata\nWith regards to rule 48, I would say it belongs to the second class, as we can see those \u0026ldquo;fine lines\u0026rdquo; of development surrounded by a sea of nothing, which, by the way, remind me of the growing of very simple kelp on the sea.\nOne interesting take-home message might be that the really important part here is, in fact, the rule, and not the initial conditions: no matter how much we have tried to change those, the pattern ends up conforming to a given shape.\nSecond task: The game of life The second task for this assignment is to generate our own cellular automata from scratch using python, and see how it evolves depending on the original conditions. For this assignment, I will use a modified version of the code found in the pythonic preambulations blog, which I updated to make it compatible with python3 and to account for some libraries\u0026rsquo; deprecations; if you prefer a pre-packaged solution, you could use cellypylib instead ( pip install cellpylib ). I know this is not as fancy as could be, and I know it does not expand on Conway\u0026rsquo;s inital GoL concept, but, to be quite honest, this was already too difficult for me to accomplish and I belive it to be the most I am capable of.\nFirst, we have to define the function that automates each step in the cellular automaton\u0026rsquo;s evolution. Here, we are using scipy\u0026rsquo;s convolute2d method, which applies a filter (also known as a kernel) to a given matrix. In this case, we are using a matrix of 3x3 all-ones, which means that all neighbor\u0026rsquo;s states are taken into account (in a square grid) and params wrap and same, which ensure that the output is the same size as the original matrix and that the boundary is treated as the one surrounding each cell. We then return 1 (alive) for any cell surrounded by exactly 3 living cells and for any living cell surrounded by exactly 2 living cells. For the rest, 0 (dead) is returned, following the principles initially described in Conway\u0026rsquo;s original paper.\nThis, in my opinion, is a quite simple and elegant solution to implementing the GoL model in python; if you wish for a more manual solution, the original code provides an alternative \u0026ldquo;life_step_1\u0026rdquo; function, which uses nothing but numpy.\ndef life_step(X): nbrs_count = convolve2d(X, np.ones((3, 3)), mode=\u0026#39;same\u0026#39;, boundary=\u0026#39;wrap\u0026#39;) - X return (nbrs_count == 3) | (X \u0026amp; (nbrs_count == 2)) Now, we can define the axis where we will animate the plotting function: it has an init, that sets the background, and an animate object, that sets the steps of the animations. I have found 200 frames and an speed of 30 to be enough for an interesting movie, and I have chosen anim.to_html5_video() because it didnt produce an empty plot, as HTML(anim.to_jshtml()) does (thanks BindiChen). Thus:\ndef animate_GoL(initial_conditions, frames = 200, speed = 30, dpi = 10, save = False): X = np.asarray(initial_conditions).astype(bool) X_blank = np.zeros_like(X) fig = plt.figure(figsize=(30, 40), dpi=dpi) ax = fig.add_axes([0, 0, 1, 1], xticks=[], yticks=[], frameon=False) im = ax.imshow(X, cmap=plt.cm.binary, interpolation=\u0026#39;nearest\u0026#39;) im.set_clim(-0.05, 1) def init(): #This plots the background of each frame im.set_data(X_blank) return (im,) def animate(i): #This generates each frame im.set_data(animate.X) animate.X = life_step(animate.X) return (im,) animate.X = X anim = animation.FuncAnimation(fig, animate, init_func=init,frames=frames, interval=speed) if save == True : anim.save(\u0026#39;game_of_life.gif\u0026#39;, writer=\u0026#39;imagemagick\u0026#39;) video = anim.to_html5_video(); html = display.HTML(video) display.display(html); plt.close() Lets give it a test run!\nNote: To get the full experience, please use the provided Jupyter notebook; the exported, LaTeX to PDF version of this Notebook is for visualizing only, and, because of its static nature, it cannot match the quality of the animated data representations themselves.\ninit = np.random.choice([0, 1], size=(30,40)) #Completely random intial conditions animate_GoL(init) It works! However, since the seed is random, I cannot provide with a description of what is happening, given that that would depend on what your computer decided to generate. What I can do is show you some interesting spatial patterns that evolve in curious ways over time:\ninit = np.zeros((45, 60), dtype=int) #Initialize a random grid #Penta decathlon init[5,6] = 1; init[6,6] = 1 init[7,5] = 1; init[7,6] = 1; init[7,7] = 1 init[10,5] = 1; init[10,6] = 1; init[10,7] = 1 init[11,6] = 1; init[12,6] = 1; init[13,6] = 1; init[14,6] = 1; init[15,5] = 1; init[15,6] = 1; init[15,7] = 1 init[18,5] = 1; init[18,6] = 1; init[18,7] = 1 init[19,6] = 1; init[20,6] = 1 #Beehive init[10,20] = 1; init[10,21] = 1; init[11,19] = 1; init[11,22] = 1; init[12,20] = 1; init[12,21] = 1; #Boat init[20,26] = 1; init[20,27] = 1; init[21,26] = 1; init[21,28] = 1; init[22,27] = 1; #Beacon init[10,27] = 1; init[10,28] = 1; init[11,27] = 1; init[13,29] = 1; init[13,30] = 1; init[12,30] = 1; #Beehive init[5,25] = 1; init[5,26] = 1; init[6,24] = 1; init[6,27] = 1; init[7,25] = 1; init[7,26] = 1; #Pulsar init[5,42] = 1; init[5,43] = 1; init[5,44] = 1; init[5,48] = 1; init[5,49] = 1; init[5,50] = 1; init[7,40] = 1; init[7,45] = 1; init[7,47] = 1; init[7,52] = 1; init[8,40] = 1; init[8,45] = 1; init[8,47] = 1; init[8,52] = 1; init[9,40] = 1; init[9,45] = 1; init[9,47] = 1; init[9,52] = 1; init[10,42] = 1; init[10,43] = 1; init[10,44] = 1; init[10,48] = 1; init[10,49] = 1; init[10,50] = 1; init[12,42] = 1; init[12,43] = 1; init[12,44] = 1; init[12,48] = 1; init[12,49] = 1; init[12,50] = 1; init[13,40] = 1; init[13,45] = 1; init[13,47] = 1; init[13,52] = 1; init[14,40] = 1; init[14,45] = 1; init[14,47] = 1; init[14,52] = 1; init[15,40] = 1; init[15,45] = 1; init[15,47] = 1; init[15,52] = 1; init[17,42] = 1; init[17,43] = 1; init[17,44] = 1; init[17,48] = 1; init[17,49] = 1; init[17,50] = 1; #Heaveyweight SpaceShip (loooks like a bird! UwU) init[29,5:7] = 1; init[30,2:5] = 1; init[30,6] = 1; init[30,7] = 1; init[31,2:7] = 1; init[32,3:6] = 1; #Temporary Patterns init[31,35:51] = 1; init[31,20:26] = 1; animate_GoL(init, speed=120) Here, we can see some periodic patterns, such as the penta-decathlon (period 15), the pulsar (period 3) or the beacon (period 2); as well as some static patterns, such as the beehive and the boat. There is also one spaceship, a periodic pattern that moves over space; and some temporary patterns, one of which evolves toward a period-two ocilator which ends up killing our bird-like spaceship over time, slowly spanding and breaking havoc through the entire board. That is the beauty of the game of life: both really complex and really simple patterns can emmerge from a series of initial patterns, simulating the complexity of real, live systems with predictable and mathematical rules.\nAnd, finally, as a quirck, ¡we can re-geneate the image at the start of this paper by hand! Just define some initial conditions:\ninit = np.zeros((45, 70), dtype=int) init[0,24] = 1; init[1,22] = 1; init[1,24] = 1 init[2,12] = 1; init[2,13] = 1; init[2,20] = 1; init[2,21] = 1; init[2,34] = 1; init[2,35] = 1 init[3,11] = 1; init[3,15] = 1; init[3,20] = 1; init[3,21] = 1; init[3,34] = 1; init[3,35] = 1 init[4,0] = 1; init[4,1] = 1; init[4,10] = 1; init[4,16] = 1; init[4,20] = 1; init[4,21] = 1 init[5,0] = 1; init[5,1] = 1; init[5,10] = 1; init[5,14] = 1; init[5,16] = 1;init[5,17] = 1;init[5,22] = 1;init[5,24] = 1 init[6,10] = 1; init[6,16] = 1; init[6,24] = 1 init[7,11] = 1; init[7,15] = 1 init[8,12] = 1; init[8,13] = 1; And plot it:\nanimate_GoL(init) Here, we can see a gun, is a pattern with a main part that repeats periodically, like an oscillator, and that also periodically emits spaceships. I believe this represents an interesting illustration as to how reproducible this game is. More patterns can be found here.\nIf I, personally, were to design my own \u0026ldquo;Game of Life\u0026rdquo;-inspired cellular automaton, I would probably try to do something to implement the SEIRCM model explained in Lesson 2\u0026rsquo;s exercises: it would thus have 6 different states:\nS: susceptible to contracting the disease E: exposed to infected individuals, incubate the disease but are not yet contagious. I: infected, spread the disease R: have overcome the disease, do not spread the virus and have developed immunity to it. C: infected individuals requiring intensive care. M: deceased by COVID-19 With the transitions between this states being determined by the following equations:\n\\begin{align*} \\frac{dS}{dt} \u0026amp;= -\\beta:I:\\frac{S}{N}\\ \\frac{dE}{dt} \u0026amp;= \\beta:I:\\frac{S}{N} - \\delta E\\ \\frac{dI}{dt} \u0026amp;= \\delta:E - (1-a):\\gamma:I - \\eta🅰️I\\ \\frac{dH}{dt} \u0026amp;= \\eta:(1-a):I-\\tau:(1-u):H - u:\\sigma:H\\ \\frac{dC}{dt} \u0026amp;= u:\\sigma:H - m:\\rho:min(UCI, C) - max(0, C-UCI) - \\omega:(1-m):min(UCI,C)\\ \\frac{dR}{dt} \u0026amp;= \\gamma:(1-a):I + \\tau:(1-u):H + \\omega:(1-m):min(UCI,C)\\ \\frac{dM}{dt} \u0026amp;= m:\\rho:min(UCI, C) + max(0, C-UCI)\\ \\end{align*}\nIf I were to choose, I would select for an hexagonal grid, instead of an squared one, given that it has both translational and rotational symetry, and, due to its higer number of neighbours, that it more accurately represents the high-interconnectedness that exists in today\u0026rsquo;s world, specially in Spain, which was the subject of the analysis in Exercise 2. Also, hexagons are the bestagons! :D\nThe model would, of course, present no randomness, since virus transmision has clear causality, and would be only in 2D, since adding a third dimmension seems like too much of a distraction with little return on investment: we humans do live in a 3D world, but most of our interactions take place at ground level, except maybe when we are in la luna de valencia.\nUnfortunately, I am not knowledgeable enough to implement this approach.\nThird task: Essay Note: This essay can also be found attached as a separate PDF The existence or not of free will is one of the problems that have most tormented mankind over the centuries, eliciting reactions from fields as varied as psychology, biology and physics.\nThe field of neurology does not seem to offer conclusive answers to this question. The Lisbet experiment, for example, demonstrated that the actions we take are previously and unconsciously decided by the brain, and that it is then that our consciousness, our \u0026ldquo;inner voice\u0026rdquo;, feels the desire to perform these actions, thus ruling out the possibility of a thinking entity outside the constraints of the physical world. We must, therefore, go deeper into the matter.\nIn the field of mathematics, the advent of Conway\u0026rsquo;s Game of Life (GoL) has meant, according to Wolfram 2002, \u0026ldquo;the single most surprising scientific discovery I have ever made\u0026rdquo;. The possibility of simulating terribly complex interactions from a really small set of rules makes us wonder about the very nature of the universe: is it possible that, just as life arose from simple molecules, we could obtain constantly evolving intelligences and thinking beings if we had a large enough GoL? Is it possible, then, that we are nothing more than mere toys of the universe, particles subject to the immutable rules of physics whose path is predefined?\nAt the beginning of the research conducted for this article, I had the impression, which I think is quite common, that the rules of quantum physics (our universe\u0026rsquo;s equivalent of Conway\u0026rsquo;s rules in the GoL) left, due to their probabilistic nature, a space of hope for free will; however, it is true that this theory is as deterministic as Newtonian physics, and, given that decisions, however difficult to predict, cannot be free if they are enclosed by the laws of probability, this does not seem an interesting path to follow for the defenders of free will.\nHowever, there is still some work to be done in this area, since we do not yet know whether our brain is a decoherent system (i.e. one in which the probabilities of the quantum wave function tend to collapse on a deterministic path) or a chaotic (i.e. unpredictable) one, which would allow us to glimpse, among the randomness of chaos, gaps for free will.\nEven more: recent developments, such as Dennett 2003, have shown that, even if a system is deterministic, its consequences need not be inevitable: although in the microscopic world particles follow predefined trajectories, in the abstract world in which we humans move, based on concepts, it is possible to avoid the consequences of laws based on such concepts. One can establish, for example, a law such that \u0026ldquo;in the GoL, Eaters eat Gliders\u0026rdquo;; or \u0026ldquo;on planet Earth, lynxes eat rabbits\u0026rdquo;; however, just as a lynx can stumble and the rabbit escape, a glider can, in certain cases, escape from an Eater, and so can, on occasion, a human being make unexpected decisions of his own, at least in the abstract layer that is the one that really matters to us.\nIt is because of this (besides, let\u0026rsquo;s not fool ourselves, because of my pre-existing personal convictions) that I prefer to opt for the side that defends the existence of free will, even if not consciously, but internally, because it leaves a free space for our ability to act to improve the world as much as possible. In any case, the debate is still open, as there is no conclusive evidence either way; and while it is impossible to know, at least for the moment, how much space the universe leaves for human action, what we can say with certainty is that our perpetual interest in the subject, which spans multiple disciplines over the centuries, says a lot about our desires as a species.\nReferences Baldwin, John. \u0026ldquo;Stephen Wolfram . A New Kind of Science, Wolfram Media, Inc, Champaign, IL, 2002, Xiv + 1197 Pp.\u0026rdquo; Bulletin of Symbolic Logic, vol. 10, no. 1, March 2004, pp. 112-14. Cambridge University Press, https://doi.org/10.1017/S1079898600004200.\nBerto, Francesco, and Jacopo Tagliabue. \u0026ldquo;Cellular Automata.\u0026rdquo; The Stanford Encyclopedia of Philosophy, edited by Edward N. Zalta, Spring 2021, Metaphysics Research Lab, Stanford University, 2021. Stanford Encyclopedia of Philosophy, https://plato.stanford.edu/archives/spr2021/entries/cellular-automata/.\nDennett, Daniel Clement. Freedom Evolves. New York : Viking, 2003. Internet Archive, http://archive.org/details/freedomevolves00denn.\nEL EXPERIMENTO DE LIBET | El experimento que dice que no existe el libre albedrío. www.youtube.com, https://www.youtube.com/watch?v=wESoyC5Y9lU. Accessed December 9, 2021.\n¿Existe el Destino? www.youtube.com, https://www.youtube.com/watch?v=NvuCZJyoing. Accessed December 9, 2021.\n¿Somos realmente LIBRES de decidir nuestro futuro? www.youtube.com, https://www.youtube.com/watch?v=hXZiJ6TlFh8. Accessed December 9, 2021.\nNOTE: I have decided to make the essay a bit longer due to the text outlay of LaTeX, which shows much less text on screen than, for example, LibreOffice.\nThis document can be easily converted to PDF using:\n`pandoc --pdf-engine=xelatex \u0026quot;Essay.md\u0026quot; -o \u0026quot;Essay on Free Will.pdf\u0026quot;.\n","date":"December 8, 2021","hero":"/es/posts/master-en-biolog%C3%ADa-computacional/modelizaci%C3%B3n-de-sistemas-biol%C3%B3gicos/celular-automata/hero.png","permalink":"https://www.pablomarcos.me/es/posts/master-en-biolog%C3%ADa-computacional/modelizaci%C3%B3n-de-sistemas-biol%C3%B3gicos/celular-automata/","summary":"A cellular automaton is a discrete model of computation consisting on a regular grid of cells, each in one of a finite number of states, such as on and off. For each cell, a set of cells called its neighborhood is defined relative to the specified cell. An initial state is selected by assigning a state for each cell. A new generation is created according to some fixed rule that determines the new state of each cell in terms of the current state of the cell and the states of the cells in its neighborhood.","tags":null,"title":"The Cellular Automata"},{"categories":null,"contents":"The idea for this assignment is find the orthologue pairs between species Arabidopsis and S. pombe using BioRuby to do BLAST and parse the BLAST-generated reports. We will use use \u0026ldquo;reciprocal best BLAST\u0026rdquo;, scanning for Protein A of Species A in the whole proteome of Species B, and then BLAST the top hit against all proteins in Species A. If the top hits match, the protein is a good Orthologue candidate.\nTo decide on \u0026ldquo;sensible\u0026rdquo; BLAST parameters, we have done a bit of online reading, and cited the papers or websites that provided the information. Also, we have writen a few sentences describing how we would continue to analyze the putative orthologues we just discovered, to prove that they really are orthologues. We WILL NOT write the code - just describe in words what that code would do.\nDocumentation can be accessed using YarDoc:\nyardoc README.md blast_orthologues.rb ./Clases/Embl_to_GFF3.rb ./Clases/ImportExportModule.rb ./Clases/BlastCmdEnhanced.rb yard server I have knowingly gitignored the html files from the repo to make it more legible, since they can be easily generated.\nTo run the script, use: ruby blast_orthologues.rb ./SampleData/target_genome.fa ./SampleData/protein.fa report_file.txt\nDesign choices BLAST parameters For the e-value (measures similarity beyond randomness), I choose 0.000001 as my cutoff, following the advice given by Ward and Moreno-Hagelsieb and standard practice. This paper, Ward and Moreno-Hagelsieb also suggest that we add a Smith-Waterman final alignment (-F “m S” -s T) to our BLASTP options; however, this would mean changing the code, rendering BlastCmdEnhanced..makeblast_besthits() useless and greatly increasing computing time, which is already quire high (around 7 hrs). So, we will just keep a soft filter (-F “m S”), which masks low information segments only during the search phase, reducing the values of masking to increase the accuracy scores of the alignment. In normal, command-line, bash BLAST, there are more detailed formatting options: \u0026quot;blastp -query {0} -subject {1} -evalue {2} -outfmt '6 qseqid qseq sseqid sseq qcovs pident evalue' \u0026gt; {3}\u0026quot;, which can be used to filter for coverage and identity, as well as for e-value; unfortunately, BioRuby does seems to be really bad at managing this, so I have been forced to design the following backstop solution: For coverage, which is the percent of the query length that is included in the aligned segments, bioruby only returns absolute values (in total nucleotides), and calls it \u0026ldquo;overlap\u0026rdquo; (in violation of existing naming conventions and for no reason at all), so I decided to divide it by the total length of the sequence and multiplying by 100 to get the percentage value, which PAPER A suggest should be of more than 50%. This makes sense: we dont want a sequence with a really good e-value (i.e. really good alignment) but with, for instance only 10 nucleotides known out of a total length of 1000. For identity, bioruby is supposed to provide a percent_identity() method; however, whenever I have called it in the program it refused to work, always returning 0 when identity() did return non-zero values. Thus, I had to get the percentage value myself, once again by dividing by .query_len and multiplying by 100. According to Ostlund et al, this is usually higher than 60% for eukaryotes, but I decided to go with a value of 30% as it seems to be more commonplace; after all, if the genes are orthologues, we would expect at least some of the bases to be identical, although others might have changed due to neutral evolution and mutations. This large amount of conditions greatly reduces the amount of positives, from 2400 with e-value only to\u0026hellip; actually 0!! After doing some tests, I have found that this is due mainly to identity, so I have deactivated that parameter (setting the treshold to 0). This is not that big of a deal: identity should be smaller the closer two samples are in time, but orthologues can be far, far away! With the new treshold, I get way less matches (1800), but, given the high coverage, we can be pretty sure they are good putative orthologues. Some problems might arise if two or more FASTA sequences with non-unique headers are provided as either query or target. Fortunately, none of the files provided here exhibited this problem (and they shouldn\u0026rsquo;t! its malpractice on the user\u0026rsquo;s end to provide with non-standard files, let alone non-unique FASTA headers!) File input I have decided against allowing tar.gz or any compressed files as an input; I believe file decompression is better left to the OS and the user, which I have thus forced to use fasta files.\nGuessing BLAST type Some internet solutions suggested creating a function for automatically guessing the BLAST types. It would go along the following lines:\n``` def self.guess_blast_type(query, target) if self.guess_fastatype(target) =='dna' and self.guess_fastatype(query) =='dna' return 'blastn' elsif self.guess_fastatype(target) =='prot' and self.guess_fastatype(query) =='prot' return 'blastp' elsif self.guess_fastatype(target) =='prot' and self.guess_fastatype(query) =='dna' return 'blastx' elsif self.guess_fastatype(target) =='dna' and self.guess_fastatype(query) =='prot' return 'tblastn' elsif self.guess_fastatype(target) =='dna' and self.guess_fastatype(query) =='dna' return 'tblastx' else abort(\u0026quot;[ERROR]: Something went wrong when trying to detect BLAST type\u0026quot;) end end ``` However, as you can see, there is a conflict between tblastx and blastn: they both use DNA fastas as source type, and choosing between them depends essentially on the desires of the user running the program. Thus, I have decided against including such function in my code: I could ignore tblastx and create a function only useful for this assignment, but, given that the idea, as I see it, is to create as general and as useful and reusable classes as possible, I believe it is better to ask the user to do this manually.\nWhat to do next? (Bonus 1%) To confirm whether the list of putative orthologues found in report.txt is, in fact, made up of real orthologues or not, Trachana et al. suggest that we use the genome of a third species, ideally related both to A. Thaliana and S. pombe (our target and query organisms) to perform a Cluster of Orthologous Genes, a clustering method based on the results of our Best Reciprocal BLASTs. By selecting only as valid those clusters of orthologues with hits for the three species, we would be a little more sure that we are dealing with genuinely orthologous genes\nAnother option would be to use the mentioned third specied to generate a phylogenetic tree usign alignment algorithms such as ClustalW, and infering orthology from speciation events; this is not only less computationally efficient than the clustering algorithms presented in the last paragraph, but, as Bapteste et al. suggest, it is not even a failproof way of showing gene-to-gene relation.\nBibliography Bapteste, E., et al. «Do orthologous gene phylogenies really support tree-thinking?» BMC Evolutionary Biology, vol. 5, mayo de 2005, p. 33. PubMed Central, https://doi.org/10.1186/1471-2148-5-33.\nMoreno-Hagelsieb, G., y K. Latimer. «Choosing BLAST Options for Better Detection of Orthologs as Reciprocal Best Hits». Bioinformatics, vol. 24, n.º 3, febrero de 2008, pp. 319-24. DOI.org (Crossref), https://doi.org/10.1093/bioinformatics/btm585.\nOn the definition of sequence identity. (s. f.). Recuperado 20 de diciembre de 2021, de https://lh3.github.io/2018/11/25/on-the-definition-of-sequence-identity\nOstlund, G., Schmitt, T., Forslund, K., Kostler, T., Messina, D. N., Roopra, S., Frings, O., \u0026amp; Sonnhammer, E. L. L. (2010). InParanoid 7: New algorithms and tools for eukaryotic orthology analysis. Nucleic Acids Research, 38(Database), D196-D203. https://doi.org/10.1093/nar/gkp931\nSmith, E. (s. f.). Library guides: Ncbi bioinformatics resources: an introduction: blast: compare \u0026amp; identify sequences. Recuperado 20 de diciembre de 2021, de https://guides.lib.berkeley.edu/ncbi/blast\nTrachana, Kalliopi, et al. «Orthology prediction methods: A quality assessment using curated protein families». Bioessays, vol. 33, n.º 10, octubre de 2011, pp. 769-80. PubMed Central, https://doi.org/10.1002/bies.201100062.\nWard, Natalie, y Gabriel Moreno-Hagelsieb. «Quickly Finding Orthologs as Reciprocal Best Hits with BLAT, LAST, and UBLAST: How Much Do We Miss?» PLOS ONE, vol. 9, n.º 7, julio de 2014, p. e101850. PLoS Journals, https://doi.org/10.1371/journal.pone.0101850.\nYuan, Y. P., et al. «Towards Detection of Orthologues in Sequence Databases». Bioinformatics (Oxford, England), vol. 14, n.º 3, 1998, pp. 285-89. PubMed, https://doi.org/10.1093/bioinformatics/14.3.285.\nZhou, Y., y L. F. Landweber. «BLASTO: A Tool for Searching Orthologous Groups». Nucleic Acids Research, vol. 35, n.º Web Server, mayo de 2007, pp. W678-82. DOI.org (Crossref), https://doi.org/10.1093/nar/gkm278.\nCode and Acknowledgements Source code for the assignment can be found here\nThe header image for this post is CC-By-Sa 3.0 by Alberto Salguero on Wikimedia Commons\nThis document can easily be converted to PDF format using pandoc:\npandoc --pdf-engine=xelatex README.md -o README.pdf\n","date":"December 7, 2021","hero":"/es/posts/master-en-biolog%C3%ADa-computacional/bioinfo-programming-challenges/assignment-4/hero.jpg","permalink":"https://www.pablomarcos.me/es/posts/master-en-biolog%C3%ADa-computacional/bioinfo-programming-challenges/assignment-4/","summary":"The idea for this assignment is find the orthologue pairs between species Arabidopsis and S. pombe using BioRuby to do BLAST and parse the BLAST-generated reports. We will use use \u0026ldquo;reciprocal best BLAST\u0026rdquo;, scanning for Protein A of Species A in the whole proteome of Species B, and then BLAST the top hit against all proteins in Species A. If the top hits match, the protein is a good Orthologue candidate.","tags":null,"title":"Finding Orthologues in Arabidopsis Thaliana"},{"categories":null,"contents":"by Yaiza ARNAIZ ALCACER, Pablo MARCOS LOPEZ, Lexane LOUIS and Alexandre VERGNAUD.\nFigure 1 : Simple statistics Context The plot we have chosen comes from the field of Medical Sciences. In a paper by 1 from 2019, the authors analyze the evolution of gene expression changes of intestine tissue in celiac patients, with the aim of deciphering the complex molecular bases behind this complex disease, which affects 0.71 % 2 of the inhabitants of the United States and the European Union.\nPresentation of the figure Celiac disease is an autoimmune disease, in which susceptible patients attack their own tissues after eating gluten, damaging their small intestine and making it unable to take in nutrients. To analyze its underlying mechanisms, the authors picked gene expression profiles for experiment GSE112102 3 under analysis platform GPL10558 from the \u0026quot;Gene Expression Omnibus\u0026quot; database (this is also terribly explained in the paper). The GSE112102 experiment found that 4 some genes have enhanced differential expression in patients of Celiac Disease, compared to those who dont suffer from it. They then proceeded to show this differential expression of those genes (also without telling us about which one(s)) in the box plot we are presenting in Figure 1\nAnalysis of the figure Here, we can see a plot of 24 box plots, each representing one of the patients of the experiment GSE112102, and the differential expression of some protein(s) or gene(s) which name(s) we are not being told.\nMistakes and successes of the figure We have divided the analysis of the figure in two parts. First, regarding the graph itself, we have found the following errors, as presented in Figure 2:\nThe title does not serve as a good description of what is depicted inside : we just get to see the name of the experiment and \u0026quot;selected samples\u0026quot;, but with no clues about what was the selection criteria. Looking at the original paper, it seems that they selected all of the patients.\nThe legend is not readable, since it overlaps with the box plots. This appears like a major error, since most plot generation libraries, such as python\u0026rsquo;s matplotlib, already place the legend in the most appropriate place automatically.\nThe y-axis is not labeled at all : what do those numbers mean ?\nThe x-axis, meanwhile, is overcluttered, with a label for each of the patients. We believe this is too much information, and that the authors would have been more professional if they had simply merged all patients in each of the groups into a single box plot per case (one for celiac patients and one for non-celiac patients).\nRegarding the previous point, a significancy test or score is lacking. How can we know if these results are conclusive or not ?\nOn the other hand, we also found the following things to be pretty well managed :\nThe y-axis allows for easy data visualization, since it is neither too big nor too small : it fits the data just right. Only some ticks are shown, which reduces visual clutter.\nAll the parts of the box plots are visible, and none is cut.\nThe white background and lack of grid lines make the graph simple and refreshing.\nMistakes and successes of the caption With regards to the figure\u0026rsquo;s caption, we have found the following (summarized in Figure 3):\nThere is no description of what the CD acronym means. We know thanks to the context that this is \u0026quot;celiac disease\u0026quot;, but this should have been better explained.\nThere is no summary commenting the figure, just a title line (although at least that is present)\nImprovements In order to finish our study of the figure, we should make suggestions about improvements. We would say that a better plotting type to compare numerous items (the 24 samples in the figure) would be a bar chart.\nAnother option could be to use the data of all the control samples and all the patients to create two box plots, representing the two groups; showing the comparisons between the two groups would have been easier with this approach. Furthermore, a statistic test to show if the results are significant or not would be a great improvement to bring strength in the study.\nIn general, we cannot see how this article improves the original article it was derived from ; it seems like a chaotic attempt at data description, which failed miserably.\nFigure 2 : Composite figure Context The composite plot we have chosen comes from a scientific article called Singing in a silent spring : Birds respond to a half-century soundscape reversion during the COVID-19 shutdown published on September 24, 2020. This article 5 attempts to provide an explanation to an unprecedented phenomenon that occurred between April and June 2020 when the COVID-19 health crisis forced nearly half of humanity to remain confined. The slowdown in human activities brought noise pollution to a level never reached in half a century, offering a silent spring to the population. During this period, many city-dwellers had the sensation of hearing the birds singing again.\nThe authors decided to observe a common songbird in San Francisco Bay, the White-crowned Sparrow, to see if there is a correlation between this new sound environment and the communication between these birds.\nPresentation of the figure Throughout the study, which took place during the shutdowns, the authors made numerous observations and recordings to compare them with old records or data and then, be able to understand the behaviour of the White-crowned Sparrows facing this silent spring.\nThe composite figure we chose shows different plots of background noise level, an essential point of the study. The unit of the figures is LAF90 : a weighted, sound level just exceeded for 90 % of the measurement period and calculated by statistical analysis. Generally, these figures show that the background noise level was much higher before the shutdown, especially in urban areas. Indeed, the noise of rural areas, characterized by natural background noise such as the wind or the ocean, has been less affected. It also shows that the number of vehicles crossing the Golden Gate Bridge between April and May 2020 has returned to levels never reached since 1954. And finally, it shows the evolution of the trill (number of notes per second) minimum frequency of the Berkeley song dialect (used by birds in urban areas) over the years.\nAnalysis of the figure Unlike in the first study, we chose to divide this study into two parts which are the mistakes and the successes of the entire representation (= figure + caption). We have found more successes and less failures in this figure than in the past one.\nMistakes of the representation The caption is very long and dense. In fact it is not easy to read. Maybe too precise for a visually effective representation.\nAn explanation of the unit LAF90 is missing. Considering this, for non-expert people, it is difficult to understand the meaning of the representation.\nSuccesses of the representation We have a good homogeneity of the colors and not a big amount of them. Actually we have only two colors : the red and the blue in order to represent respectively the urban and the rural areas.\nThe representations are clear and clean. They are not overloaded but the important things are there.\nThe units are present everywhere and the axes\u0026rsquo;s titles are clear.\nThe caption offers clear explanations of the study in order to not overload the figures.\nWe have an entire view of the study in one composite figure effectively represented with multiple computations and statistics linked to each other.\nThe format of four graphs is of a good size. Certainly there are not too many figures compared to the big amount of data being presented.\nImprovements In order to give ideas about how to manage the failures of the composite figure, we can propose some improvements. Perhaps it would be better to add some legends on the figures than to put all in the caption (as red=urban and blue=rural) for A and B. The major point is too unload the caption.\nBibliography This document is availaible under the CC By SA 4.0 License\nS. Rezaei-Tavirani, M. Rostami-Nejad, and F. Montazar. Highlighted role of VEGFA in follow up of celiac disease. Gastroenterology and Hepatology From Bed to Bench, 12(3) :254–259, 2019.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nA. Rubio-Tapia, J. F. Ludvigsson, T. L. Brantner, J. A. Murray, and J. E. Everhart. The prevalence of celiac disease in the United States. The American Journal of Gastroenterology, 107(10) :1538–1544 ; quiz 1537, 1545, Oct. 2012.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nGEO Accession viewer.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nP. Acharya, R. Kutum, R. Pandey, A. Mishra, R. Saha, A. Munjal, V. Ahuja, M. Mukerji, and G. K. Makharia. First Degree Relatives of Patients with Celiac Disease Harbour an Intestinal Transcriptomic Signature that Might Protect them from Enterocyte Damage. Clinical and Translational Gastroenterology, 9(10) :195, Oct. 2018.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nE. P. Derryberry, J. N. Phillips, G. E. Derryberry, M. J. Blum, and D. Luther. Singing in a silent spring : Birds respond to a half-century soundscape reversion during the COVID-19 shutdown. Science, 2020.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","date":"December 3, 2021","hero":"/es/posts/master-en-biolog%C3%ADa-computacional/estad%C3%ADstica/assignment-1/hero.jpg","permalink":"https://www.pablomarcos.me/es/posts/master-en-biolog%C3%ADa-computacional/estad%C3%ADstica/assignment-1/","summary":"by Yaiza ARNAIZ ALCACER, Pablo MARCOS LOPEZ, Lexane LOUIS and Alexandre VERGNAUD.\nFigure 1 : Simple statistics Context The plot we have chosen comes from the field of Medical Sciences. In a paper by 1 from 2019, the authors analyze the evolution of gene expression changes of intestine tissue in celiac patients, with the aim of deciphering the complex molecular bases behind this complex disease, which affects 0.71 % 2 of the inhabitants of the United States and the European Union.","tags":null,"title":"Practical Assignment 1"},{"categories":null,"contents":" Tasks: for 10% (easy) Using BioRuby, examine the sequences of the ~167 Arabidopsis genes from the last assignment by retrieving them from whatever database you wish Loop over every exon feature, and scan it for the CTTCTT sequence Take the coordinates of every CTTCTT sequence and create a new Sequence Feature. Add that new Feature to the EnsEMBL Sequence object. Once you have found and added them all, loop over each one of your CTTCTT features and create a GFF3-formatted file of these features. Output a report showing which genes on your list do NOT have exons with the CTTCTT repeat Tasks: for 10% (hard) Re-execute your GFF file creation so that the CTTCTT regions are now in the full chromosome coordinates used by EnsEMBL. Save this as a separate new file. Prove that your GFF file is correct by uploading it to ENSEMBL and adding it as a new “track” to the genome browser of Arabidopsis Along with your code, for this assignment please submit a screenshot of your GFF track for the AT2G46340 gene on the ENSEMBL website to proof that you were successful. Documentation can be accessed using YarDoc:\nyardoc README.md insertional_mutagenesis.rb ./Clases/Embl_to_GFF3.rb ./Clases/ImportExportModule.rb yard server I have knowingly gitignored the html files from the repo to make it more legible, since they can be easily generated.\nTo run the script, use: ruby insertional_mutagenesis.rb ./SampleData/ArabidopsisSubNetwork_GeneList.txt report_file.gff3\n","date":"November 30, 2021","hero":"/es/posts/master-en-biolog%C3%ADa-computacional/bioinfo-programming-challenges/assignment-3/hero.png","permalink":"https://www.pablomarcos.me/es/posts/master-en-biolog%C3%ADa-computacional/bioinfo-programming-challenges/assignment-3/","summary":"Tasks: for 10% (easy) Using BioRuby, examine the sequences of the ~167 Arabidopsis genes from the last assignment by retrieving them from whatever database you wish Loop over every exon feature, and scan it for the CTTCTT sequence Take the coordinates of every CTTCTT sequence and create a new Sequence Feature. Add that new Feature to the EnsEMBL Sequence object. Once you have found and added them all, loop over each one of your CTTCTT features and create a GFF3-formatted file of these features.","tags":null,"title":"GFF feature files and visualization"},{"categories":null,"contents":" The Kuramoto Model is a mathematical model used to describe the synchronization of a large set of coupled oscillators. This formulation, which was motivated by the behaviour of both physical and biological oscilators, has found widespread application in neuroscience.\nThe model makes several assumptions, including that there is weak coupling, that the oscillators are identical or nearly identical, and that interactions depend sinusoidally on the phase difference between each pair of objects.\nFor the exercises, we need to simulate some kuramoto oscilators coupled using an external forcing: for one of the systems, this external forcing will be the average phase of all the oscilators, while, for the other, we will be using a uniform distribution between 0 and $2 \\cdot \\pi$. Thus:\nimport numpy as np #Supports common numeric operations from numpy import random #To create sets of random numbers import matplotlib.pyplot as plt #For graphs from colour import Color #Make color gradients in a programmatic way import seaborn as sns #Statistical data visualization import pandas as pd #Data Visualization library from matplotlib import animation, rc #Make anymations from IPython.display import HTML #Convert to HTML (display animations) #A function to calculate θ(t) def get_new_theta(theta,omega,K,dt, method): new_theta = np.copy(theta); oscilators_diff = [] dtheta = np.zeros(len(theta)) psi = np.arctan2(sum(np.sin(theta)), sum(np.cos(theta)))%(2*np.pi) #Average phase forcing = random.uniform(0, 2*np.pi) #Uniform dist between 0 and 2*pi #Putting it here makes r calculated with starting values but not ending values r = (1/len(theta))*np.sqrt(sum(np.sin(theta))**2+sum(np.cos(theta))**2) for j in range(1,len(theta)): #for each pair of oscilators if method == \u0026#34;average\u0026#34;: dtheta += K*np.sin(psi - theta) #Adding to whole arrays is easier elif method == \u0026#34;random\u0026#34;: dtheta += K*np.sin(forcing - theta) elif method == \u0026#34;oscilators\u0026#34;: shift_theta = np.roll(theta,j) # periodic boundary dtheta += K*np.sin(shift_theta - theta) oscilators_diff = shift_theta - theta else: #Custom forcing dtheta += K*np.sin(method - theta) dtheta += omega #Adds intrinsic frequencies; they could be int or array new_theta += dt*dtheta return r, new_theta%(2*np.pi), oscilators_diff #modulo 2*pi #Run new_theta recursively nstep times def do_nsteps(theta,omega,K,dt,nsteps, method): r_array = []; oscilators_diff = []; new_theta = np.copy(theta) #Initializations for i in range(nsteps): r, new_theta, osc = get_new_theta(new_theta,omega,K,dt, method) r_array.append(r); oscilators_diff.append(osc) return r_array, new_theta, oscilators_diff def fill2d(theta,omega,K,dt,nsteps,method): zeta = np.zeros((len(theta),nsteps)) # our 2d array zeta[:,0] = theta # initial conditions new_theta = theta for i in range(1,nsteps): r, new_theta, oscilators_diff = get_new_theta(new_theta,omega,K,dt,method) zeta[:,i] = new_theta return zeta This code, which has been adapted from a course by Rochester University, implements the two methods we have been asked to implement and can work with a number of N oscilators. Now, we would like to study the syncronization transition through order parameter $r$ as $K$ varies in the two ensembles. To do this, we first calculate r for a range of 100 values of k between 0 and 10.\nDefine the plotting function:\ndef plot_r_values(mymethod, legend=True): r_dict= {} colors = list(Color(\u0026#34;red\u0026#34;).range_to(Color(\u0026#34;blue\u0026#34;),36)) theta = 2*np.pi*np.random.random(100) omega = 2*np.pi*np.random.random(100) for k in np.linspace(0,0.07,36): #Used cutre trial and error to settle on this values r_dict[k], theta, osc = do_nsteps(theta,omega,k,0.1,100, mymethod) for i, element in enumerate(r_dict): plt.plot(np.linspace(0,100,100),r_dict[element], label=round(element,2), color=f\u0026#34;{colors[i]}\u0026#34;) plt.xlabel(\u0026#39;Time\u0026#39;, fontsize=15) plt.ylabel(\u0026#39;Order parameter\u0026#39;, fontsize=15) if isinstance(mymethod, float): plt.title(f\u0026#39;External coupling with $\\Omega$ = {round(mymethod, 3)}\u0026#39;, fontsize=18) else: plt.title(f\u0026#39;Evolution of the order parameter for {mymethod} coupling\u0026#39;, fontsize=18) if legend == True: plt.legend(ncol=2) Plot for average forcing and for external forcing of random.uniform(0, 2*np.pi)\nplt.subplots(figsize=(20, 5)) plt.subplot(1, 2, 1) plot_r_values(\u0026#34;average\u0026#34;) plt.subplot(1, 2, 2) plot_r_values(\u0026#34;random\u0026#34;) As we can see, the system coupled by average internal phase experiences pretty fast syncronization, almost too fast to bee seen on the graph (hum! suspicious\u0026hellip; something might be wrong?). For the systems that reach complete syncronization, that happens after only 2 to 3 oscilations, while the ones that reach partial sync take a bit more to reach their maximum r, with that r being clearly dependent on K (as one can see with the gradient) It should also be noted that the values of K tested (between 0 and 0.07) are quite, quite low, so we can say this system is highly sensible to coupling. Looking at the legends, the transition to partial syncronization (total syncronization is more difficult to study as previously explained) seems to occur after about 5 repetitions for k = 0.04; or after 15 rounds for k = 0.02\nThe system coupled using a random distribution, however, never reaches a point where it becames stable; and this makes sense! By applying a random external force in each oscilation, we are just \u0026ldquo;driving the system crazy\u0026rdquo;, making it impossible for it to reach a steady state. Thus, we will try to apply a more \u0026ldquo;coherent\u0026rdquo; external force, one which will act as a \u0026ldquo;master ocilator\u0026rdquo; that helps all oscilators syncronize; in this case, this forces will be $\\pi$ and $\\frac{\\pi}{2}$\nplt.subplots(figsize=(20, 5)) plt.subplot(1, 2, 1) plot_r_values(np.pi, legend=False) #Already know the legend, make it clearer plt.subplot(1, 2, 2) plot_r_values(np.pi/2, legend=False) As we can see, both graphs are pretty simmilar: They have some complete coupling, some partial coupling and some random noise (no coupling at all). When compared with the previous graph, we can see than, unline with external random force, coupling does appear; but, the coupling is always less intense (there is less lines in the upper part of the graph) than when using $\\Psi$ as the coupling method. Thus, it can be said that interal average phase is a better coupling method than an external forcing\nOnce again, the color gradient clearly shows how higher k values lead to higher r values (makes sense)\nNow, we would like to plot the distribution of phase differences between all pairs of oscillators for different values of K, before and after the transition. To do this, we obtain the values from get_new_theta function. We use k = 0.05, and 5 vs 20 rounds of change:\n(To avoid repeating work, I will do this for Average Phase Coupling only)\nFirst, we set the plotting function and the initial (must be common) values for plotting:\ntheta = 2*np.pi*np.random.random(100); omega = 2*np.pi*np.random.random(100) def plot_phase_diffs(k, numosc): r, new_theta, osc = do_nsteps(theta,omega,k,0.1,numosc,\u0026#34;oscilators\u0026#34;) #Get things to plot plt.subplots(figsize=(20, 5)) plt.subplot(1, 2, 1) plt.xlabel(\u0026#39;Oscilators\u0026#39;, fontsize=15) plt.ylabel(\u0026#39;Phase\u0026#39;, fontsize=15) plt.title(f\u0026#39;Phase of oscilators for k = {k} \u0026amp; {numosc} oscilations\u0026#39;, fontsize=18) plt.plot(new_theta,\u0026#39;bo\u0026#39;) plt.subplot(1, 2, 2) plt.xlabel(\u0026#39;Number of oscilatos\u0026#39;, fontsize=15) plt.ylabel(\u0026#39;Phase difference\u0026#39;, fontsize=15) plt.title(f\u0026#39;Phase difference for k = {k} \u0026amp; {numosc} oscilations\u0026#39;, fontsize=18) plt.hist(osc[-1], color=\u0026#34;#0000ff\u0026#34;); And then, we plot:\nplot_phase_diffs(0.05, 5) As we can see, after 5 oscilations, most oscilators stand completely separated and different. Lets see after 20:\nplot_phase_diffs(0.05, 20) Wow! As we can see, the oscilators have converged clearly: The histogram shows a distribution much more centered than the previous one: this is also visible at the left, where oscilators now seem much more \u0026ldquo;aligned!\u0026rdquo; (its bad for me to say it, but it is definetely an amazing data visualization hum)\nFinally, we would like to vary $\\omega$ and $K$ to stimate where the arnold tongues are situated. Lets have a quick recap: arnold tongues are, essentially, areas of entrainment, that is, the regions where the oscilators tend to syncronize with each other. We can approximate arnold tongues by using the order parameter $r$, given that, the higher the r, the more syncronized; another, more accurate way of doing it would be by plotting the same heatmap but with a comparison of the oscillator frequencies $\\theta$ and the external forcing $\\Omega$. To do this, we can define the following plotting function:\ntheta = 2*np.pi*np.random.random(100); omega = 2*np.pi*np.random.random(100) #Reset initial values def plot_heatmap(mymethod): my_omegas = np.linspace(0, 4*np.pi, 25); my_ks = np.linspace(0,0.07,36) omega_axis = np.around(my_omegas, 2); k_axis = np.around(my_ks, 2) r_matrix = pd.DataFrame(index=my_ks, columns=my_omegas) for omega in my_omegas: for k in np.linspace(0,0.07,36): r, new_theta, osc = do_nsteps(theta,omega,k,0.1,10,mymethod) r_matrix.at[k,omega] = round(r[-1], 4) #Append r after n oscillations (n-1 actually) sns.heatmap(r_matrix.to_numpy().tolist(), cmap = \u0026#34;viridis\u0026#34;, xticklabels = omega_axis, yticklabels = k_axis) if isinstance(mymethod, float): plt.title(f\u0026#39;Heatmap of $r$ with $\\Omega$ = {round(mymethod, 2)}\u0026#39;, fontsize=18) else: plt.title(f\u0026#39;Heatmap of $r$ with {mymethod} coupling\u0026#39;, fontsize=18) plt.xlabel(\u0026#39;External forcing $\\omega$\u0026#39;, fontsize=15) plt.ylabel(\u0026#39;Coupling coefficient $k$\u0026#39;, fontsize=15) And plot it:\nplt.subplots(figsize=(20, 20)) plt.subplot(2, 2, 1) plot_heatmap(\u0026#34;average\u0026#34;) plt.subplot(2, 2, 2) plot_heatmap(\u0026#34;random\u0026#34;) plt.subplot(2, 2, 3) plot_heatmap(np.pi) plt.subplot(2, 2, 4) plot_heatmap(np.pi/2) In the 4 graphs plotted above, we can see the following:\nThe average-phase coupled map presents arnold tongues for all k \u0026lt; 0.03 (approximately). Of course, $\\omega$ does nothing here: this map is not coupled by external force, but just by average phase!! The randomly-coupled map has a completely random, pixelated behaviour: we cannot find any arnold tongues (or any pattern really) For both of the externally coupled maps, arnold tongues are found, and in a pretty simmilar way: the higher the k, the more likely the oscilators will be coupled. However, with regards to omega, there seems to be a treshold around $2 \\cdot \\pi$, after which no coupling is found; in fact, there seems to be a point of \u0026ldquo;maximum disorder\u0026rdquo; around $3 \\cdot \\pi$, after which syncronization seems to get a bit better, probably doe to random syncronizations (noise) And, ¡that would mostly be it! As a bonus, if you wish, there is a way to plot a dynamic phase map, ¡just as in Ride my Kuramotorcicle! using the same code from Rochester University. Please, refer to their course for full details.\nAs a closing note, one way to improve the code in this Jupyter Notebook would be to rename the theta and omega vectors, which currently have to be updated anytime a function is called, to avoid conflict; however, I didnt have enough time for that :(\n","date":"November 17, 2021","hero":"/es/posts/master-en-biolog%C3%ADa-computacional/modelizaci%C3%B3n-de-sistemas-biol%C3%B3gicos/kuramoto-oscilators/hero.png","permalink":"https://www.pablomarcos.me/es/posts/master-en-biolog%C3%ADa-computacional/modelizaci%C3%B3n-de-sistemas-biol%C3%B3gicos/kuramoto-oscilators/","summary":"The Kuramoto Model is a mathematical model used to describe the synchronization of a large set of coupled oscillators. This formulation, which was motivated by the behaviour of both physical and biological oscilators, has found widespread application in neuroscience.\nThe model makes several assumptions, including that there is weak coupling, that the oscillators are identical or nearly identical, and that interactions depend sinusoidally on the phase difference between each pair of objects.","tags":null,"title":"The Kuramoto Oscilator Model"},{"categories":null,"contents":" A recent paper executes a meta-analysis of a few thousand published co-expressed gene sets from Arabidopsis. They break these co-expression sets into ~20 sub-networks of \u0026lt;200 genes each, that they find consistently co-expressed with one another. Assume that you want to take the next step in their analysis, and see if there is already information linking these predicted sub-sets into known regulatory networks. One step in this analysis would be to determine if the co-expressed genes are known to bind to one another.\nUsing the co-expressed gene list from the \u0026ldquo;Assignment 2\u0026rdquo; folder:\nUse a combination of any or all of: dbFetch, Togo REST API, EBI’s PSICQUIC REST API, DDBJ KEGG REST, and/or the Gene Ontology Find all protein-protein interaction networks that involve members of that gene list Determine which members of the gene list interact with each other. Documentation can be accessed using YarDoc:\nyardoc README.md find_interactions.rb ./Clases/Annotations.rb ./Clases/Gene.rb ./Clases/InteractionNetwork.rb ./Clases/ImportExportModule.rb yard server I have knowingly gitignored the html files from the repo to make it more legible, since they can be easily generated.\nTo run the script, use: ruby find_interactions.rb ./SampleData/ArabidopsisSubNetwork_GeneList.txt report_file.txt\n","date":"November 9, 2021","hero":"/es/posts/master-en-biolog%C3%ADa-computacional/bioinfo-programming-challenges/assignment-2/hero.jpg","permalink":"https://www.pablomarcos.me/es/posts/master-en-biolog%C3%ADa-computacional/bioinfo-programming-challenges/assignment-2/","summary":"A recent paper executes a meta-analysis of a few thousand published co-expressed gene sets from Arabidopsis. They break these co-expression sets into ~20 sub-networks of \u0026lt;200 genes each, that they find consistently co-expressed with one another. Assume that you want to take the next step in their analysis, and see if there is already information linking these predicted sub-sets into known regulatory networks. One step in this analysis would be to determine if the co-expressed genes are known to bind to one another.","tags":null,"title":"Your worst nightmare"},{"categories":null,"contents":" During the COVID-19 outbreak in spring 2020, preventive confinement at day D for susceptible individuals was implemented in several countries to “bend” the infection curve and control the spread of the virus. Let us assume that confined individuals are effectively protected from infection. A “bending” or flattening of the (log-scale) curve representing the number of active cases was widely publicized as a sign that the epidemics was under control.\nGiven a set of parameters compatible with the current knowledge about COVID-19, estimate the fraction of the susceptible population that must be put into preventive confinement to stop the spread of the virus. How does the delay D affect the effectiveness of the confinement? Explore parameter values that do and do not lead to virus remission. Is the bending of the active cases curve a good predictor of whether the infection will keep growing or remit? Study the date at which the peak of the infection is reached. How sensitive is it with respect to small changes in the parameters? Given the uncertainty of COVID-19 epidemiological parameters, what do you think about media predictions of the peak and end dates of the infection? To adequately model the CoViD-19 pandemic, I will use a compartimental model approach, more concretely a SEIR model based on a publication by Alóndiga et al which was actually created by\u0026hellip; hum! me!. In this work, we define a modified SEIR model with the following compartiments:\n$$\\frac{dS}{dt} = -\\beta I \\frac{S}{N} $$\n$$\\frac{dE}{dt} = \\beta I \\frac{S}{N} - \\delta E$$ $$\\frac{dI}{dt} = \\delta E - (1-a) \\gamma I - \\eta a I$$ $$\\frac{dH}{dt} = \\eta (1-a) I-\\tau (1-u) H - u \\sigma H$$ $$\\frac{dC}{dt} = u \\sigma H - m \\rho min(UCI, C) - max(0, C-UCI) - \\omega (1-m) min(UCI,C)$$ $$\\frac{dR}{dt} = \\gamma (1-a) I + \\tau (1-u) H + \\omega (1-m) min(UCI,C)$$ $$\\frac{dM}{dt} = m \\rho min(UCI, C) + max(0, C-UCI)$$\nThis compartiments can be described as:\nS: susceptible to contracting the disease E: exposed to infected individuals, incubate the disease but are not yet contagious. I: infected, spread the disease R: have overcome the disease, do not spread the virus and have developed immunity to it. C: infected individuals requiring intensive care. M: deceased by COVID-19 (For equations in LateX and Jupyter see here)\nThe meaning of all the parameters at large can be checked in the extended article, availaible here, but, to sum up the most interesting aspects of the model:\nimport sympy as sp #as allways, we need the imports first :D import numpy as np import scipy.integrate as scint import matplotlib.pyplot as plt The $R_{0} $ decreases progressively following a logistic curve starting with the date of aplication of the measures, A. This is to take into account natural human behaviour: people dont inmediately cessate all activity after confinement, and they dont just start taking preventive measures the day the confinement is decreted; rather, fear and necessity make for a more progressive approach: def R_0(t, R_0i, R_0f, k, A): return (R_0i-R_0f) / (1 + np.exp(-k*(-t+A))) + R_0f UCI internment is accounted for with the creation of an Hospitalizados and a Criticos class, made up of the individuals that require internment and of those that require intensive care, and are thus more at risk of death. ICU occupation rates are also accounted for, and it is estimated that, due to political pressure, UCI capacity will also increase with time def modelo(dias, N, UCI_100, R_0i, R_0f, k, A, u, m, a, s): def b(t): return R_0(t, R_0i, R_0f, k, A) * gamma def UCI(t): UCI_0 = UCI_100 / 100_000 * N return UCI_0 + s*UCI_0*t P0 = N-1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0 tiempo = np.linspace(0, dias-1, dias, dtype=int) Ps = scint.odeint(F, P0, tiempo, args=(N, b, m, u, a, UCI)) S = Ps[:,0] E = Ps[:,1] I = Ps[:,2] H = Ps[:,3] C = Ps[:,4] R = Ps[:,5] M = Ps[:,6] R_0t = R_0(tiempo, R_0i, R_0f, k, A) return S, E, I, H, C, R, M, R_0t Finally, we can tell python the equations we will use; the parameters, such as delta and rho, are extracted from relevant previous literature:\ndelta = 1/3; gamma = 1/9.0; sigma = 1/4.5 rho = 1/14; omega = 1/6.5; eta = 1/9.0 tau = 1/14; UCI_100 = 9.7 #UCI_100 = the pre-existing UCI capacity per 100.000 habitants def F(P, t, N, b, m, u, a, UCI): S, E, I, H, C, R, M = P dS = -b(t) * S * I / N dE = b(t) * S * I / N - delta * E dI = delta * E - I*eta*a - I*gamma*(1-a) dH = eta*a*I-tau*(1-u)*H-u*sigma*H dC = u*sigma*H - m*rho*min(UCI(t),C) - max(0,C-UCI(t))-omega*(1-m)*min(UCI(t),C) dR = I*gamma*(1-a) + tau*(1-u)*H + omega*(1-m)*min(UCI(t),C) dM = m*rho*min(UCI(t),C) + max(0,C-UCI(t)) return dS, dE, dI, dH, dC, dR, dM In the paper, the authors fit the model to actual, real, pre-existing data, and come up with the following values, with 2 exceptions: population is the Spainish population in the 2020 census, according to Wikipedia, and \u0026ldquo;dias\u0026rdquo; is the number of days we are studying; in this case, 184\nR_0i = 4.244; R_0f= 0.848; u = 0.150 k = 5.392; m = 0.199; a = 0.125 s = 0.012; population = 47450795; dias = 184 And that\u0026rsquo;s it! With this, we can define the plot function that we will use to analyse the behaviour of covid (and of our model!)\ndef plotseircm(A): mod = modelo(dias, population, UCI_100, R_0i, R_0f, k, A, u, m, a, s) tiempo = np.linspace(0,dias) susceptibles = mod[0] exposeados = mod[1] infectados = mod[2] hospitalizados = mod[3] criticos = mod[4] recuperados = mod[5] muertos= mod[6] R_0t = mod[7] muertes_diarias=[0] for i in range(0, len(muertos)-1): muertes_diarias.append(muertos[i+1]-muertos[i]) plt.plot(range(dias), exposeados, \u0026#34;-\u0026#34;, label = \u0026#34;Exposed\u0026#34;, color =\u0026#39;#ff7f0e\u0026#39;) plt.plot(range(dias), infectados, \u0026#34;-\u0026#34;, label = \u0026#34;Infected\u0026#34;, color = \u0026#39;#2ca02c\u0026#39;) plt.plot(range(dias), hospitalizados, \u0026#34;-\u0026#34;, label = \u0026#34;Hospitalized\u0026#34;, color = \u0026#39;#e377c2\u0026#39; ) plt.plot(range(dias), criticos, label = \u0026#34;Critical\u0026#34;, color = \u0026#39;#d62728\u0026#39;) plt.plot(range(dias), muertos, \u0026#34;-\u0026#34;, label = \u0026#34;Muertos\u0026#34;, color = \u0026#39;black\u0026#39;) plt.xlabel(\u0026#34;Time (days)\u0026#34;) plt.ylabel(\u0026#34;Population\u0026#34;) plt.xticks(rotation=40) plt.legend() plt.subplots_adjust(bottom=0.2, top=1.3) #So that plots look cute plt.ticklabel_format(style=\u0026#39;plain\u0026#39;, axis = \u0026#39;y\u0026#39;) #Guardamos como pinta el panorama panorama=[muertos[123],susceptibles[123], exposeados[123],infectados[123],hospitalizados[123], criticos[123],recuperados[123]] return panorama We define a dataframe to store the raw, numeric values we will later use to compare, and we plot the graphs:\ntotals = [] plt.subplots(figsize=(15, 10)) plt.subplot(2, 2, 1) plt.title(\u0026#39;Ground Truth\u0026#39;) totals.append(plotseircm(73)) plt.subplot(2, 2, 2) plt.title(\u0026#39;Scenario 1: 1 Week Before\u0026#39;) totals.append(plotseircm(68)) plt.subplot(2, 2, 3) plt.title(\u0026#39;Scenario 2: 1 Week After\u0026#39;) totals.append(plotseircm(80)) plt.subplot(2, 2, 4) plt.title(\u0026#39;Scenario 3: No confinement\u0026#39;) totals.append(plotseircm(100000)) #Out of scope, effectively no confinement plt.show() import pandas as pd df = pd.DataFrame(columns=[\u0026#39;Dead\u0026#39;,\u0026#39;Susceptible\u0026#39;,\u0026#39;Exposed\u0026#39;,\u0026#39;Infected\u0026#39;,\u0026#39;Hospitalized\u0026#39;, \u0026#39;Critical\u0026#39;,\u0026#39;Recovered\u0026#39;], data=totals) pd.set_option(\u0026#39;display.float_format\u0026#39;, lambda x: \u0026#39;%.f\u0026#39; % x) df Dead Susceptible Exposed Infected Hospitalized Critical Recovered 0 25085 44592488 75403 268567 47499 11390 2430362 1 3747 46234518 30963 107323 18580 4946 1050718 2 201714 38982780 203888 804348 154462 15043 7088558 3 1262664 6799399 149374 2571360 895854 41298 35730846 As we can see, there is quite a big difference between the 4 scenarios, so, yes, ¡confinement is useful! Also in the table above, we can see that, the bigger the delay \u0026lsquo;D\u0026rsquo;, the more ineffective the confinement is, as the epidemic has had more time to progress; had the Spanish government decreed confinement a week earlier, more than 20000 lives could have been spared.\nNow, given the known parameters for the model (provided in cell [4]), we can calculate the fraction of the susceptible population that must be put into preventive confinement to stop the spread of the virus. According to PNAS, mathematically, \u0026ldquo;the herd immunity threshold can be very simply calculated as $\\omega = 1 - \\frac{1}{R_{0}} $\u0026rdquo;. If we take the model\u0026rsquo;s predicted $R_{0}i $ (that is, the one that corresponds to free virus circulation) of 4.244, we get an $\\omega $ of $1 - \\frac{1}{4.244} $ = 0.7643, that is, 76.43 percent of the population that needs to be virus-proof for the epidemic to cessate. If we discount the percentage of infected population, I, from that percentage, we get the fraction of individuals that must be confined to stop the disease from spreading.\nGiven that the Infected population is constantly changing, as is $R_{0} $ according to our model, this proportion might be better defined by creating a function.\nNow, lets explore some different parameter values and see wether they lead to virus remision or not. I will play with $\\delta $, $\\gamma $ and $\\eta $, the original pàrameters in the SEIR model; $\\beta $ cannot be modified since it is set to dynamically change with time (our way of affecting $R_{0}(t) $).Thus:\nplt.subplots(figsize=(15, 10)) plt.subplot(2, 2, 1) delta = 0.3; gamma = 0.11; eta = 0.11 plt.title(\u0026#39;Evolution when $\\delta$ = {0}, $\\gamma$ = {1} and $\\eta$ = {2}\u0026#39;.format(str(delta), str(gamma), str(eta))) plotseircm(1000); plt.subplot(2, 2, 2) delta = 0.7; gamma = 0.11; eta = 0.11 plt.title(\u0026#39;Evolution when $\\delta$ = {0}, $\\gamma$ = {1} and $\\eta$ = {2}\u0026#39;.format(str(delta), str(gamma), str(eta))) plotseircm(1000); plt.subplot(2, 2, 3) delta = 0.3; gamma = 0.9; eta = 0.11 plt.title(\u0026#39;Evolution when $\\delta$ = {0}, $\\gamma$ = {1} and $\\eta$ = {2}\u0026#39;.format(str(delta), str(gamma), str(eta))) plotseircm(1000); plt.subplot(2, 2, 4) delta = 0.3; gamma = 0.11; eta = 0.75 plt.title(\u0026#39;Evolution when $\\delta$ = {0}, $\\gamma$ = {1} and $\\eta$ = {2}\u0026#39;.format(str(delta), str(gamma), str(eta))) plotseircm(1000); plt.show() As was predictable, each one of the parameters does what it was assigned to do: $\\delta $ is the rate at which individuals incubating the virus develop symptoms and become infectious, so increasing it leads to a huge spike in the Infected department (although,in reality, it may lead to a decrease: the big problem with CoViD are asymptomatic patients). $\\gamma $ is the inverse of the period of recuperation: if we increase it, the Exposed department increases, as, when individuals take more to recover, they can infect more people; finally, $\\eta $, the inverse of the period of symptom development: if it increases, the number of infected patients decrease, while the hospitalizations increase, as do, in turn, deaths. Exposed, however, remains constant: it seems like the infection has become more lethal.\nWith regards to the bending of the curve, common sense, as well as the analysis on Exercise 2.1, seem to indicate that, while the bending is not a predictor of wether the infection will end or not, it is, in fact, a good predictor of its evolution (it is, after all, a growth curve), so it does help decide wether the infection will keep growing or remit for the time being, helping the system cope.\nFinally, we can study the peak of the infection and its sensibility to small changes in the parametes:\nplt.subplots(figsize=(15, 10)) plt.subplot(2, 2, 1) delta = 0.3333; gamma = 0.1111; eta = 0.1111 plt.title(\u0026#39;Evolution when $\\delta$ = {0}, $\\gamma$ = {1} and $\\eta$ = {2}\u0026#39;.format(str(delta), str(gamma), str(eta))) plotseircm(1000); plt.subplot(2, 2, 2) delta = 0.3333; gamma = 0.15; eta = 0.15 plt.title(\u0026#39;Evolution when $\\delta$ = {0}, $\\gamma$ = {1} and $\\eta$ = {2}\u0026#39;.format(str(delta), str(gamma), str(eta))) plotseircm(1000); plt.subplot(2, 2, 3) delta = 0.4; gamma = 0.1111; eta = 0.1111 plt.title(\u0026#39;Evolution when $\\delta$ = {0}, $\\gamma$ = {1} and $\\eta$ = {2}\u0026#39;.format(str(delta), str(gamma), str(eta))) plotseircm(1000); plt.subplot(2, 2, 4) delta = 0.5; gamma = 0.2; eta = 0.2 plt.title(\u0026#39;Evolution when $\\delta$ = {0}, $\\gamma$ = {1} and $\\eta$ = {2}\u0026#39;.format(str(delta), str(gamma), str(eta))) plotseircm(1000); plt.show() As can be seen, the impact of really, really small changes in the parameter\u0026rsquo;s value is neglegible, but, when we do a somewhat bigger modification9, the thing changes: the prediction is off by more than 20 days. Given that accurate models can become more and more complex, whith an increased number of parameters and equations, small changes in said parameters can widely change the \u0026ldquo;peak date\u0026rdquo; prediction. In general, it can be said that it is difficult to predict things in the field of mathematical biology, and that models in general do have some degree of uncertainty; however, even though media predictions were in fact bad, they were the best we had, and thus modelling remains an important and useful field to manage pandemics.\nPost header by Our World in Data via thelocal.es - CC BY-SA 4.0.\n","date":"November 3, 2021","hero":"/es/posts/master-en-biolog%C3%ADa-computacional/modelizaci%C3%B3n-de-sistemas-biol%C3%B3gicos/seir-modified-model/hero.jpg","permalink":"https://www.pablomarcos.me/es/posts/master-en-biolog%C3%ADa-computacional/modelizaci%C3%B3n-de-sistemas-biol%C3%B3gicos/seir-modified-model/","summary":"During the COVID-19 outbreak in spring 2020, preventive confinement at day D for susceptible individuals was implemented in several countries to “bend” the infection curve and control the spread of the virus. Let us assume that confined individuals are effectively protected from infection. A “bending” or flattening of the (log-scale) curve representing the number of active cases was widely publicized as a sign that the epidemics was under control.","tags":null,"title":"Analysis of the efects of Lockdowns on CoViD"},{"categories":null,"contents":"Introducción La estadística es, según la RAE, el \u0026ldquo;Estudio de los datos cuantitativosde la población, de los recursos naturales e industriales, del tráfico o de cualquier otra manifestación de las sociedades humanas.\u0026rdquo; Sin embargo,cabe preguntarnos, ¿de qué sirve todo esto? Tiene sentido buscarpatrones ocultos en los datos, o son estos patrones un mero artefacto denuestra imaginación?\nLo cierto es que, en la naturaleza, la objetividad es una ilusión: los datos simplemente existen, sin más, y somos los seres humanos los que,mediante el análisis matemático de los mismos, podemos conferirles un significado que los convierta en un sistema estructurado de información y conocimiento.\nEs así que la estadística es uno de los métodos científicos más fiables de los que disponemos los seres humanos a la hora de buscar significados a conjuntos de datos aparentemente aleatorios, y, por tanto, para tomar decisiones de manera racional y sistemática.\nEl pensador racional En los textos de literatura clásica grecorromana, se genera el concepto de pensador racional: se trata del ser humano deseable para los antiguos, capaz de aislar los sentimientos y tomar decisiones usandosolamente la razón, siguiendo las leyes de la aritmética para discriminar, con una perfección maquinal, cuál es la carta ganadora,libre al fin de influencias externas. Este es, por cierto, elmismo camino seguido por Descartes en su famoso \u0026ldquo;Cogito, ergo sum\u0026quot;,donde llega a demostrar, usando su razón como elemento aislado, su propia existencia.\nAdemás, el pensador debe aprender a distinguir la verdadera racionalidad de la inteligencia: mientras que la segunda se refiere únicamente a los mecanismos mentales que garantizan la supervivencia, usando patronespre-entrenados del cerebro para tomar decisiones de manera rápida, la primera, entrenable mediante la educación, nos permite analizar cuidadosa e individualmente cada problema, buscando la respuesta más adecuada. Dicho de otro modo, en palabras de Sherlock Holmes: \u0026ldquo;No hay nada más engañoso que una cosa evidente\u0026rdquo;\nRepresentación de un problema A la hora de tomar decisiones, existen una serie de alternativas,variables dependientes de un contexto cambiante que sólo con su debido tiempo revelará cuál era la opción más óptima.\nA la vista de la aleatoreidad del tiempo, uno podría caer en una visión nihilista del mundo, y plantearse si, dada la imposibilidad de conocer el futuro (descontando claro, chamanes y pitonisas por igual) es imposible tomar decisiones con sentido. Ante esta terrible perspectiva,aparece el concepto de consecuencias, el retorno potencial de inversión sobre una serie de alternativas y contextos posibles, que nos puede ayudar a tomar decisiones.\nDe esta manera, podemos representar un problema de distintas maneras: bien como una tabla, donde las filas son las alternativas, las columnas, los posibles escenarios, y la intersección de las mismas, las consecuencias obtenidas; o bien como un árbol, donde cada tronco representa una alternativa, cada rama un escenario, y cada hoja una consecuencia\nVer diapositivas 11 y 13 de la Presentación \u0026ldquo;Lesson 1 (October 5th)\u0026rdquo; para ejemplos\nToma de decisiones: certeza vs incertidumbre Ahora que sabemos representar sistemáticamente un problema, nos preguntamos: ¿cómo tomar la decisión acertada? Para ello, existen una serie de criterios estandarizados, que dependen del tipo de problema al que nos enfrentemos:\nDecisiones bajo certeza: son aquellas en las que existen relaciones directas de causa-efecto entre cada acción y sus consecuencias: por ejemplo, si queremos gestionar recursos productivos, podemos usar métodos tales como la programación lineal(optimización), o las teorias de valores cardinales y ordinales. Decisiones bajo incertidumbre: aparecen dos nuevos conceptos: elriesgo, cuando conocemos las posibilidades de las distintas consecuencias (aunque no sepamos, lógicamente, cual triunfará) y laincertidumbre o incertidumbre estricta, donde ni siquiera conocemos las probabilidades de las distintas consecuencias. Para esta clase de casos, tenemos los siguientes métodos: Criterio Maximin-Minmax de Wald o criterio pesimista: Para cada alternativa se supone el peor escenario, y, dentro de este, se elige la alternativa que conduce a la mejor consecuencia. Su mayor crítica es que, al ser un método conservador (supone el peor escenario), podríamos estar renunciando a beneficios enormes que no tengan un riesgo tanto mayor Criterio Maximax: Asume el mejor escenario, y, de él, elige la alternativa que conduce a la mejor consecuencia. No es muy utilizado por su exagerado optimismo. Criterio de Hurwicz: Combina los puntos de vista pesimista y optimista, valorando cada alternativa con una combinación ponderada entre la mejor y la peor Criterio de Savage o coste-oportunidad: Las consecuencias de cada alternativa se comparan con las consecuencias de las demás bajo el mismo contexto. Se trata, por así decirlo, de encontrar la opción \u0026ldquo;menos mala\u0026rdquo;, y es muy usado en el ámbito de la economía. -Criterio de \u0026quot;razón insuficiente\u0026quot; de Laplace: Tiene en cuenta todos los valores. Como las probabilidades de los contextos son desconocidas, se asigna el mismo valor de probabilidad para todos ellos, eligiendo la alternativa con mayor valor esperado. Si lo modificamos para añadir probabilidades subjetivas, obtenemos la Modificación Bayesiana. Excepto el criterio de Laplace, que es más objetivo, todos los criterios dependen en los sentimientos subjetivos del pensador, y en su nivel de optimismo. Para encontrar un compromiso entre objetividad y el punto de vista del tomador de decisiones, nace la Teoría de la Utilidad Esperada, una herramienta común para decisiones bajo incertidumbre y bajo certeza. En esta teoría, las consecuencias se valoran según su preferencia, y los escenarios según las creencias del tomador de decisiones racional, que es el agente objetivo: se genera así unafunción de utilidad,que contiene las posibles consecuencias, y elobjetivo es encontrar una alternativa tal, que maximice la utilidad.\nToma de decisiones: colectivas vs interactivas En función del número de participantes, las decisiones se pueden clasificar en:\nDecisiones no interactivas: También llamadas decisiones colectivas, son aquellas en las que un grupo, en lugar de un sólo individuo, debe tomar la decisión. Caso ordinal: En este caso, hay una cantidad finita de alternativas X, con un número también finito de agentes que tienen una serie de preferencias sobre el set X. Hay maneras de proceder: una regla de votación es una función que asigna una alternativa a cada serie de preferencias, escogiéndose la opción con más votos, sin elaborar un ranking; por otro lado, una regla de decisiones sociales hace un ranking de las alternativas según las preferencias de los agentes. Aunque el segundo es el ideal, al asegurar que se obtiene la opción con la que más contenta estaría la mayoría, el primero resulta muchas veces más práctico y realista. Caso cardinal: Permite llevar a cabo una agregación de las funciones de utilidad de la TUE, con distintos Principios a la hora de seleccionar el valor óptimo: el Liberal (mayoritario), el Social (equilibrado), el de Fraternidad (minoritario) o el Aristocrático (no todas las funciones de utilidad cuentan igual)• Decisiones no interactivas: También llamadas decisiones pornegociación, permiten establecer el efecto que las acciones de unindividuo tendrá en el/los otro/s, a diferencia de lo que ocurría en lasdecisiones colectivas, donde las decisiones, si bien influyen en larealidad material de los demás electores, no deben influir en el sentidode su voto. Toma de decisiones: multicriterio Un tomador de decisiones puede querer obtener varias cosas de su elección; por ejemplo, un banquero querrá maximizar el retorno de la inversión, pero también minimizar el tiempo en que esto sucede y el riesgo al que expone su patrimonio. Ante esto, tenemos que decidir qué tipo de estrategias utilizar: eficientes, de compromiso o satisfactorias.\nConclusiones La estadística es una de las más potentes herramientas de las que disponemos los seres humanos a la hora de tomar decisiones informadas sobre el mundo. Dejando a un lado los sentimientos, y tomando un enfoque racional, podemos encontrar la solución más óptima a todos los problemas de la vida, usando cualquiera de los métodos anteriormente mencionados en función de nuestras preferencias y nuestra tolerancia al riesgo. Por ejemplo, usando métodos de negociación colectiva, podemos encontrar la manera de resolver problemas tan graves como el cambio climático, la crisis de la democracia o la pobreza, evitando en el camino problemas como la tragedia de los comunes.\nLa estadística marca así el camino, pero, en el juego de la vida, somos nosotros los que decidimos, y es nuestra responsabilidad usar las conclusiones científicas y racionales como nosotros deseemos; y es aquí donde nuestro juego de valores, y nuestros sentidos, sí que pueden jugar un papel mas allá de la razón, siempre teniendo en cuenta las consecuencias de nuestras acciones.\nReferencias Algunos recursos que me han interesado, que me han ayudado a entender este tema o que me han parecido de interés:\nBayes theorem, the geometry of changing beliefs.https://www.youtube.com/watch?\nGame Theory: The Science of Decision-Making. https://www.youtube.com/watch?v=MHS-htjGgSY\nLa curiosa historia del hombre que NO podía usar sus emociones: https://www.youtube.com/watch?v=Y85OfTdWFYI\n«Politics Podcast». FiveThirtyEight,https://fivethirtyeight.com/tag/politics-podcast/, concretamente la sección \u0026ldquo;Good use of polling or bad use of polling\u0026rdquo;\n","date":"November 2, 2021","hero":"/es/posts/master-en-biolog%C3%ADa-computacional/estad%C3%ADstica/lecture-notes/hero.jpg","permalink":"https://www.pablomarcos.me/es/posts/master-en-biolog%C3%ADa-computacional/estad%C3%ADstica/lecture-notes/","summary":"Introducción La estadística es, según la RAE, el \u0026ldquo;Estudio de los datos cuantitativosde la población, de los recursos naturales e industriales, del tráfico o de cualquier otra manifestación de las sociedades humanas.\u0026rdquo; Sin embargo,cabe preguntarnos, ¿de qué sirve todo esto? Tiene sentido buscarpatrones ocultos en los datos, o son estos patrones un mero artefacto denuestra imaginación?\nLo cierto es que, en la naturaleza, la objetividad es una ilusión: los datos simplemente existen, sin más, y somos los seres humanos los que,mediante el análisis matemático de los mismos, podemos conferirles un significado que los convierta en un sistema estructurado de información y conocimiento.","tags":null,"title":"Notas de Clase"},{"categories":null,"contents":" A basic SIR model, such as the one that we will now proceed to study, includes three cathegories:\nSusceptibles: The individuals that may be infected Infected: The individuals currently carrying the virus, which can pass it on to the next generation Recovered: Those that have been infected, have recovered, and cannot pass on the infection (this model assumes permanent inmunity) Which relate among them using the following parameters:\n$\\beta$: number of infected persons per sick individual per day. $\\mu$: 1/period of infection. This is the rate at which infected individuals recover. Thus, we get the following set of equations that define the model:\n$$\\frac{dS}{dt} = -\\beta S I \\qquad \\frac{dI}{dt} = \\beta S I - \\mu I \\qquad \\frac{dR}{dt} = \\mu I$$\nAs we can see, the system does not explicitly reference time, and rather works indirectly with the variation in the population of the different epidemiological groups. Given that $\\mu$ is defined as the time an individual takes to recover, we can define the parameter $\\tau = \\mu^{-1}$ as the average time an individual remains infected. To set an epidemic treshold, we can use $R_{0}$, a tried and tested parameter defined as:\n$$R_{0} = \\frac{\\beta \\cdot S_{0}}{\\mu}$$\nHere, if $R_{0}$ \u0026gt; 1, the infection propagates, growing exponentially; if, however, $R_{0}$ \u0026lt; 1, the infection progressively deflates, ending eventually. In this model, the longer an individual spends Infected ($\\uparrow \\tau$, and thus $\\downarrow \\mu$), the higher the $R_{0}$, and, thus, the more contagious the infection is. For $\\beta$, the effect is the opposite: the more individuals one is able to infect, the more contagious the disease gets.\nTo understand the effecfts of the initial conditions, we can use the plane phase:\n#Imports module import matplotlib.pyplot as plt import numpy as np def plane_phase(b, u): S, I = np.meshgrid(np.arange(0,10,1), np.arange(0,10,1)) f = -b*S*I g = b*S*I-u*I plt.quiver(S,I,f,g) plt.title(\u0026#39;Quiver graph for β = {0} \u0026amp; u = {1}\u0026#39;.format(b, u)) plt.xlabel(\u0026#39;Susceptible\u0026#39;) plt.ylabel(\u0026#39;Infected\u0026#39;) We can plot it for different parameter values that generate an $R_{0}$ above the epidemic value of 1:\nplt.subplots(figsize=(12, 5)) plt.subplot(1, 2, 1) plane_phase(100, 1) plt.subplot(1, 2, 2) plane_phase(5, 4) plt.show() As we can see, despite the different set values, if $R_{0}$ \u0026gt; 1, the infected population grows quickly, making the susceptible population decrease as fast (since N = I + S + R = constant). If, however, we select values which get us to an $R_{0}$ \u0026lt; 1, we get an increasingly smaller infected population, with susceptibles decreasing or even staying the same as the epidemic extinguishes so fastly it cannot even turn Susceptibles to Infected an Recovered at a rate big enough to show in the graph. As we can see in the last example, values of $\\beta$ = 1 and $\\mu$ = 1 do not necessarily produce neither $R_{0}$ = 1 (as it is also dependent on $S_{0}$) nor stability (as will be discussed below).\nplt.subplots(figsize=(20, 5)) plt.subplot(1, 3, 1) plane_phase(1, 75) plt.subplot(1, 3, 2) plane_phase(0.3, 4) plt.subplot(1, 3, 3) plane_phase(1, 1) plt.show() With regards to stability, we can mathematically analyze the system. Since the population, N, remains constant (as the model ignores new births and deaths), we can skip the Recovered class in order to understand how the system behaves (See \u0026ldquo;Mathematical Models in Biology, by Leah Edelstein, page 247): it would be simply equivalent to saying that all Recovered patients go outside the scope of the model, since, assuming they are inmune,they can no longer go back to any other compartiment, and stop being of interest to us.\nimport sympy as sp import warnings warnings.filterwarnings(\u0026#34;ignore\u0026#34;) S, I, R, b, u = sp.symbols(\u0026#39;S I R β 𝜇\u0026#39;) f = -b*S*I g = b*S*I-u*I f, g (-I*S*β, I*S*β - I*𝜇) sp.solve([f, g], [S, I]) [] As we can see, the model has no solution, no steady states and no points where the population of Infected and Susceptible individuals remain unchanged. This makes sense: the model assumes a constant population N = I + S + R, and with S only able to decrease and R only able to increase, as new births and deaths are not accounted for. This result is consistent with what we found when playing with the plane phase: there are no points of stability, only inevitable derive towards extinction or full infection and recovery\nThe maximum incidence, i.e. the moment in time with the most infected individuals, can be graphically plotted as:\nfrom scipy.integrate import odeint import mpld3 mpld3.enable_notebook() def deriv(y, t, N, beta, u): S, I, R = y dSdt = -beta * S * I / N dIdt = beta * S * I / N - u * I dRdt = u * I return dSdt, dIdt, dRdt N = 1000 S0, I0, R0 = 999, 1, 0 # initial conditions: one infected, rest susceptible t = np.linspace(0, 39, 40) # Grid of time units y0 = S0, I0, R0 # Initial conditions vector def plotsir(t, S, I, R): plt.plot(t, S, \u0026#39;b\u0026#39;, alpha=0.7, linewidth=2, label=\u0026#39;Susceptible\u0026#39;) plt.plot(t, I, \u0026#39;y\u0026#39;, alpha=0.7, linewidth=2, label=\u0026#39;Infected\u0026#39;) plt.plot(t, R, \u0026#39;g\u0026#39;, alpha=0.7, linewidth=2, label=\u0026#39;Recovered\u0026#39;) plt.legend() plt.xlabel(\u0026#39;Time (days)\u0026#39;) #legend = ax.legend() #legend.get_frame().set_alpha(0.5) plt.subplots(figsize=(15, 10)) plt.subplot(2, 2, 1) plt.title(\u0026#39;Figure A1\u0026#39;); beta = 1.0; mu = 0.25 #Infection lasts 4 time units (days) # Integrate the SIR equations over the time grid, t. ret = odeint(deriv, y0, t, args=(N, beta, mu)); S, I, R = ret.T plotsir(t,S,I,R) plt.subplot(2, 2, 2) plt.title(\u0026#39;Figure B1\u0026#39;); beta = 7; mu = 0.5 ret = odeint(deriv, y0, t, args=(N, beta, mu)); S, I, R = ret.T plotsir(t,S,I,R) plt.subplot(2, 2, 3) plt.title(\u0026#39;Figure C1\u0026#39;); beta = 1; mu = 0.4 ret = odeint(deriv, y0, t, args=(N, beta, mu)); S, I, R = ret.T plotsir(t,S,I,R) plt.subplot(2, 2, 4) plt.title(\u0026#39;Figure D1\u0026#39;); beta = 1; mu = 0.7 ret = odeint(deriv, y0, t, args=(N, beta, mu)); S, I, R = ret.T plotsir(t,S,I,R) plt.show() In Figure A, we see that the maximum ratio of infections is reached after about 12 days; we can modify this to fit a different, more contagious epidemic, such as in Figure B. As we can see, in this crazy scenario, the peak is reached really abruptly, since the infection is incredibly contagious. The peak of infected individuals is reached afyer only 3 days, and, after 12 days, all the population (N=1000) is recovered (since this model does not account for deaths). With a more mild disease, the infected curve can be less and less steep, making the epidemic last longer, but probably overcharging the sanitary system less: see Figure C. We can even have a disease so mild it fails to infect all the population! In Figure D, a form of \u0026ldquo;herd immunity\u0026rdquo; is reached after 30 days.\nWe can also use this to confirm there are no endemic states when variating the initial conditions:\nbeta = 1.0; mu = 0.25 #Set params as in Figure A plt.subplots(figsize=(15, 10)) plt.subplot(2, 2, 1) plt.title(\u0026#39;Figure A2\u0026#39;); S0, I0, R0 = 999, 1, 0; y0 = S0, I0, R0 ret = odeint(deriv, y0, t, args=(N, beta, mu)); S, I, R = ret.T plotsir(t,S,I,R) plt.subplot(2, 2, 2) plt.title(\u0026#39;Figure B2\u0026#39;); S0, I0, R0 = 700, 300, 0; y0 = S0, I0, R0 ret = odeint(deriv, y0, t, args=(N, beta, mu)); S, I, R = ret.T plotsir(t,S,I,R) plt.subplot(2, 2, 3) plt.title(\u0026#39;Figure C2\u0026#39;); S0, I0, R0 = 500, 500, 0 ; y0 = S0, I0, R0 ret = odeint(deriv, y0, t, args=(N, beta, mu)); S, I, R = ret.T plotsir(t,S,I,R) plt.subplot(2, 2, 4) plt.title(\u0026#39;Figure D2\u0026#39;); S0, I0, R0 = 100, 900, 0 ; y0 = S0, I0, R0 ret = odeint(deriv, y0, t, args=(N, beta, mu)); S, I, R = ret.T plotsir(t,S,I,R) plt.show() As can be seen, the system behaves the same, changing the initial conditions is equivalent to moving the start of the epidemic over the \u0026ldquo;days\u0026rdquo; axis\nFinally, we can fix both the initial conditions and the value of the epidemic threshold, and change $\\beta$ and $\\mu$, such that we can better understand the qualitative role of such parameters. If we fix, say, $R_{0}$ = 1.5, $S_{0}$ = 999 and $I_{0}$ = 1, all possible combinations of $\\beta$ and $\\mu$ might given by $\\beta = \\mu \\cdot 1.5015$. Thus:\nS0, I0, R0 = 999, 1, 0 y0 = S0, I0, R0 plt.subplots(figsize=(15, 10)) plt.subplot(2, 2, 1) plt.title(\u0026#39;Figure A4\u0026#39;); beta = 10.5105; mu = 7 #Infection lasts 4 time units (days) ret = odeint(deriv, y0, t, args=(N, beta, mu)); S, I, R = ret.T plotsir(t,S,I,R) plt.subplot(2, 2, 2) plt.title(\u0026#39;Figure B4\u0026#39;); beta = 0.75075; mu = 0.5 ret = odeint(deriv, y0, t, args=(N, beta, mu)); S, I, R = ret.T plotsir(t,S,I,R) plt.subplot(2, 2, 3) plt.title(\u0026#39;Figure C4\u0026#39;); beta = 4.5045; mu = 3 ret = odeint(deriv, y0, t, args=(N, beta, mu)); S, I, R = ret.T plotsir(t,S,I,R) plt.subplot(2, 2, 4) plt.title(\u0026#39;Figure D4\u0026#39;); beta = 150.15; mu = 100 ret = odeint(deriv, y0, t, args=(N, beta, mu)); S, I, R = ret.T plotsir(t,S,I,R) plt.show() Here, we can see that, despite keeping $R_{0}$ stable, the widely different combinations of $\\beta$ and $\\mu$ produce quite different results; after all, it is $\\beta$ and $\\mu$ that define the evolution of the epidemic, with $R_{0}$ only an indirect way to know wether the population will keep rising or not; here, since $R_{0}$ was not that big (only 1.5) and the population was quite small (only 1000 individuals) the herd inmunity treshold is easy to reach. Nonetheless, despite the big visual differences among the graphs, we can see that all of them show quite similar evolutions, only with different time scales; thus, $R_{0}$ remains an interesting estimator of the general evolution of an epidemic.\nWith regards to the qualitative role of the parameters, $\\beta$ represents the number of infected persons per sick individual per day, and $\\mu$ the rate at which infected individuals recover. The bigger the first and the smallest the second, the more agressive the epidemic is.\nPost header by ARAKI Satoru at Wikimedia Commons - Own work, CC BY-SA 4.0.\n","date":"November 2, 2021","hero":"/es/posts/master-en-biolog%C3%ADa-computacional/modelizaci%C3%B3n-de-sistemas-biol%C3%B3gicos/sir-model/hero.png","permalink":"https://www.pablomarcos.me/es/posts/master-en-biolog%C3%ADa-computacional/modelizaci%C3%B3n-de-sistemas-biol%C3%B3gicos/sir-model/","summary":"A basic SIR model, such as the one that we will now proceed to study, includes three cathegories:\nSusceptibles: The individuals that may be infected Infected: The individuals currently carrying the virus, which can pass it on to the next generation Recovered: Those that have been infected, have recovered, and cannot pass on the infection (this model assumes permanent inmunity) Which relate among them using the following parameters:\n$\\beta$: number of infected persons per sick individual per day.","tags":null,"title":"Studying the SIR Model"},{"categories":null,"contents":"Your goal is to create a model that can predict whether a customer will churn (0 or 1) based on the features in this dataset. See the slides for more information.\nInitial Steps First, we need to create the Spark Session\n!apt-get install openjdk-8-jdk-headless -qq \u0026gt; /dev/null !wget -q https://mirrors.sonic.net/apache/spark/spark-3.1.2/spark-3.1.2-bin-hadoop3.2.tgz !tar xzf spark-3.1.2-bin-hadoop3.2.tgz !pip install -q findspark import os os.environ[\u0026#34;JAVA_HOME\u0026#34;] = \u0026#34;/usr/lib/jvm/java-8-openjdk-amd64\u0026#34; os.environ[\u0026#34;SPARK_HOME\u0026#34;] = \u0026#34;/content/spark-3.1.2-bin-hadoop3.2\u0026#34; import findspark findspark.init() from pyspark.sql import SparkSession spark = SparkSession.builder.master(\u0026#34;local[*]\u0026#34;).getOrCreate() Afterwards, we can read the file and inspect it\n#Please drop the file in the environment\u0026#39;s \u0026#39;Files\u0026#39; panel df = spark.read.options(header=\u0026#34;true\u0026#34;, inferSchema=\u0026#34;true\u0026#34;).csv(\u0026#34;/content/customer_churn.csv\u0026#34;) df.describe().toPandas() summary Names Age Total_Purchase Account_Manager Years Num_Sites Onboard_date Location Company Churn 0 count 900 900 900 900 900 900 900 900 900 900 1 mean None 41.81666666666667 10062.82403333334 0.4811111111111111 5.27315555555555 8.587777777777777 None None None 0.16666666666666666 2 stddev None 6.127560416916251 2408.644531858096 0.4999208935073339 1.274449013194616 1.7648355920350969 None None None 0.3728852122772358 3 min Aaron King 22.0 100.0 0 1.0 3.0 2006-01-02 04:16:13 00103 Jeffrey Crest Apt. 205 Padillaville, IA ... Abbott-Thompson 0 4 max Zachary Walsh 65.0 18026.01 1 9.15 14.0 2016-12-28 04:07:38 Unit 9800 Box 2878 DPO AA 75157 Zuniga, Clark and Shaffer 1 Deciding on the features of the Model #To see which columns we wish to select, we do: df.printSchema() root |-- Names: string (nullable = true) |-- Age: double (nullable = true) |-- Total_Purchase: double (nullable = true) |-- Account_Manager: integer (nullable = true) |-- Years: double (nullable = true) |-- Num_Sites: double (nullable = true) |-- Onboard_date: string (nullable = true) |-- Location: string (nullable = true) |-- Company: string (nullable = true) |-- Churn: integer (nullable = true) Here, we have to think wether it is work to index some columns: Both company name and location are pretty much unique, so I see no use for indexing them, although it could be interesting (but far too complicated) to index by country, for instance. I could also index the date by year, or add it to the model as datetime variable type, but, since I dont recall how to tell spark to accept a column as datetime (cant get .to_date() to work :/ ) I will leave it out for now.\n#And we proceed to select them: mycols = df.select([\u0026#39;Churn\u0026#39;, \u0026#39;Age\u0026#39;, \u0026#39;Total_Purchase\u0026#39;, \u0026#39;Years\u0026#39;, \u0026#39;Num_sites\u0026#39;]) #Hint: since \u0026#39;Account_Manager\u0026#39; is currently selected as random, we dont have to select it :p final_train = mycols.na.drop() Preparing the data Since this dataset did not provide us with a \u0026rsquo;train\u0026rsquo; and a \u0026rsquo;test\u0026rsquo; sub-database, we need to manually make the divission ourselves. Thus:\n(train, test) = final_train.randomSplit([0.7,0.3]) train.describe().show(), test.describe().show() +-------+-------------------+------------------+------------------+------------------+------------------+ |summary| Churn| Age| Total_Purchase| Years| Num_sites| +-------+-------------------+------------------+------------------+------------------+------------------+ | count| 604| 604| 604| 604| 604| | mean| 0.1456953642384106|41.764900662251655|10058.752913907281| 5.24854304635762| 8.509933774834437| | stddev|0.35309296232693316| 6.293619382523293|2443.5585608960027|1.2693878324304124|1.6725966135800687| | min| 0| 22.0| 100.0| 1.0| 4.0| | max| 1| 65.0| 16955.76| 9.15| 13.0| +-------+-------------------+------------------+------------------+------------------+------------------+ +-------+-------------------+-----------------+------------------+------------------+-----------------+ |summary| Churn| Age| Total_Purchase| Years| Num_sites| +-------+-------------------+-----------------+------------------+------------------+-----------------+ | count| 296| 296| 296| 296| 296| | mean|0.20945945945945946| 41.9222972972973|10071.131317567566|5.3233783783783775|8.746621621621621| | stddev|0.40761195202746325|5.782853919155999| 2339.838924409756| 1.285407304203774|1.932765316381917| | min| 0| 27.0| 3263.0| 1.81| 3.0| | max| 1| 58.0| 18026.01| 8.84| 14.0| +-------+-------------------+-----------------+------------------+------------------+-----------------+ (None, None) Creating the model Since it seems like the divission was correctly made, and like we have enough items both in the train and test dataframes, we can proceed to building the model based on the train data:\nfrom pyspark.ml.feature import (VectorAssembler, OneHotEncoder, VectorIndexer, StringIndexer) assembler = VectorAssembler(inputCols=[\u0026#39;Age\u0026#39;, \u0026#39;Total_Purchase\u0026#39;, \u0026#39;Years\u0026#39;, \u0026#39;Num_sites\u0026#39;], outputCol=\u0026#39;features\u0026#39;) Interesting to know is that we can use:\nfrom pyspark.ml.feature import (VectorAssembler, OneHotEncoder, VectorIndexer, StringIndexer) gender_indexer = StringIndexer(inputCol=\u0026#39;Sex\u0026#39;, outputCol=\u0026#39;SexIndex\u0026#39;) gender_encoder = OneHotEncoder(inputCol=\u0026#39;SexIndex\u0026#39;, outputCol=\u0026#39;SexVec\u0026#39;) embarked_indexer = StringIndexer(inputCol=\u0026#39;Embarked\u0026#39;, outputCol=\u0026#39;EmbarkedIndex\u0026#39;) embarked_encoder = OneHotEncoder(inputCol=\u0026#39;EmbarkedIndex\u0026#39;, outputCol=\u0026#39;EmbarkedVec\u0026#39;) To index vectors, and then add this to pipeline and assembler:\nassembler = VectorAssembler(inputCols=[\u0026#39;Pclass\u0026#39;,\u0026#39;SexVec\u0026#39;,\u0026#39;Age\u0026#39;,\u0026#39;SibSp\u0026#39;,\u0026#39;Parch\u0026#39;,\u0026#39;Fare\u0026#39;,\u0026#39;EmbarkedVec\u0026#39;], outputCol=\u0026#39;features\u0026#39;) pipeline = Pipeline(stages=[gender_indexer, embarked_indexer, gender_encoder, embarked_encoder, assembler, lr]) To work with parameters such as sex or others; however, as I have discussed previously, I will not use this here.\nfrom pyspark.ml.classification import LogisticRegression lr = LogisticRegression(featuresCol=\u0026#39;features\u0026#39;, labelCol=\u0026#39;Churn\u0026#39;) from pyspark.ml import Pipeline pipeline = Pipeline(stages=[assembler, lr]) model = pipeline.fit(train) Once we have developed the model, we curate the test data and use this\ncurated_test = test.na.drop() predictions = model.transform(curated_test) Visualizing the results Lets see the results!\npredictions.toPandas() Churn Age Total_Purchase Years Num_sites features rawPrediction probability prediction 0 0 27.0 8628.80 5.30 7.0 [27.0, 8628.8, 5.3, 7.0] [6.675936689418812, -6.675936689418812] [0.9987406968746404, 0.001259303125359601] 0.0 1 0 28.0 9090.43 5.74 10.0 [28.0, 9090.43, 5.74, 10.0] [2.1718950670750665, -2.1718950670750665] [0.8976971350218025, 0.10230286497819752] 0.0 2 0 29.0 9617.59 5.49 8.0 [29.0, 9617.59, 5.49, 8.0] [4.976543878227016, -4.976543878227016] [0.9931493932780102, 0.006850606721989783] 0.0 3 0 29.0 10203.18 5.82 8.0 [29.0, 10203.18, 5.82, 8.0] [4.801549127250805, -4.801549127250805] [0.9918499609427239, 0.00815003905727607] 0.0 4 0 30.0 13473.35 3.84 10.0 [30.0, 13473.35, 3.84, 10.0] [2.633769753642472, -2.633769753642472] [0.9330035738020525, 0.06699642619794755] 0.0 ... ... ... ... ... ... ... ... ... ... 291 1 50.0 14398.89 5.54 12.0 [50.0, 14398.89, 5.54, 12.0] [-2.6224366916584323, 2.6224366916584323] [0.06770831797659854, 0.9322916820234015] 1.0 292 1 51.0 8100.43 4.92 13.0 [51.0, 8100.43, 4.92, 13.0] [-3.522746635680445, 3.522746635680445] [0.02867190349642679, 0.9713280965035732] 1.0 293 1 55.0 5024.52 8.11 9.0 [55.0, 5024.52, 8.11, 9.0] [0.4878396300738217, -0.4878396300738217] [0.619597372506964, 0.380402627493036] 0.0 294 1 56.0 12217.95 5.79 11.0 [56.0, 12217.95, 5.79, 11.0] [-1.7215722913002764, 1.7215722913002764] [0.15166875344376754, 0.8483312465562325] 1.0 295 1 58.0 9703.93 5.16 11.0 [58.0, 9703.93, 5.16, 11.0] [-1.4845273670253718, 1.4845273670253718] [0.18474456093726596, 0.815255439062734] 1.0 296 rows × 9 columns\nEvaluating accuracy Now that we have made the predictions, lets evaluate the accuracy of our prediction model:\nfrom pyspark.ml.evaluation import BinaryClassificationEvaluator evaluator = BinaryClassificationEvaluator(rawPredictionCol=\u0026#39;prediction\u0026#39;, labelCol=\u0026#39;Churn\u0026#39;) acc = evaluator.evaluate(predictions) acc 0.738833746898263 As we can see, the precition model is not that great, but it works to an extent.As previously discussed, we may be able to improve the model by indexing other parameters.\n","date":"November 1, 2021","hero":"/es/posts/master-en-biolog%C3%ADa-computacional/big-data/predicting-churn-risk/hero.jpg","permalink":"https://www.pablomarcos.me/es/posts/master-en-biolog%C3%ADa-computacional/big-data/predicting-churn-risk/","summary":"Your goal is to create a model that can predict whether a customer will churn (0 or 1) based on the features in this dataset. See the slides for more information.\nInitial Steps First, we need to create the Spark Session\n!apt-get install openjdk-8-jdk-headless -qq \u0026gt; /dev/null !wget -q https://mirrors.sonic.net/apache/spark/spark-3.1.2/spark-3.1.2-bin-hadoop3.2.tgz !tar xzf spark-3.1.2-bin-hadoop3.2.tgz !pip install -q findspark import os os.environ[\u0026#34;JAVA_HOME\u0026#34;] = \u0026#34;/usr/lib/jvm/java-8-openjdk-amd64\u0026#34; os.environ[\u0026#34;SPARK_HOME\u0026#34;] = \u0026#34;/content/spark-3.1.2-bin-hadoop3.2\u0026#34; import findspark findspark.init() from pyspark.sql import SparkSession spark = SparkSession.","tags":null,"title":"Predicting Churn Risk"},{"categories":null,"contents":"Your job is to create a regression model that will help predict how many crew members will be needed for future ships. In other words, use the features you think will be useful to predict the value in the Crew column.\nCreate the Spark Session !apt-get install openjdk-8-jdk-headless -qq \u0026gt; /dev/null !wget -q https://mirrors.sonic.net/apache/spark/spark-3.1.2/spark-3.1.2-bin-hadoop3.2.tgz !tar xzf spark-3.1.2-bin-hadoop3.2.tgz !pip install -q findspark import os os.environ[\u0026#34;JAVA_HOME\u0026#34;] = \u0026#34;/usr/lib/jvm/java-8-openjdk-amd64\u0026#34; os.environ[\u0026#34;SPARK_HOME\u0026#34;] = \u0026#34;/content/spark-3.1.2-bin-hadoop3.2\u0026#34; import findspark findspark.init() from pyspark.sql import SparkSession spark = SparkSession.builder.master(\u0026#34;local[*]\u0026#34;).getOrCreate() Read the file and inspect it #Please drop the file in the environments \u0026#39;Files\u0026#39; panel df = spark.read.options(header=\u0026#34;true\u0026#34;, inferSchema=\u0026#34;true\u0026#34;).csv(\u0026#34;/content/cruise_ship_info.csv\u0026#34;) df.describe().toPandas() summary Ship_name Cruise_line Age Tonnage passengers length cabins passenger_density crew 0 count 158 158 158 158 158 158 158 158 158 1 mean Infinity None 15.689873417721518 71.28467088607599 18.45740506329114 8.130632911392404 8.830000000000005 39.90094936708861 7.794177215189873 2 stddev None None 7.615691058751413 37.229540025907866 9.677094775143416 1.793473548054825 4.4714172221480615 8.63921711391542 3.503486564627034 3 min Adventure Azamara 4 2.329 0.66 2.79 0.33 17.7 0.59 4 max Zuiderdam Windstar 48 220.0 54.0 11.82 27.0 71.43 21.0 Indexing non-numeric parameters Here, we can see that Cruise_line, which we were told is an important parameter, is an string variable. This is a problem! ML algorithms work better with numbers, as it is unclear how to process a string. ¿The solution? We can index the Cruise_line!\nSource: Stack Overflow\nfrom pyspark.ml import Pipeline from pyspark.ml.feature import StringIndexer #Bonus! Change this code to index multiple columns at once! indexers = [StringIndexer(inputCol=column, outputCol=column+\u0026#34;_index\u0026#34;).fit(df) for column in list([\u0026#34;Cruise_line\u0026#34;]) ] pipeline = Pipeline(stages=indexers) df_indexed = pipeline.fit(df).transform(df) df_indexed.describe().toPandas() summary Ship_name Cruise_line Age Tonnage passengers length cabins passenger_density crew Cruise_line_index 0 count 158 158 158 158 158 158 158 158 158 158 1 mean Infinity None 15.689873417721518 71.28467088607599 18.45740506329114 8.130632911392404 8.830000000000005 39.90094936708861 7.794177215189873 5.063291139240507 2 stddev None None 7.615691058751413 37.229540025907866 9.677094775143416 1.793473548054825 4.4714172221480615 8.63921711391542 3.503486564627034 4.758744608182735 3 min Adventure Azamara 4 2.329 0.66 2.79 0.33 17.7 0.59 0.0 4 max Zuiderdam Windstar 48 220.0 54.0 11.82 27.0 71.43 21.0 19.0 Plotting a scatter matrix Of course, the mean and stdev values for Cruise_line_index are pointless, since they just represent the Cruise Operator Company; BUT! they help us understand the data, since we can now use pandas to do a quick check among numeric variables, and see if they are related or not (source: TowardsDataScience)\nimport pandas as pd #Fun but interesting fact: if you add \u0026#34;;\u0026#34; at the end of your command, you dont get matplotlib axes warnings! Yuhuuu! pd.plotting.scatter_matrix(df_indexed.toPandas(), figsize=(10, 10)); We can see that there are some variables which show very good correlation between them, such as passenger number and number of cabins; this makes sense, and can be used as \u0026ldquo;intuitive proof\u0026rdquo; that we are doing something correctly.\nWith regard to our parameter of interest, crew, we see that it seems to be most closely relate to tonnage and number of cabins, another common sense, intuitive deduction, since bigger ships need more crew to manage them and to guide the customers.\nCrusise_line index doesnt seem to be very related to crew, but this could be an artifact caused by the ordering of the data in decreasing order of Cruse_line_index in the graph; with numeric correlation, a similar problem could arise, since, for potential correlation, we would expect the highest index (19) to have highest values. I will thus continue using the parameter on the model, as I was asked, and see what happens :p\nGenerating a ML Model Now that we have done an initial exploratory analysis on the data, lets try to generate a ML model using pyspark that tells us which characteristics here are most important:\nfrom pyspark.ml.feature import VectorAssembler #By using a list comprehension we can define inputcols as the exclusion of some columns from df_indexed assembler = VectorAssembler(inputCols= [e for e in df_indexed.columns if e not in (\u0026#39;Ship_name\u0026#39;, \u0026#39;Cruise_line\u0026#39;, \u0026#39;crew\u0026#39;)] , outputCol=\u0026#39;features\u0026#39;, handleInvalid=\u0026#39;skip\u0026#39;) output = assembler.transform(df_indexed) from pyspark.ml.feature import Imputer imputer = Imputer(inputCols=[\u0026#39;crew\u0026#39;], outputCols=[\u0026#39;label\u0026#39;], strategy=\u0026#39;mean\u0026#39;) imputer_model = imputer.fit(output) output = imputer_model.transform(output) Just to be sure everything went correctly in our pipeline, lets show the output dataframe:\noutput.show() +-----------+-----------+---+------------------+----------+------+------+-----------------+----+-----------------+--------------------+-----+ | Ship_name|Cruise_line|Age| Tonnage|passengers|length|cabins|passenger_density|crew|Cruise_line_index| features|label| +-----------+-----------+---+------------------+----------+------+------+-----------------+----+-----------------+--------------------+-----+ | Journey| Azamara| 6|30.276999999999997| 6.94| 5.94| 3.55| 42.64|3.55| 16.0|[6.0,30.276999999...| 3.55| | Quest| Azamara| 6|30.276999999999997| 6.94| 5.94| 3.55| 42.64|3.55| 16.0|[6.0,30.276999999...| 3.55| |Celebration| Carnival| 26| 47.262| 14.86| 7.22| 7.43| 31.8| 6.7| 1.0|[26.0,47.262,14.8...| 6.7| | Conquest| Carnival| 11| 110.0| 29.74| 9.53| 14.88| 36.99|19.1| 1.0|[11.0,110.0,29.74...| 19.1| | Destiny| Carnival| 17| 101.353| 26.42| 8.92| 13.21| 38.36|10.0| 1.0|[17.0,101.353,26....| 10.0| | Ecstasy| Carnival| 22| 70.367| 20.52| 8.55| 10.2| 34.29| 9.2| 1.0|[22.0,70.367,20.5...| 9.2| | Elation| Carnival| 15| 70.367| 20.52| 8.55| 10.2| 34.29| 9.2| 1.0|[15.0,70.367,20.5...| 9.2| | Fantasy| Carnival| 23| 70.367| 20.56| 8.55| 10.22| 34.23| 9.2| 1.0|[23.0,70.367,20.5...| 9.2| |Fascination| Carnival| 19| 70.367| 20.52| 8.55| 10.2| 34.29| 9.2| 1.0|[19.0,70.367,20.5...| 9.2| | Freedom| Carnival| 6|110.23899999999999| 37.0| 9.51| 14.87| 29.79|11.5| 1.0|[6.0,110.23899999...| 11.5| | Glory| Carnival| 10| 110.0| 29.74| 9.51| 14.87| 36.99|11.6| 1.0|[10.0,110.0,29.74...| 11.6| | Holiday| Carnival| 28| 46.052| 14.52| 7.27| 7.26| 31.72| 6.6| 1.0|[28.0,46.052,14.5...| 6.6| |Imagination| Carnival| 18| 70.367| 20.52| 8.55| 10.2| 34.29| 9.2| 1.0|[18.0,70.367,20.5...| 9.2| |Inspiration| Carnival| 17| 70.367| 20.52| 8.55| 10.2| 34.29| 9.2| 1.0|[17.0,70.367,20.5...| 9.2| | Legend| Carnival| 11| 86.0| 21.24| 9.63| 10.62| 40.49| 9.3| 1.0|[11.0,86.0,21.24,...| 9.3| | Liberty*| Carnival| 8| 110.0| 29.74| 9.51| 14.87| 36.99|11.6| 1.0|[8.0,110.0,29.74,...| 11.6| | Miracle| Carnival| 9| 88.5| 21.24| 9.63| 10.62| 41.67|10.3| 1.0|[9.0,88.5,21.24,9...| 10.3| | Paradise| Carnival| 15| 70.367| 20.52| 8.55| 10.2| 34.29| 9.2| 1.0|[15.0,70.367,20.5...| 9.2| | Pride| Carnival| 12| 88.5| 21.24| 9.63| 11.62| 41.67| 9.3| 1.0|[12.0,88.5,21.24,...| 9.3| | Sensation| Carnival| 20| 70.367| 20.52| 8.55| 10.2| 34.29| 9.2| 1.0|[20.0,70.367,20.5...| 9.2| +-----------+-----------+---+------------------+----------+------+------+-----------------+----+-----------------+--------------------+-----+ only showing top 20 rows Nice! As we can see, we have all our expected columns (the ones in the matrix except those in the list comprehension), and two new ones: features, which is what Spark will use for its ML approach, and label, which is populated with the crew values and will help us predict it.\n#Lets select the data we want to use: final_data = output.select([\u0026#39;features\u0026#39;, \u0026#39;label\u0026#39;]) We can now divide the data in two: a training dataset and a test dataset, which will be used to validate the conclussions of our trained data. For the process to work, sampling has to be random, so that no undue influence is exerted through data selection (say, if we picked the first 20 values); and, the data has to be divided somehow: here, we create an 65/35 split because I felt like 70/30 left too few individuals in the test data category\ntrain_data, test_data = final_data.randomSplit([0.65, 0.35]) final_data.describe().show(), train_data.describe().show(), test_data.describe().show() +-------+-----------------+ |summary| label| +-------+-----------------+ | count| 158| | mean|7.794177215189873| | stddev|3.503486564627034| | min| 0.59| | max| 21.0| +-------+-----------------+ +-------+------------------+ |summary| label| +-------+------------------+ | count| 104| | mean| 7.825096153846162| | stddev|3.4947555702107467| | min| 0.6| | max| 21.0| +-------+------------------+ +-------+------------------+ |summary| label| +-------+------------------+ | count| 54| | mean| 7.73462962962963| | stddev|3.5523607420094137| | min| 0.59| | max| 13.6| +-------+------------------+ (None, None, None) #We import the ML Linear Regression module and stablish the label from pyspark.ml.regression import LinearRegression lr = LinearRegression(labelCol=\u0026#39;label\u0026#39;) lr_model = lr.fit(train_data) test_results = lr_model.evaluate(test_data) Cheking the results of the model #The residuals for an observation is the difference between the observation (the y-value) and the fitted line. test_results.residuals.show() #And the root Mean Squared Error is a meassure of accuracy test_results.rootMeanSquaredError +--------------------+ | residuals| +--------------------+ | -1.3259944983423733| | -0.8514434688157184| | 0.25203246173775007| | -0.5786778245997795| | 0.17444151538670027| | -1.3832902288430144| | -0.8158551082693961| | -0.4427713845911221| | 0.3604712596502839| | -0.6603995683195034| |-0.25945919107257787| | 0.3813308210539432| | -0.7477823634981888| | 0.6855294216709122| | -0.6200715933698913| | -0.7413835788890388| | -0.6541795737066902| | 0.9199379275910768| | 0.9199379275910768| | -0.4181630087049122| +--------------------+ only showing top 20 rows 0.6939264035154783 Well\u0026hellip; it seems like we dont get reeeealy good values. Lets check the R² just in case\u0026hellip;\ntest_results.r2 0.9611214023707688 Woah! That\u0026rsquo;s a big R² !I guess our results were not so bad after all!!\nLets see the predictions then!\nlr_model.transform(test_data).show() +--------------------+-----+------------------+ | features|label| prediction| +--------------------+-----+------------------+ |[5.0,86.0,21.04,9...| 8.0| 9.325994498342373| |[6.0,30.276999999...| 3.55| 4.401443468815718| |[6.0,110.23899999...| 11.5| 11.24796753826225| |[6.0,112.0,38.0,9...| 10.9| 11.47867782459978| |[6.0,113.0,37.82,...| 12.0| 11.8255584846133| |[7.0,89.6,25.5,9....| 9.87|11.253290228843014| |[7.0,116.0,31.0,9...| 12.0|12.815855108269396| |[7.0,158.0,43.7,1...| 13.6|14.042771384591122| |[8.0,77.499,19.5,...| 9.0| 8.639528740349716| |[8.0,110.0,29.74,...| 11.6|12.260399568319503| |[9.0,59.058,17.0,...| 7.4| 7.659459191072578| |[9.0,81.0,21.44,9...| 10.0| 9.618669178946057| |[9.0,85.0,19.68,9...| 8.69| 9.437782363498188| |[9.0,88.5,21.24,9...| 10.3| 9.614470578329088| |[9.0,90.09,25.01,...| 8.69| 9.31007159336989| |[9.0,105.0,27.2,8...|10.68|11.421383578889039| |[9.0,110.0,29.74,...| 11.6| 12.25417957370669| |[9.0,113.0,26.74,...|12.38|11.460062072408924| |[9.0,113.0,26.74,...|12.38|11.460062072408924| |[10.0,81.76899999...| 8.42| 8.838163008704912| +--------------------+-----+------------------+ only showing top 20 rows Trying to improve the model Thats it! We have a LR, ML model that accurately (0.96) predicts the number of crew members in a Cruise ship. However, ¿could we do better? If we look back at the scatter matrix, we see that some values related better to crew than others. By excluding them, could we increase the models accuracy?\npd.plotting.scatter_matrix(df_indexed.toPandas(), figsize=(10, 10)); Lets try to remove passenger_density, Age and Cruise_line_index (even though yes, we were told this was an important parameter for the company) and see what happens:\nassembler = VectorAssembler(inputCols= [e for e in df_indexed.columns if e not in (\u0026#39;Ship_name\u0026#39;, \u0026#39;Cruise_line\u0026#39;, \u0026#39;crew\u0026#39;, \u0026#39;passenger_density\u0026#39;, \u0026#39;Age\u0026#39;, \u0026#39;Cruise_line_index\u0026#39;)] , outputCol=\u0026#39;features\u0026#39;, handleInvalid=\u0026#39;skip\u0026#39;) output = assembler.transform(df_indexed) imputer = Imputer(inputCols=[\u0026#39;crew\u0026#39;], outputCols=[\u0026#39;label\u0026#39;], strategy=\u0026#39;mean\u0026#39;) imputer_model = imputer.fit(output) output = imputer_model.transform(output) final_data = output.select([\u0026#39;features\u0026#39;, \u0026#39;label\u0026#39;]) train_data, test_data = final_data.randomSplit([0.65, 0.35]) lr = LinearRegression(labelCol=\u0026#39;label\u0026#39;) lr_model = lr.fit(train_data) test_results = lr_model.evaluate(test_data) test_results.residuals.show(), test_results.rootMeanSquaredError, test_results.r2 +--------------------+ | residuals| +--------------------+ | -0.8231610867408612| | -0.2639915612753747| | -0.1339915612753746| | -0.1943057005996871| |-0.22359048492732336| |-0.41929365405413854| | 0.01972284792550294| |-0.05123127867500...| | 0.08804027838669004| |-0.09625319517634523| | -0.2464422157023174| | 0.6476692405794164| | 0.11971319676579384| | 1.2077329156434358| | 0.2875155279976793| | -1.0783646108913167| | -0.1617301231003614| | -0.1617301231003614| | -0.6314669369748263| |-0.44057575750874367| +--------------------+ only showing top 20 rows (None, 0.9260213023938995, 0.9148216903559475) Wow! When we do this supposed \u0026ldquo;optimisation\u0026rdquo; of the model, we can see that, while the R² is not so different (0.91 vs 0.96, an acceptable decrease), the RMSD grows massively (from 0.69 to 0.92). And, given that an smaller RMSD is better this change in the model can be deemed unacceptable: all the characteristics together predict crew members better that when taking some of them out.\nA PCA approach could help us define which parameters are the most important for the model in a more \u0026lsquo;mathematic\u0026rsquo; way, but, for the time being, we will keep the first model: an R² of 0.96 is good enough, I believe.\n","date":"October 26, 2021","hero":"/es/posts/master-en-biolog%C3%ADa-computacional/big-data/predicting-crew-members/hero.jpg","permalink":"https://www.pablomarcos.me/es/posts/master-en-biolog%C3%ADa-computacional/big-data/predicting-crew-members/","summary":"Your job is to create a regression model that will help predict how many crew members will be needed for future ships. In other words, use the features you think will be useful to predict the value in the Crew column.\nCreate the Spark Session !apt-get install openjdk-8-jdk-headless -qq \u0026gt; /dev/null !wget -q https://mirrors.sonic.net/apache/spark/spark-3.1.2/spark-3.1.2-bin-hadoop3.2.tgz !tar xzf spark-3.1.2-bin-hadoop3.2.tgz !pip install -q findspark import os os.environ[\u0026#34;JAVA_HOME\u0026#34;] = \u0026#34;/usr/lib/jvm/java-8-openjdk-amd64\u0026#34; os.environ[\u0026#34;SPARK_HOME\u0026#34;] = \u0026#34;/content/spark-3.1.2-bin-hadoop3.2\u0026#34; import findspark findspark.","tags":null,"title":"Predicting Crew Members"},{"categories":null,"contents":"Fundamental concepts Blending Inheritance What implications from Blending Inheritance were dismissed by Mendel? Before Mendel, people used to think that, given that both parents contribute fluids to their offspring, the descendant\u0026rsquo;s traits would be a mixture of the two individual\u0026rsquo;s characteristcs: a \u0026ldquo;common middle ground\u0026rdquo;, a central point probably influenced by Aristotle\u0026rsquo;s theory of balance.\nThus, blending inheritance proponents assumed that half the variation is removed each generation, and that this variation had to be somehow replenished by mutation. Mendel\u0026rsquo;s main innovation was to show that, in fact, the offspring does not always represent the middle ground of the parents: some phenotypes are recessive, and may be hidden if a more dominant genotype appears.\nTest Crossing Indicate crosses and phenotypes you would make to get the genotype of a dominant phenotype This case, which is common to see in genetics, is called retrocrossing or test-crossing: to see the genotype of a dominant phenotype, we cross said dominant phenoptype with a recessive, homozygotic parent: if the offspring is 100% dominant, we can conclude the parent is homozygotic dominant; however, if the offspring is 50% dominant and 50% recessive, we can conclude the parent was heterozygotic, meaning with a dominant phenotype, but carrier of both an allele that, by itself, would produce a dominant phenotype, and one which would produce a recessive phenotype.\nThis, of course, only works under \u0026ldquo;normal\u0026rdquo; (i.e. trully mendelian) conditions: if there are epystatic phenotypes, or if any other non-mendelian form of heritance is involved, this method would need to be altered.\nAnother option, specifically for self-polinating organisms such as some plants, is selfing: we cross the individeual with itself, and analyse the results.\nBreeding Values Calculate the breeding values and rank for next generation cycle four genotypes with yield value of A=1200Kg/ha, B=5500kg/ha, C=900kg/ha and D=3600Kg/ha, given that heritability was 0.3, the mean of the yield values was 3500 kg/ha and that there were 2000 different wheat genotypes implied #Given that the Estimated Breeding Value = Heritability * P BreedingValueA \u0026lt;- 0.3*(1200) BreedingValueB \u0026lt;- 0.3*(5500) BreedingValueC \u0026lt;- 0.3*(900) BreedingValueD \u0026lt;- 0.3*(3600) paste(BreedingValueA, BreedingValueB, BreedingValueC, BreedingValueD) Since the Breeding Values can be used as an estimator for the genetic potential of an individual, we rank the individuals in decreasing order of BVs, taking into account that we want to increase yield.\nThus: B \u0026gt; D \u0026gt; \u0026gt; \u0026gt; \u0026gt; A \u0026gt; C\nHeritability We have processed 20 phenotypes of soybeans over 3 environments and 3 repetitions, finding out that the correlation between the breeding value and the character is 0.25, that the additive genetic variance is 4, and that the environmental variance is 36. Please, calculate heritability and find out if the trait is highly heritable or not. Would you select for oil content at the beginning or end of the breeding process? Variance_e \u0026lt;- 36 Variance_a \u0026lt;- 4 replicates \u0026lt;- 20 environments \u0026lt;- 3 heritability \u0026lt;- Variance_a/((Variance_a)+(Variance_a/replicates*environments)) heritability We can thus infer that the heritability (narrow sense) is around .869; that is, this trait is highlty heritable. As such, we could select for oil content at the beginning of the process, keeping the parents which have higher yields and re-crossing them to maximize it; unlike with a low-heritability trait, there is no problem of trait-loss due to possible changes in environmental conditions\n\\\nLinear Algebra Exercises Create the following vectors: (1,2,3,\u0026hellip;,19,20) seq(from = 1, to = 20, by = 1) (20,19,18,17,16,..,1) rev(1:20) (1,2,3,\u0026hellip;,19,20,19,18,\u0026hellip;,2,1) assign(\u0026#34;vector3\u0026#34;, c(1:19, 20:1) ) vector3 (4,6,3) and assign it to the name tmp. tmp \u0026lt;- c(4,6,3) tmp Look at the help of function rep and make the following vector. (4,6,3,4,6,3,4,6,3) where there are 10 occurences of 4 help(rep) rep(tmp, 10) Use the function ‘paste’ to create the following character vectors of length 10. # (“SNP 1”, “SNP 2”, “SNP 3”,...,“SNP 10”) paste(\u0026#34;SNP\u0026#34;, 1:10) #(“SNP1”, “SNP2”, “SNP3”,...,“SNP10”) paste(\u0026#34;SNP\u0026#34;, 1:10, sep=\u0026#34;\u0026#34;) Look at the function sample and create to random integers with replacement from 1 to 500 with length 100 and store it in a object call xvect and yvect. xvect = sample(1:500, 100, replace=TRUE) yvect = sample(1:500, 100, replace=TRUE) Pick out the values in xvect that are greater than 300. What are the index positions of xvect which are greater than 300?. xvect_300 \u0026lt;- xvect[xvect\u0026gt;300] which_xvect_300 \u0026lt;- which(xvect \u0026gt; 300) What are the values in yvect which correspond to the values in xvect which are \u0026gt; 300 (at the same index postions). yvect_xvect_300 \u0026lt;- yvect[xvect\u0026gt;300] Pick out the elements in xvect at index positions 1,4,7,10,13. xvect[c(1,4,7,10,13)] Sort yvect in decreasing order. Check the sort function in R. yvect_sorted \u0026lt;- sort(yvect, decreasing=TRUE) Sort the numbers in the vector xvect in the order of increasing values in yvect. Look at the help file for order function in r. xvect_sorted \u0026lt;- xvect[order(yvect, decreasing=TRUE)] \\newpage\nExercises with Matrices Enter matrix A in R and then replace the third column of A by the sum of the second and third columns. A = rbind(c(1,1,3), c(5,2,6), c(-2,-1, -3)) A[,3] = A[,2]+A[,3] A Create the B matrix and calculate B^T B using R. Check the crossprod function in R. vector4 = c(10, -10, 10) B = matrix(rep(vector4,15), nrow=15, byrow=T) crossprod = crossprod(t(B)) Create a 6 × 10 matrix of random integers chosen from 1, 2,. . . , 10 by executing the following two lines of code, and learn each of the functions set.seed(75) aMat \u0026lt;- matrix( sample(10, size=60, replace=T), nr=6) aMat a) Find the number of entries in each row which are greater than 4.\n#This gives a table: each column is a row with occurences count rowSums(aMat \u0026gt; 4) b) Which rows contain exactly two occurances of the number 7.\nwhich(rowSums(aMat == 7) == 2) Put the following system of linear equations in matrix notation To put a system in matrix notation, we generate two separate matrixes: the \u0026ldquo;unknowns\u0026rdquo; matrix and the \u0026ldquo;values\u0026rdquo; matrix, which, when multiplied together, gives us back the system of equations. #This code generates the matrixes in R values \u0026lt;- rbind(1:5,c(2,1,2,3,4), c(3,2,1,2,3), c(4,3,2,1,2)) unknowns \u0026lt;- cbind( c(\u0026#34;x1\u0026#34;, \u0026#34;x2\u0026#34;, \u0026#34;x3\u0026#34;, \u0026#34;x4\u0026#34;, \u0026#34;x5\u0026#34;)) equals_to \u0026lt;- cbind(c(7,-1,-3,7)) Since R doesnt let me print matrixes together in a cute way, lets see how this looks using LaTeX $$ \\begin{bmatrix} 1 \u0026amp; 2 \u0026amp; 3 \u0026amp; 4 \u0026amp; 5 \\ 2 \u0026amp; 1 \u0026amp; 2 \u0026amp; 3 \u0026amp; 4 \\ 3 \u0026amp; 2 \u0026amp; 1 \u0026amp; 2 \u0026amp; 3 \\ 4 \u0026amp; 3 \u0026amp; 2 \u0026amp; 1 \u0026amp; 2 \\end{bmatrix} \\cdot \\begin{bmatrix} x_{1}\\ x_{2}\\ x_{3}\\ x_{4}\\ x_{5} \\end{bmatrix} \\begin{bmatrix} 7\\ -1\\ -3\\ 7 \\end{bmatrix}$$\n","date":"October 21, 2021","hero":"/es/posts/master-en-biolog%C3%ADa-computacional/gab/assignment-2/hero.png","permalink":"https://www.pablomarcos.me/es/posts/master-en-biolog%C3%ADa-computacional/gab/assignment-2/","summary":"Fundamental concepts Blending Inheritance What implications from Blending Inheritance were dismissed by Mendel? Before Mendel, people used to think that, given that both parents contribute fluids to their offspring, the descendant\u0026rsquo;s traits would be a mixture of the two individual\u0026rsquo;s characteristcs: a \u0026ldquo;common middle ground\u0026rdquo;, a central point probably influenced by Aristotle\u0026rsquo;s theory of balance.\nThus, blending inheritance proponents assumed that half the variation is removed each generation, and that this variation had to be somehow replenished by mutation.","tags":null,"title":"Assignment 2"},{"categories":null,"contents":" Your task is to use Object-oriented programming to achieve two things:\n\u0026ldquo;Simulate\u0026rdquo; planting 7 grams of seeds from each of the records in the seed stock genebank; then, you should update the genebank information to show the new quantity of seeds that remain after a planting. The new state of the genebank should be printed to a new file, using exactly the same format as the original file seed_stock_data.tsv. If the amount of seed is reduced to zero or less than zero, then a friendly warning message should appear on the screen. The amount of seed left in the gene bank is, of course, not LESS than zero guiño\nProcess the information in cross_data.tsv and determine which genes are genetically-linked. To achieve this, you will have to do a Chi-square test on the F2 cross data. If you discover genes that are linked, this information should be added as a property of each of the genes (they are both linked to each other).\nTo run the script, use: ruby process_database.rb ./SampleData/gene_information.tsv ./SampleData/seed_stock_data.tsv ./SampleData/cross_data.tsv new_stock_file.tsv\n","date":"October 19, 2021","hero":"/es/posts/master-en-biolog%C3%ADa-computacional/bioinfo-programming-challenges/assignment-1/hero.jpg","permalink":"https://www.pablomarcos.me/es/posts/master-en-biolog%C3%ADa-computacional/bioinfo-programming-challenges/assignment-1/","summary":"Your task is to use Object-oriented programming to achieve two things:\n\u0026ldquo;Simulate\u0026rdquo; planting 7 grams of seeds from each of the records in the seed stock genebank; then, you should update the genebank information to show the new quantity of seeds that remain after a planting. The new state of the genebank should be printed to a new file, using exactly the same format as the original file seed_stock_data.tsv. If the amount of seed is reduced to zero or less than zero, then a friendly warning message should appear on the screen.","tags":null,"title":"Creating Objects"},{"categories":null,"contents":" For this assignment, I will study Male Pattern Hair Loss, also called baldness or, more technically, alopecia. Although not technically a “disease” (it can be more accurately defined as a normal phenotipic variation) its high prevalence among caucasian men (~40% of men over 40),1 the age I am approaching and my family history, I believe this topic could be of great interest for me to investigate.\nAnalysis of Molecular Mechanisms Androgenetic alopecia is a hereditary condition, in which scalp hair progressively thins, and eventually stops growing, under the influence of DiHidroTestosterone, an hormone derived from Testosterone.2 Although common myths persist, it is a poligenic disease: family history may influence the risk of suffering this condition, but does not perfecly predict it.3\nThe molecular mechanism underlying this is pretty simple: in susceptible patients (those with the indicated genes), the transformation of free Testosterone into DHT by 5α-reductase, an enzyme involved in steroids metabolism, leads to a progressive thinning of the hairs being produced by the folicules, which is called “miniaturization”, until they stop growing at all.4 Hair on the scalp is genetically predisposed to be more sensitive to DHT, which is why the traditional “pattern”, with hair on the sides remaining, appears.2\nFigure 1. Testosterone is reduced into DHT by 5α-reductase, an enzyme that uses NADPH. CC-By-4.0 by Governa et al. on PeerJ\nTarget selection Finasteride is a drug developed in 1992 to treat Benign Prostate Hiperplasia, a disease similar to MPHL in that it is caused by excessive levels of DHT.5 In 1998, after some studies found that BPH patients presented increased hair growth and an stop in hair loss when taking this drug, and given its strong safety profile, the FDA approved it to treat MPHL too.6\nFinasteride works by blocking 5αR, and, more specifically, isoform II of the enzyme, which is expressed mostly in the prostate and in scalp skin7 (type I is expressed mostly in the liver and the rest of the skin).8 It works through competitive inhibition, binding to 5αR to produce DiHidroFinasteride instead of DHT. Since it eventually dissociates from the Enzyme-Substrate complex, it is not considered a suicide inhibitor.9 It has a short half-life of 5-7 hours, and, although more effective drugs with comparative side effects (such as Dutasteride, which is more effective at blocking 5αR) exist, they have not yet been approved to target MPHL (only BPH).10\nTo sum up: the desired target, the protein whose activity we wish to block, is 5α-Reductase\nHypothesis for Drug Design We ideally would like to find a drug that selectively inhibits isoform II of 5αR, while keeping isoform I as untouched as posiible, since DHT remains an important and potent androgen, and its supression can lead to side effects such as decreased libido and sperm levels, erectile dysfunction and even depression.11\nImportant to know is that 5α-Reductase Inhibitors (5ARIs) have been found to be teratogenic (that is, they cause developmental malformations) so, whatever drug we design, should never be used in minors or in pregnant women.12\nLigand based-virtual screening The results can be seen in the accompanying Jupyter Notebook, but they can be summarized as follows:\nAfter searching for typeII 5α-Reductase typeII on CHEMBL, we get drugs which interact with it and order them in increasing order or IC50 (that is, decreasing order of inhibitory power) The best drug is CHEMBL296415, with IC50 of 0.00086 ug/mL; however, we cannot be sure if it targets typeI 5aR (which will be undesirable) as well as typeI; so, we change the SwissSimilarity target to Finasteride. To see if we can improve FNS’s IC50 value (16 nM) we use SwissSimilarity to find similarly shaped molecules, using methods such as MACCS and Morgan Keys and Dice and Tanimoto indexes Out of the 400 “similar” molecules provided by Swiss, we pick those with Dice (MACCS) \u0026gt; 0.625, so as to get more than 150 molecules as commissioned, but not too much (it is to be noted that SwissSimilarity scores were really bad, but this is to be expected due to method selection) With this, 172 molecules are selected, and exported to SimilarityAnalysis.csv In Assignments 2 and 3, we will describe how to perform docking analysis on such molecules, to see whether one has a higher affinity that the original molecule (Finasteride) For more details, please refer to the full notebook.\nReferences Header Image by Andrea Piacquadio at Pexels\nA. Goren et al., «A preliminary observation: Male pattern hair loss among hospitalized COVID-19 patients in Spain – A potential clue to the role of androgens in COVID-19 severity», J. Cosmet. Dermatol., vol. 19, n.o 7, pp. 1545-1547, 2020, doi: 10.1111/jocd.13443.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nV. A. Randall, «Molecular Basis of Androgenetic Alopecia», en Aging Hair, R. M. Trüeb y D. J. Tobin, Eds. Berlin, Heidelberg: Springer, 2010, pp. 9-24. doi: 10.1007/978-3-642-02636-2_2.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nW. C. Chumlea et al., «Family history and risk of hair loss», Dermatol. Basel Switz., vol. 209, n.o 1, pp. 33-39, 2004, doi: 10.1159/000078584.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n«baldness | dermatology | Britannica». https://www.britannica.com/science/baldness (accedido nov. 13, 2021).\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nHealth Products Regulatory Authority of England, «Public Assessment Report for a Medicinal Product for Human Use - Finasteride 1mg film-coated tablets». Accedido: nov. 13, 2021. [^En línea]. Disponible en: https://www.hpra.ie/img/uploaded/swedocuments/Public_AR_PA22753-002-001_20102020172946.pdf\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nP. M. Zito, K. G. Bistas, y K. Syed, «Finasteride», en StatPearls, Treasure Island (FL): StatPearls Publishing, 2021. Accedido: nov. 13, 2021. [^En línea]. Disponible en: http://www.ncbi.nlm.nih.gov/books/NBK513329/\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nF. Azzouni, A. Godoy, Y. Li, y J. Mohler, «The 5 Alpha-Reductase Isozyme Family: A Review of Basic Biology and Their Role in Human Diseases», Adv. Urol., vol. 2012, p. 530121, 2012, doi: 10.1155/2012/530121.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nW. D. Steers, «5alpha-reductase activity in the prostate», Urology, vol. 58, n.o 6 Suppl 1, pp. 17-24; discussion 24, dic. 2001, doi: 10.1016/s0090-4295(01)01299-7.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nS. L. Hulin-Curtis, D. Petit, W. D. Figg, A. W. Hsing, y J. K. Reichardt, «Finasteride metabolism and pharmacogenetics: new approaches to personalized prevention of prostate cancer», Future Oncol. Lond. Engl., vol. 6, n.o 12, pp. 1897-1913, dic. 2010, doi: 10.2217/fon.10.149.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n«FICHA TECNICA DUTASTERIDA CINFA 0,5 MG CAPSULAS BLANDAS EFG». https://cima.aemps.es/cima/dochtml/ft/80595/FT_80595.html (accedido nov. 13, 2021).\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n«Side Effects of 5-Alpha Reductase Inhibitors: A Comprehensive Review - PubMed». https://pubmed.ncbi.nlm.nih.gov/27784557/ (accedido nov. 12, 2021).\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n«PROSPECTO FINASTERIDA SANDOZ 1 mg COMPRIMIDOS RECUBIERTOS CON PELICULA EFG». https://cima.aemps.es/cima/dochtml/p/70567/P_70567.html (accedido nov. 12, 2021).\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","date":"October 15, 2021","hero":"/es/posts/master-en-biolog%C3%ADa-computacional/lead-discovery/assignment-1/hero.png","permalink":"https://www.pablomarcos.me/es/posts/master-en-biolog%C3%ADa-computacional/lead-discovery/assignment-1/","summary":"For this assignment, I will study Male Pattern Hair Loss, also called baldness or, more technically, alopecia. Although not technically a “disease” (it can be more accurately defined as a normal phenotipic variation) its high prevalence among caucasian men (~40% of men over 40),1 the age I am approaching and my family history, I believe this topic could be of great interest for me to investigate.\nAnalysis of Molecular Mechanisms Androgenetic alopecia is a hereditary condition, in which scalp hair progressively thins, and eventually stops growing, under the influence of DiHidroTestosterone, an hormone derived from Testosterone.","tags":null,"title":"Designing a pharmacophore and ensuring ADMET properties"},{"categories":null,"contents":" El ejercicio que se presenta a continuación se refiere al análisis de una de las ecuaciones presentadas en las diapositivas de la Lección 1, utilizando los métodos hasta ahora aprendidos y toda la información adicional que sea necesaria. La función elegida es el mapa del cubo:\n$$x_{n+1} = r \\cdot x_{n}^2 \\cdot (1 - x_{n})$$ En primer lugar, debemos identificar las regiones de parámetros que producen una dinámica significativa. Suponiendo que r \u0026gt; 0, la mayor descendencia posible se produce cuando la función se maximiza; es decir, cuando su derivada = 0 así:\nimport sympy as sp #Defino las variables matemáticas en uso x, r= sp.symbols(\u0026#39; x, r \u0026#39;) #Y la función sobre la que trabajo f = r*x**2*(1-x) f r*x**2*(1 - x) #Calculo su derivada derivada = sp.simplify(sp.diff(f, x)) derivada r*x*(2 - 3*x) #Y resuelvo para derivada = 0 sp.solve(derivada, x) [0, 2/3] maxval = f.subs({\u0026#34;x\u0026#34;:2/3}) maxval # This equals 4/27, trust me 0.148148148148148*r Podemos concluir que el valor máximo de descendencia se producirá cuando x = $\\frac{2}{3}$. Así, vemos que el valor máximo de $ x_{n+1} $ es $r \\cdot \\frac{4}{27}$ (al sustituir el valor en la función inicial. Es decir, que los valores interesantes de r estarán entre 0 y 27/4, es decir, 6.75\nA continuación, pasamos a calcular los puntos fijos. Estos son aquellos en los que f(x) = x, por lo que, dado el mapa del cubo, tendríamos:\n#Y resuelvo para f(x) = x criticos = sp.solve(f-x, x) criticos [0, 1/2 - sqrt(r*(r - 4))/(2*r), 1/2 + sqrt(r*(r - 4))/(2*r)] Es decir: los puntos estables son 0 (la solución trivial) y:\nsp.simplify(criticos[1]) (r - sqrt(r*(r - 4)))/(2*r) sp.simplify(criticos[2]) (r + sqrt(r*(r - 4)))/(2*r) Ahora, ¿son estos puntos estables? Es decir, ante pequeñas perturbaciones, ¿la función vuelve sola al punto fijo, o se escapa? Para saberlo, debemos recurrir a la condición de estabilidad general: $|f\u0026rsquo;(x^{*})| \u0026lt; 1$; esto es así porque, para tener estabilidad, la pendiente de la función en el punto de equilibrio debe estar contenida entre -1 y 1, las pendientes de las rectas a 45º y -45º, respectivamente. En el caso que nos ocupa:\n#Calculamos la expresión que nos dirá para que valores de r el punto es estable, y para cuales no condicion_estable_pt1 = sp.simplify(derivada.subs({\u0026#34;x\u0026#34;:criticos[1]})) condicion_estable_pt1 #Que tiene que estar entre -1 y 1 para ser estable -r/2 + sqrt(r*(r - 4))/2 + 3 sp.solve(sp.Abs(condicion_estable_pt1) \u0026lt; 1) #Fun fact: si no lo llevo simplificado no me lo resuelve False #Repetimos para x2 condicion_estable_pt2 = sp.simplify(derivada.subs({\u0026#34;x\u0026#34;:criticos[2]})) condicion_estable_pt2 -r/2 - sqrt(r*(r - 4))/2 + 3 sp.solve(sp.Abs(condicion_estable_pt2) \u0026lt; 1) (4 \u0026lt; r) \u0026amp; (r \u0026lt; 16/3) Ante los resultados anteriores, podemos conjeturar que el primer punto fijo no trivial, llamemosle $x_{1}$, es inestable: no valor de r tal que la condición de estabilidad se verifique. Respecto al segundo punto no trivial, llamemosle $x_{2}$, vemos que, efectivamente, SI que hay una serie de valores para los cuales es estable: aquellos en los que r sea mayor de 4 pero menor de 16/3, esto es, 5.33\nAhora que ya conocemos la función más en profundidad, y sabemos cuales son los valores de r que resultan de interés, procedemos a dibujar los diagramas de tela de araña para dichos valores de interés, comprobando así si se verifican nuestras conclusiones: el punto $x_1$ debería ser siempre inestable, mientras que, para valores seleccionados, debería ser posible observar cómo $x_2$ atrae hacia sí las trayectorias poblacionales.\n#Here is the code I used; please,see sources document import numpy as np import matplotlib.pyplot as plt dpi = 72 def plot_cobweb(f, r, x0, nmax): x = np.linspace(0, 1, 500) # Plot y = f(x) and y = x plt.plot(x, f(x, r), c=\u0026#39;#444444\u0026#39;, lw=2) plt.plot(x, x, c=\u0026#39;#444444\u0026#39;, lw=2) # Iterate x = f(x) for nmax steps, starting at (x0, 0). px, py = np.empty((2,nmax+1,2)) px[0], py[0] = x0, 0 for n in range(1, nmax, 2): px[n] = px[n-1] py[n] = f(px[n-1], r) px[n+1] = py[n] py[n+1] = py[n] # Plot the path traced out by the iteration. plt.plot(px, py, c=\u0026#39;b\u0026#39;, alpha=0.7) # Annotate and tidy the plot. plt.xlabel(\u0026#39;$x$\u0026#39;) plt.ylabel(f.latex_label) plt.title(\u0026#39;$x_0 = {:.1}, r = {:.2}$\u0026#39;.format(x0, r)) class AnnotatedFunction: def __init__(self, func, latex_label): self.func = func self.latex_label = latex_label def __call__(self, *args, **kwargs): return self.func(*args, **kwargs) Primero, probemos para valores menores de 4; a juzgar por nuestros cálculos, no debería haber mas atractor que $x_0$: la extinción\nfunc = AnnotatedFunction(lambda x,r: r*x**2*(1-x), r\u0026#39;$rx(1-x)$\u0026#39;) plt.subplots(figsize=(15, 5)) plt.subplot(1, 2, 1) plot_cobweb(func, 3.5, 0.3, 2000) plt.subplot(1, 2, 2) plot_cobweb(func, 3.5, 0.1, 2000) plt.show() Como podemos ver (aunque son meros ejemplos, podríamos probar más), da igual el valor $x_0$ que elija: ante una r \u0026lt; 4, no hay puntos de estabilidad.\nplt.subplots(figsize=(15, 5)) plt.subplot(1, 2, 1) plot_cobweb(func, 6.6, 0.3, 2000) plt.subplot(1, 2, 2) plot_cobweb(func, 6.6, 0.1, 2000) plt.show() Para valores mayores a 5.33 (pero intentando dejarlo dentro del valor de referencia r \u0026lt; 6.75 para mantener la utilidad biológica de estas aproximaciones) vemos que sucede lo mismo: aunque ahora sí que aparecen puntos fijos (es decir, que, a diferencia de en las gráficas anteriores, *sí que existen puntos tales que f(x) = x, pues la recta y el mapa se intersectan), estos siguen sin ser estables: la población se extingue inevitablemente\nplt.subplots(figsize=(15, 5)) plt.subplot(1, 2, 1) plot_cobweb(func, 5.0, 0.3, 2000) plt.subplot(1, 2, 2) plot_cobweb(func, 5.0, 0.1, 2000) plt.show() Sin embargo, en los valores seleccionados, ¡eureka! encontramos un equilibrio válido para mantener a nuestra población sana y salva. Sin embargo, vemos que aún con estas, no todos los valores de población inicial conllevan estabilidad, sino que, para los dos puntos de equilibrio estables (la solución trivial y el llamado $x_2$), existe un rango de valores, o cuenca de atracción, bajo los cuales, las trayectorias que se inicien en dicha cantidad de población, evolucionarán a uno u otro punto de equilibrio.\nEste tipo de sistemas, enlos que existen dos puntos de equilibrio y la manera de comportarse de los mimos depende de los valores de población incial, se denominan biestables, y el cálculo de las cuencas de atracción de cada punto será esencial para entender la evolución poblacional de las especies que sigan este régimen.\nA continuación, se nos pide examinar el contenido de la función para dinámicas de segundo periodo; es decir, queremos encontrar puntos que sean estables cada dos generaciones. Así, lo primero sería calcular la función de segundo periodo: F(F(x))\nf_2 = f.subs({\u0026#34;x\u0026#34;:f}) f_2 r**3*x**4*(1 - x)**2*(-r*x**2*(1 - x) + 1) #Y resuelvo para f(x) = x criticos_2 = sp.solve(f_2-x, x) criticos_2 [0, (r - sqrt(r**2 - 4*r))/(2*r), (r + sqrt(r**2 - 4*r))/(2*r)] #Ahora, para los puntos fijos, calculo su derivada derivada_2 = sp.simplify(sp.diff(f_2, x)) #Y sustituyo en ella el punto fijo, obteniendo una expresión condicion_estable_2_pt1 = sp.simplify(derivada_2.subs({\u0026#34;x\u0026#34;:criticos_2[1]})) condicion_estable_2_pt1 #Que tiene que estar entre -1 y 1 para ser estable r**2/2 - r*sqrt(r*(r - 4))/2 - 4*r + 3*sqrt(r*(r - 4)) + 9 #Y repito para el punto 2 condicion_estable_2_pt2 = sp.simplify(derivada_2.subs({\u0026#34;x\u0026#34;:criticos_2[2]})) condicion_estable_2_pt2 #Que tiene que estar entre -1 y 1 para ser estable r**2/2 + r*sqrt(r*(r - 4))/2 - 4*r - 3*sqrt(r*(r - 4)) + 9 sp.solve(sp.Abs(condicion_estable_2_pt1) \u0026lt; 1) False sp.solve(sp.Abs(condicion_estable_2_pt2) \u0026lt; 1) (4 \u0026lt; r) \u0026amp; (r \u0026lt; 16/3) Curiosamente, obtengo los mismos valores para el primer periodo que para el segundo; es decir, que cualquier punto que sea estable de generación en generación lo será también cada dos generaciones, aparentemente. Vamos a ver cómo se vería esto en una gráfica:\nfunc = AnnotatedFunction(lambda x,r: r**3*x**4*(x-1)**2*(r*x**2*(x-1)+1), r\u0026#39;$rx(1-x)$\u0026#39;) plt.subplots(figsize=(15, 5)) plt.subplot(1, 2, 1) plot_cobweb(func, 5.0, 0.3, 2000) plt.subplot(1, 2, 2) plot_cobweb(func, 5.0, 0.1, 2000) plt.show() Como podemos ver, usando los mismos números que antes, efectivamente, el mismo punto es un atractor.\nFinalmente, queremos sintetizar, en un sólo gráfico, el funcionamiento de la función objeto de estudio: esta es la gráfica de bifurcación, aunque también nos podría servir la gráfica de los exponentes de Lyapunov, que sin embargo es mucho más difícil de realizar en código de manera sencilla, con lo que nos quedaremos con la primera:\nR = np.linspace(2.5,7,100000) #Aquí pongo los valores que quiera calcular de r X = [] #Eje x: los valores de control del parámetro r Y = [] #Eje y: valores del mapa del cubo for r_val in R: X.append(r_val) x_val = np.random.random() #inicializo x para cada valor de r for n in range(100): x_val = r_val*x_val**2*(1-x_val) Y.append(x_val) plt.plot(X, Y, ls=\u0026#34;\u0026#34;, marker=\u0026#34;,\u0026#34;) plt.show() /tmp/ipykernel_16522/2430161262.py:10: RuntimeWarning: overflow encountered in double_scalars x_val = r_val*x_val**2*(1-x_val) Como podemos ver, en esta gráfica relacionamos una serie de valores de r con sus correspondientes valores en el mapa del cubo; hasta ciertos valores, la población incial es demasiado baja, y es atraida al punto estable $x_{0} = 0$. Una vez llegamos a un cierto valor, la atracción cambia, y ahora los valores tienden al segundo punto fijo, que podemos llamar $x_{1}$. Una vez sobrepasamos el valor calculado anteriormente con symbolab, la condición de estabilidad del segundo punto ya no se cumple, por lo que este punto se vuelve inestable y la función se bifurca: aparecerán nuevos puntos estables de segundo orden, en torno a r = 6; según estos van aumentando, el proceso de desestabilización se repite, y nuevos puntos de otros órdenes aparecen; sin embargo, como vimos al comienzo del todo, a partir de r = 6.75 los valores dejan de tener relevancia biológica, por lo que, lo que es al caso de nuestro estudio actual, no nos resultaría relevante.\nSources:\nCreating a Bifurcation Diagram in Python en Stack Overflow\nSistemas No Lineales, de Rafael Ramírez Plotting the bifurcation diagram of a chaotic dynamical system en GitHub Bifurcation diagram of Logistic map || Python en Youtube\nThe Lyapunov exponent with Python - Graz University Post header by Glosser.ca at Wikimedia Commons - Own work, CC BY-SA 4.0.\n","date":"October 14, 2021","hero":"/es/posts/master-en-biolog%C3%ADa-computacional/modelizaci%C3%B3n-de-sistemas-biol%C3%B3gicos/logistic-map/hero.png","permalink":"https://www.pablomarcos.me/es/posts/master-en-biolog%C3%ADa-computacional/modelizaci%C3%B3n-de-sistemas-biol%C3%B3gicos/logistic-map/","summary":"El ejercicio que se presenta a continuación se refiere al análisis de una de las ecuaciones presentadas en las diapositivas de la Lección 1, utilizando los métodos hasta ahora aprendidos y toda la información adicional que sea necesaria. La función elegida es el mapa del cubo:\n$$x_{n+1} = r \\cdot x_{n}^2 \\cdot (1 - x_{n})$$ En primer lugar, debemos identificar las regiones de parámetros que producen una dinámica significativa. Suponiendo que r \u0026gt; 0, la mayor descendencia posible se produce cuando la función se maximiza; es decir, cuando su derivada = 0 así:","tags":null,"title":"El mapa Logístico"},{"categories":null,"contents":"Nothing ruins the thrill of buying a brand new car more quickly than seeing your new insurance bill. The sting’s even more painful when you know you’re a good driver. It doesn’t seem fair that you have to pay so much if you’ve been cautious on the road for years.\nInaccuracies in car insurance company’s claim predictions raise the cost of insurance for good drivers and reduce the price for bad ones. But Kaymo ™ wants to stop that!\nIn this competition, you’re challenged to build a model that predicts the probability that a driver will initiate an auto insurance claim. While Kaymo ™ has used machine learning for the past 10 years, they’re looking to the CAIIS machine learning community to explore new, more powerful methods. A more accurate prediction will allow them to further tailor their prices, and hopefully make auto insurance coverage more accessible to more drivers.\nInitial data Analysis First, we need to create the Spark Session\n#In collab, we need to install everything: !apt-get install openjdk-8-jdk-headless -qq \u0026gt; /dev/null !wget -q https://mirrors.sonic.net/apache/spark/spark-3.1.2/spark-3.1.2-bin-hadoop3.2.tgz !tar xzf spark-3.1.2-bin-hadoop3.2.tgz !pip install -q findspark import os os.environ[\u0026#34;JAVA_HOME\u0026#34;] = \u0026#34;/usr/lib/jvm/java-8-openjdk-amd64\u0026#34; os.environ[\u0026#34;SPARK_HOME\u0026#34;] = \u0026#34;/content/spark-3.1.2-bin-hadoop3.2\u0026#34; import findspark findspark.init() from pyspark.sql import SparkSession spark = SparkSession.builder.master(\u0026#34;local[*]\u0026#34;).getOrCreate() #In a native Jupyter notebook, we would simply do: #from pyspark.sql import SparkSession #spark = SparkSession.builder.appName(\u0026#39;seedfinder\u0026#39;).getOrCreate() Afterwards, we can read the file and inspect it\n#Please drop the file in the environments \u0026#39;Files\u0026#39; panel df = spark.read.options(header=\u0026#34;true\u0026#34;, inferSchema=\u0026#34;true\u0026#34;).csv(\u0026#34;/content/train.csv\u0026#34;) df.describe().toPandas() summary id cat_0 cat_1 cat_2 cat_3 cat_4 cat_5 cat_6 cat_7 cat_8 cat_9 cat_10 cat_11 cat_12 cat_13 cat_14 cat_15 cat_16 cat_17 cat_18 cont_0 cont_1 cont_2 cont_3 cont_4 cont_5 cont_6 cont_7 cont_8 cont_9 cont_10 target 0 count 4066 4066 4066 4066 4066 4066 4066 4066 4066 4066 4066 4066 4066 4066 4066 4066 4066 4066 4066 4066 4066 4066 4066 4066 4066 4066 4065 4065 4065 4065 4065 4065 1 mean 3387.4498278406295 None None None None None None None None None None None None None None None None None None None 0.5051998947729144 0.49388336368386704 0.5189480562752655 0.4735777650650935 0.5086566082134892 0.5060181089955016 0.49226472593286386 0.5006876416744185 0.49040640920734324 0.4745391464905265 0.5064674250670175 0.18081180811808117 2 stddev 1935.5367394936904 None None None None None None None None None None None None None None None None None None None 0.20419268191081688 0.2109889750767605 0.2140486431630643 0.21460470991003147 0.22718105831759158 0.2395814912115326 0.21021050608421313 0.20111985652217018 0.18048630841199265 0.1951590751556047 0.20200775006813254 0.384909527996416 3 min 0 A A A A A A A A A A AA A A A A A A A A -0.005136936996862684 0.10745448158938073 0.11502362274662108 0.006599651524877015 0.17695526722535196 -0.006380447747513784 0.021270552792613227 0.1223397138826129 0.0889069927983469 0.22995944353840855 0.1278528100161272 0 4 max 6757 B O U N T ZZ Y Y Y X Y B B B B D D D D 0.9953078039956524 0.9896946695363368 0.9917094757670126 0.92705434460466 0.8511866390241221 0.8423372084057961 0.957624056103622 1.0044606397161469 1.0414219123188158 0.9895323265306966 0.9925120037993798 1 As we can see, we have a database with 8 categotical variables, 11 continuous variables and a target value, which, in this training data, is provided so that we can train the algorithm. In this database, each row represents a given insurance policy, and target represents whether the policy resulted in a claim (1) or not (0). We want to predict whether a given insurance policy lead to claims, to remove that policies from the market and minimize the chance for costly, non-desirable claims.\nBefore proceeding with any approach, we first have to index the categorical variables! This is because pyspark and machine learning algorithms in general dont work well with strings: they need a mathematical value to work with. This has its own problems, specially since the numbers are not random and are orderable, but its the best we have.\nfrom pyspark.ml import Pipeline from pyspark.ml.feature import StringIndexer #Bonus! Change this code to index multiple columns at once! #And! With a list comprehension I can specify all the cat_ things from 1 to 18 :p indexers = [StringIndexer(inputCol=column, outputCol=column+\u0026#34;_index\u0026#34;).fit(df) for column in [f\u0026#39;cat_{x}\u0026#39; for x in range(19)] ] pipeline = Pipeline(stages=indexers) df_indexed = pipeline.fit(df).transform(df) df_indexed.describe().toPandas() summary id cat_0 cat_1 cat_2 cat_3 cat_4 cat_5 cat_6 cat_7 cat_8 cat_9 cat_10 cat_11 cat_12 cat_13 cat_14 cat_15 cat_16 cat_17 cat_18 cont_0 cont_1 cont_2 cont_3 cont_4 cont_5 cont_6 cont_7 cont_8 cont_9 cont_10 target cat_0_index cat_1_index cat_2_index cat_3_index cat_4_index cat_5_index cat_6_index cat_7_index cat_8_index cat_9_index cat_10_index cat_11_index cat_12_index cat_13_index cat_14_index cat_15_index cat_16_index cat_17_index cat_18_index 0 count 4066 4066 4066 4066 4066 4066 4066 4066 4066 4066 4066 4066 4066 4066 4066 4066 4066 4066 4066 4066 4066 4066 4066 4066 4066 4066 4065 4065 4065 4065 4065 4065 4066 4066 4066 4066 4066 4066 4066 4066 4066 4066 4066 4066 4066 4066 4066 4066 4066 4066 4066 1 mean 3387.4498278406295 None None None None None None None None None None None None None None None None None None None 0.5051998947729144 0.49388336368386704 0.5189480562752655 0.4735777650650935 0.5086566082134892 0.5060181089955016 0.49226472593286386 0.5006876416744185 0.49040640920734324 0.4745391464905265 0.5064674250670175 0.18081180811808117 0.24913920314805707 2.9648303000491882 1.5393507132316773 0.6000983767830792 1.3145597638957207 0.7845548450565667 0.7137235612395475 8.324643384161337 8.775454992621741 0.9473684210526315 19.2073290703394 0.1352680767338908 0.14240039350713232 0.02336448598130841 0.46532218396458436 0.34948352188883425 0.3462862764387605 0.2825873093949828 0.2147073290703394 2 stddev 1935.5367394936904 None None None None None None None None None None None None None None None None None None None 0.20419268191081688 0.2109889750767605 0.2140486431630643 0.21460470991003147 0.22718105831759158 0.2395814912115326 0.21021050608421313 0.20111985652217018 0.18048630841199265 0.1951590751556047 0.20200775006813254 0.384909527996416 0.4325677750396001 3.2452054039576987 2.4425605042547827 1.1474084345954927 1.7807690795360278 4.2299020498829245 1.380123728813704 9.875877537204804 9.154892230985055 1.8179599316104342 30.361885464393367 0.342051749318574 0.3495033102516255 0.15107680233781368 0.4988573482064128 0.565594945313189 0.5695057140802395 0.6398622202372004 0.5579704263968471 3 min 0 A A A A A A A A A A AA A A A A A A A A -0.005136936996862684 0.10745448158938073 0.11502362274662108 0.006599651524877015 0.17695526722535196 -0.006380447747513784 0.021270552792613227 0.1223397138826129 0.0889069927983469 0.22995944353840855 0.1278528100161272 0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 4 max 6757 B O U N T ZZ Y Y Y X Y B B B B D D D D 0.9953078039956524 0.9896946695363368 0.9917094757670126 0.92705434460466 0.8511866390241221 0.8423372084057961 0.957624056103622 1.0044606397161469 1.0414219123188158 0.9895323265306966 0.9925120037993798 1 1.0 14.0 16.0 12.0 18.0 56.0 14.0 50.0 53.0 16.0 184.0 1.0 1.0 1.0 1.0 3.0 3.0 3.0 3.0 We now remove all the non-indexed (useless and repeated) columns from the original dataframe:\ndf_indexed = df_indexed.select([c for c in df_indexed.columns if c not in [f\u0026#39;cat_{x}\u0026#39; for x in range(19)]]) #Uses the same list comprehension as before! #Note that I am keeping the ID. Since it is a unique identifyer, it would do nothing to help with prediction, but it doesnt harm either Now, we have the indexed dataframe! We can proceed to do some exploratory analysis, to see what is the correlation between some of this parameters and the column of interest (target). For this, I\u0026rsquo;d usually plot the scatter matrix using pandas, but I have tried here and I have found that there are so many datapoints that the graph that appears is useless. Thus, I will plot a heatmap which tells me which variables are correlated to which\n#This is useless! Too much data! #import pandas as pd #pd.plotting.scatter_matrix(df.toPandas(), figsize=(10, 10)); import seaborn as sns import matplotlib.pyplot as plt plt.figure(figsize=(10,9)) #Make the plot easier on the eyes sns.heatmap(df_indexed.toPandas().corr()) \u0026lt;matplotlib.axes._subplots.AxesSubplot at 0x7f2b4ee3e050\u0026gt; As we can see, there is no clear correlation between any of the variables and the target parameter! This seems like a clear cut case for big data analysis, where we could try and see if each small correlation from each characteristic gets to add up and give us a good prediction after all!\nSupervised approach One of the ways we can try and predict the outcome is through supervised learning. Here, we take a train data set, show the computer how to predict an outcome from this data, and then give it some data to make some predictions on it.\nHere, we are using three different methods, which come bundled with spark:\nDecission Tree Classifier: It uses a decision tree (as a predictive model) to go from observations about an item (represented in the branches) to conclusions about the item\u0026rsquo;s target value (represented in the leaves). Random Forest Classifier: For classification tasks, the output of the random forest is the class selected by most trees. For regression tasks, the mean or average prediction of the individual trees is returned. Random decision forests correct for decision trees\u0026rsquo; habit of overfitting to their training set. Gradient Boosted Trees: It gives a prediction model in the form of an ensemble of weak prediction models, which are typically decision trees. When a decision tree is the weak learner, the resulting algorithm is called gradient-boosted trees; it usually outperforms random forest. We will use the three methods to find if their results match, and to pick the most accurate of the three.\nFirst, we must first assemble a Vector with a \u0026ldquo;features\u0026rdquo; and a \u0026ldquo;label\u0026rdquo; tag so that it can be processed by Spark.\nfrom pyspark.ml.feature import VectorAssembler, Imputer assembler = VectorAssembler(inputCols= [e for e in df_indexed.columns if e not in (\u0026#39;target\u0026#39;)] , outputCol=\u0026#39;features\u0026#39;, handleInvalid=\u0026#39;skip\u0026#39;) output = assembler.transform(df_indexed) imputer = Imputer(inputCols=[\u0026#39;target\u0026#39;], outputCols=[\u0026#39;label\u0026#39;], strategy=\u0026#39;mean\u0026#39;) imputer_model = imputer.fit(output) output = imputer_model.transform(output) Now that we have this set up, we can proceed to create the models:\n#Fist, we alias the methods to make them easier to call from pyspark.ml.classification import (RandomForestClassifier, GBTClassifier, DecisionTreeClassifier) dtc = DecisionTreeClassifier(maxBins=185) #Decision Trees require maxBins to be at least as large as the number of values in each categorical feature rfc = RandomForestClassifier(numTrees = 100, maxBins=185) gbt = GBTClassifier(maxBins=185) #We fit the three models dtc_model = dtc.fit(output) rfc_model = rfc.fit(output) gbt_model = gbt.fit(output) Now, we would like to get this model\u0026rsquo;s predictions for our test data. First, we need to read the data:\n#Please drop the file in the environments \u0026#39;Files\u0026#39; panel test_df = spark.read.options(header=\u0026#34;true\u0026#34;, inferSchema=\u0026#34;true\u0026#34;).csv(\u0026#34;/content/test.csv\u0026#34;) test_df.describe().toPandas() summary id cat_0 cat_1 cat_2 cat_3 cat_4 cat_5 cat_6 cat_7 cat_8 cat_9 cat_10 cat_11 cat_12 cat_13 cat_14 cat_15 cat_16 cat_17 cat_18 cont_0 cont_1 cont_2 cont_3 cont_4 cont_5 cont_6 cont_7 cont_8 cont_9 cont_10 0 count 36737 36737 36737 36737 36737 36737 36737 36737 36737 36737 36737 36737 36737 36737 36737 36737 36737 36737 36737 36737 36737 36737 36737 36737 36737 36736 36736 36736 36736 36736 36736 1 mean 45565.33519340175 None None None None None None None None None None None None None None None None None None None 0.5063036234343945 0.49467192032599405 0.5173822226393028 0.4762916314739485 0.5033223270949208 0.5020626194199714 0.48730612112892824 0.5028144336472354 0.48923176796866097 0.4712561312748483 0.5094019115515299 2 stddev 26266.780527766103 None None None None None None None None None None None None None None None None None None None 0.20801235880562077 0.21322934164723154 0.2147802721260591 0.21726160996894317 0.22729116824926243 0.24097697602925358 0.2117830227977653 0.20422624901247177 0.1791202966152642 0.19567532832917978 0.20527800336854235 3 min 3 A A A A A A A A A A AA A A A A A A A A -0.04726740437268828 0.09905764069525842 0.10521322746692256 -0.038177828728809315 0.17373018292252934 -0.03583375784982512 0.02298451094980952 0.09708227671225958 0.0280130873460242 0.22021805352799687 0.15451327463463016 4 max 91243 B O U N T ZZ Y Y Y X Y B B B B D D D D 0.9945586232209024 1.001207364106895 1.0105190260692984 0.9440372658550216 0.8541243700209937 0.8455319358435773 0.9646352575880386 1.0274581708548551 1.0434670315747987 0.9992554391006276 1.0066065913617064 We have to process it in the same way we processed the test data for it to work!\nindexers = [StringIndexer(inputCol=column, outputCol=column+\u0026#34;_index\u0026#34;).fit(test_df) for column in [f\u0026#39;cat_{x}\u0026#39; for x in range(19)] ] pipeline = Pipeline(stages=indexers) test_df_indexed = pipeline.fit(test_df).transform(test_df) test_df_indexed.describe().toPandas() test_df_indexed = df_indexed.select([c for c in df_indexed.columns if c not in [f\u0026#39;cat_{x}\u0026#39; for x in range(19)]]) assembler = VectorAssembler(inputCols= [e for e in test_df_indexed.columns if e not in (\u0026#39;target\u0026#39;)] , outputCol=\u0026#39;features\u0026#39;, handleInvalid=\u0026#39;skip\u0026#39;) test_output = assembler.transform(test_df_indexed) imputer = Imputer(inputCols=[\u0026#39;target\u0026#39;], outputCols=[\u0026#39;label\u0026#39;], strategy=\u0026#39;mean\u0026#39;) imputer_model = imputer.fit(test_output) test_output = imputer_model.transform(test_output) And now, we can predict the results!\n#And get their predictions dtc_preds = dtc_model.transform(test_output) rfc_preds = rfc_model.transform(test_output) gbt_preds = gbt_model.transform(test_output) To see how this all went, we can use MulticlassClassificationEvaluator, which will give us an accuracy metric\nfrom pyspark.ml.evaluation import MulticlassClassificationEvaluator #We alias the method evaluator = MulticlassClassificationEvaluator(metricName=\u0026#39;accuracy\u0026#39;) And we display the results!\nprint(f\u0026#39;DTC: {evaluator.evaluate(dtc_preds)} \\n Features Importance: {dtc_model.featureImportances}\\n\\n\u0026#39;) print(f\u0026#39;RFC: {evaluator.evaluate(rfc_preds)} \\n Features Importance: {rfc_model.featureImportances}\\n\\n\u0026#39;) print(f\u0026#39;GBT: {evaluator.evaluate(gbt_preds)} \\n Features Importance: {gbt_model.featureImportances}\\n\\n\u0026#39;) DTC: 0.8346863468634687 Features Importance: (31,[2,3,4,6,7,12,13,15,19,20,22,28],[0.014701613420158115,0.043912393310141254,0.01632263297559012,0.023272955254936112,0.008767295495164232,0.07339838005021362,0.026900352099027025,0.0010919366626692384,0.11149836670131266,0.05104627569585904,0.31490514161563943,0.31418265671928924]) RFC: 0.8191881918819188 Features Importance: (31,[0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30],[0.007222725777778714,0.013272624708238185,0.06572063427606169,0.07383599205924346,0.016249034815029796,0.015671208937598453,0.013942792952495857,0.012265185023691075,0.022093613183070106,0.03017036047394847,0.02111126567214282,0.007573440666722141,0.03675523871622703,0.05340791474285819,0.018744859020368907,0.01438979707997024,0.014423643530970557,0.006122722273276428,0.0032687974851870532,0.040250295497554885,0.05959542094667576,0.010453230392129118,0.16357534034178262,0.002108188884061558,0.0018874200854626776,0.00033413253805777044,0.004749384890559298,0.08373181729146607,0.1565634402260749,0.005186828755947708,0.02532264875534833]) GBT: 0.9035670356703567 Features Importance: (31,[0,1,2,3,4,5,6,7,9,10,11,12,13,14,15,16,17,18,19,20,21,22,26,27,28,29,30],[0.019554090543858874,0.005057591508728023,0.004343430810202281,0.00926816409691567,0.020932768384980305,0.00785722449857247,0.010564110036515958,0.003058478492707305,0.017320953448943837,0.024428612308636587,0.010505062823265568,0.02208616336937158,0.05280964313333698,0.004004845413782193,0.005113000839952551,0.008379441440475512,0.022397774964668365,0.0033914933949762242,0.21514282579871444,0.15888697801622456,0.04162650837187161,0.26367570868275864,0.0013931935084589204,0.04507176479283232,0.013432437835222032,0.004494546034221436,0.005203187449805732]) Coherent with our heatmap (although this does not always work out this way) no feature has much importance than the rest (also because there are lots of features). The method that got a better result according to out metric is GBT, so that is what we will use for our submission:\ngbt_preds.toPandas()[[\u0026#39;id\u0026#39;, \u0026#39;prediction\u0026#39;]].to_csv(\u0026#39;/content/predictions.csv\u0026#39;) ","date":"October 14, 2021","hero":"/es/posts/master-en-biolog%C3%ADa-computacional/big-data/dog-food-day/hero.png","permalink":"https://www.pablomarcos.me/es/posts/master-en-biolog%C3%ADa-computacional/big-data/dog-food-day/","summary":"Nothing ruins the thrill of buying a brand new car more quickly than seeing your new insurance bill. The sting’s even more painful when you know you’re a good driver. It doesn’t seem fair that you have to pay so much if you’ve been cautious on the road for years.\nInaccuracies in car insurance company’s claim predictions raise the cost of insurance for good drivers and reduce the price for bad ones.","tags":null,"title":"Extra Point: Auto Insurance"},{"categories":null,"contents":"A technology start-up in California has recently been hacked, and their forensic engineers have grabbed valuable information, including information like session time,locations, wpm typing speed, etc, to identify the hackers. Your goal is to use SparkML to do this.\nInitial Steps First, we need to create the Spark Session\n#In collab, we need to install everything: !apt-get install openjdk-8-jdk-headless -qq \u0026gt; /dev/null !wget -q https://mirrors.sonic.net/apache/spark/spark-3.1.2/spark-3.1.2-bin-hadoop3.2.tgz !tar xzf spark-3.1.2-bin-hadoop3.2.tgz !pip install -q findspark import os os.environ[\u0026#34;JAVA_HOME\u0026#34;] = \u0026#34;/usr/lib/jvm/java-8-openjdk-amd64\u0026#34; os.environ[\u0026#34;SPARK_HOME\u0026#34;] = \u0026#34;/content/spark-3.1.2-bin-hadoop3.2\u0026#34; import findspark findspark.init() from pyspark.sql import SparkSession spark = SparkSession.builder.master(\u0026#34;local[*]\u0026#34;).getOrCreate() #In a native Jupyter notebook, we would simply do: #from pyspark.sql import SparkSession #spark = SparkSession.builder.appName(\u0026#39;seedfinder\u0026#39;).getOrCreate() Afterwards, we can read the file and inspect it\n#Please drop the file in the environments \u0026#39;Files\u0026#39; panel df = spark.read.options(header=\u0026#34;true\u0026#34;, inferSchema=\u0026#34;true\u0026#34;).csv(\u0026#34;/content/hack_data.csv\u0026#34;) df.describe().toPandas() summary Session_Connection_Time Bytes Transferred Kali_Trace_Used Servers_Corrupted Pages_Corrupted Location WPM_Typing_Speed 0 count 334 334 334 334 334 334 334 1 mean 30.008982035928145 607.2452694610777 0.5119760479041916 5.258502994011977 10.838323353293413 None 57.342395209580864 2 stddev 14.088200614636158 286.33593163576757 0.5006065264451406 2.30190693339697 3.06352633036022 None 13.41106336843464 3 min 1.0 10.0 0 1.0 6.0 Afghanistan 40.0 4 max 60.0 1330.5 1 10.0 15.0 Zimbabwe 75.0 Deciding on the features of the Model The idea for this assignment is to use clustering methods to see if we can find which attacks belong to which hacker: essentially, we want to create n number of groups of attacks, where n is the number of involved hackers. We are also told that hackers like to equally divide work; so, for example, if we have (as we do) 335 attacks and 3 hackers, each would do 110 attacks; if, however, we only had 2 hackers, each would only do 165 attacks, and so on. I will use K-means clustering, which is not surprising, given its the only one we have been told how to use 😋.\nWe were told the \u0026ldquo;Location\u0026rdquo; feature is not really important due to VPN use, but I think including it might be interesting nonetheless, if not for the final result, just to learn a bit! So, here I use the StringIndexer to convert it to string format:\nfrom pyspark.ml import Pipeline from pyspark.ml.feature import StringIndexer #Bonus! Change this code to index multiple columns at once! indexers = [StringIndexer(inputCol=column, outputCol=column+\u0026#34;_index\u0026#34;).fit(df) for column in list([\u0026#34;Location\u0026#34;]) ] pipeline = Pipeline(stages=indexers) df_indexed = pipeline.fit(df).transform(df) df_indexed.describe().toPandas() summary Session_Connection_Time Bytes Transferred Kali_Trace_Used Servers_Corrupted Pages_Corrupted Location WPM_Typing_Speed Location_index 0 count 334 334 334 334 334 334 334 334 1 mean 30.008982035928145 607.2452694610777 0.5119760479041916 5.258502994011977 10.838323353293413 None 57.342395209580864 64.99700598802396 2 stddev 14.088200614636158 286.33593163576757 0.5006065264451406 2.30190693339697 3.06352633036022 None 13.41106336843464 50.98975334284259 3 min 1.0 10.0 0 1.0 6.0 Afghanistan 40.0 0.0 4 max 60.0 1330.5 1 10.0 15.0 Zimbabwe 75.0 180.0 Now, we can use the VectorAssembler to define our \u0026ldquo;features\u0026rdquo; column:\nfrom pyspark.ml.feature import VectorAssembler #By using a list comprehension we can define inputcols as the exclusion of some columns from df_indexed assembler = VectorAssembler(inputCols= [e for e in df_indexed.columns if e not in (\u0026#39;Location\u0026#39;)] , outputCol=\u0026#39;features\u0026#39;, handleInvalid=\u0026#39;skip\u0026#39;) output = assembler.transform(df_indexed) Another interesting thing to do is \u0026ldquo;standarization\u0026rdquo;. In essence, this adjusts all values to follow a \u0026ldquo;common scale\u0026rdquo;, so that they are easier to compare and process. Thus:\nfrom pyspark.ml.feature import StandardScaler scaler = StandardScaler(inputCol=\u0026#39;features\u0026#39;, outputCol=\u0026#39;scaled_features\u0026#39;) scalar_model = scaler.fit(output) scaled_data = scalar_model.transform(output) scaled_data.select(\u0026#39;scaled_features\u0026#39;).head() Row(scaled_features=DenseVector([0.5679, 1.3658, 1.9976, 1.2859, 2.2849, 5.3963, 1.7258])) Deciphering the number of hackers Now starts the difficult, more think-about-it part! We already know that we might have 2 OR 3 harkers, so, we are going to try to do it first with 3, then with 2, and compare which gets the best clustering score!\nfrom pyspark.ml.clustering import KMeans #Import the module First, we define and apply the models:\nkmeans3 = KMeans(featuresCol=\u0026#39;scaled_features\u0026#39;, k=3) model3 = kmeans3.fit(scaled_data) kmeans2 = KMeans(featuresCol=\u0026#39;scaled_features\u0026#39;, k=2) model2 = kmeans2.fit(scaled_data) Getting the results And then, we get the results:\nresults3 = model3.transform(scaled_data) results2 = model2.transform(scaled_data) We could also visualize the results; this has been abbreviated for efficiency\n#results3.select(\u0026#39;prediction\u0026#39;).show() #results2.select(\u0026#39;prediction\u0026#39;).show() Thats it! We have the results! Now, lets see how good the classification is: if all the attacks classify neatly in two groups, then, the two-hacker-theory would be validated; else, if we need a group more to explain all the attacks better, the three-hacker-thesis would be king! Lets see how this works:\nfrom pyspark.ml.evaluation import ClusteringEvaluator Lets generate the evaluations:\nClusteringEvaluator().evaluate(results2) 0.6555369436993117 ClusteringEvaluator().evaluate(results3) 0.3008773897853434 As we can see, ¡the two-hacker-theorem gets way, way better evaluation scores! This means that the underlying patterns in the data fit two-group classification way, way better than three-group classification! (ClusteringEvaluator uses the silhouette method, for which closeness to 1 signals closeness between the clusters and the clustering center. This centers can be shown using model.clusterCenters() )\nTo sum up: there are, definetely, two and only two hackers here\nAnd interesting to-do would be to show the characteristic\u0026rsquo;s % on each cluster, to see if we can unmask the criminals.\n","date":"October 14, 2021","hero":"/es/posts/master-en-biolog%C3%ADa-computacional/big-data/hack-data/hero.png","permalink":"https://www.pablomarcos.me/es/posts/master-en-biolog%C3%ADa-computacional/big-data/hack-data/","summary":"A technology start-up in California has recently been hacked, and their forensic engineers have grabbed valuable information, including information like session time,locations, wpm typing speed, etc, to identify the hackers. Your goal is to use SparkML to do this.\nInitial Steps First, we need to create the Spark Session\n#In collab, we need to install everything: !apt-get install openjdk-8-jdk-headless -qq \u0026gt; /dev/null !wget -q https://mirrors.sonic.net/apache/spark/spark-3.1.2/spark-3.1.2-bin-hadoop3.2.tgz !tar xzf spark-3.1.2-bin-hadoop3.2.tgz !pip install -q findspark import os os.","tags":null,"title":"Hack Data"},{"categories":null,"contents":"We have been contracted by a dog food company that uses an additive with 4 different chemicals (A, B, C and D) and a filler to preserve their food. The scientists have detected a problem: some batches of their dog food spoil much faster than expected. Since they haven\u0026rsquo;t updated their machinery, the levels of preservatives can vary a lot, so your job as a consultant is to use Machine Learning to detect which chemical is most responsible for the spoilage.\nInitial data Analysis First, we need to create the Spark Session\n!apt-get install openjdk-8-jdk-headless -qq \u0026gt; /dev/null !wget -q https://mirrors.sonic.net/apache/spark/spark-3.1.2/spark-3.1.2-bin-hadoop3.2.tgz !tar xzf spark-3.1.2-bin-hadoop3.2.tgz !pip install -q findspark import os os.environ[\u0026#34;JAVA_HOME\u0026#34;] = \u0026#34;/usr/lib/jvm/java-8-openjdk-amd64\u0026#34; os.environ[\u0026#34;SPARK_HOME\u0026#34;] = \u0026#34;/content/spark-3.1.2-bin-hadoop3.2\u0026#34; import findspark findspark.init() from pyspark.sql import SparkSession spark = SparkSession.builder.master(\u0026#34;local[*]\u0026#34;).getOrCreate() Afterwards, we can read the file and inspect it\n#Please drop the file in the environments \u0026#39;Files\u0026#39; panel df = spark.read.options(header=\u0026#34;true\u0026#34;, inferSchema=\u0026#34;true\u0026#34;).csv(\u0026#34;/content/dog_food.csv\u0026#34;) df.describe().toPandas() summary A B C D Spoiled 0 count 490 490 490 490 490 1 mean 5.53469387755102 5.504081632653061 9.126530612244897 5.579591836734694 0.2857142857142857 2 stddev 2.9515204234399057 2.8537966089662063 2.0555451971054275 2.8548369309982857 0.45221563164613465 3 min 1 1 5.0 1 0.0 4 max 10 10 14.0 10 1.0 Deciding on the Model\u0026rsquo;s Method The idea for this assignment is to use Tree Methods to find underlying patterns in data, preventing the model from making undue assumptions about the data itself and letting it speak by itself. In any case, and as with the previous models, we must first assemble a Vector with a \u0026ldquo;features\u0026rdquo; and a \u0026ldquo;label\u0026rdquo; tag so that it can be processed by Spark. For more commented code, please see other assignments:\nfrom pyspark.ml.feature import VectorAssembler, Imputer assembler = VectorAssembler(inputCols= [e for e in df.columns if e not in (\u0026#39;Spoiled\u0026#39;)] , outputCol=\u0026#39;features\u0026#39;, handleInvalid=\u0026#39;skip\u0026#39;) output = assembler.transform(df) imputer = Imputer(inputCols=[\u0026#39;Spoiled\u0026#39;], outputCols=[\u0026#39;label\u0026#39;], strategy=\u0026#39;mean\u0026#39;) imputer_model = imputer.fit(output) output = imputer_model.transform(output) As always, we divide the data into a train and a test set, so that we can test the metrics and see if everything went OK.\ntrain, test = output.randomSplit([0.7, 0.3]) We are using three different tree methods, which come bundled with spark:\nDecission Tree Classifier: It uses a decision tree (as a predictive model) to go from observations about an item (represented in the branches) to conclusions about the item\u0026rsquo;s target value (represented in the leaves). Random Forest Classifier: For classification tasks, the output of the random forest is the class selected by most trees. For regression tasks, the mean or average prediction of the individual trees is returned. Random decision forests correct for decision trees\u0026rsquo; habit of overfitting to their training set. Gradient Boosted Trees: It gives a prediction model in the form of an ensemble of weak prediction models, which are typically decision trees. When a decision tree is the weak learner, the resulting algorithm is called gradient-boosted trees; it usually outperforms random forest. We will use the three methods to find if their results match, and to pick the most accurate of the three.\n#Fist, we alias the methods to make them easier to call from pyspark.ml.classification import (RandomForestClassifier, GBTClassifier, DecisionTreeClassifier) dtc = DecisionTreeClassifier() rfc = RandomForestClassifier(numTrees = 100) gbt = GBTClassifier() #We fit the three models dtc_model = dtc.fit(train) rfc_model = rfc.fit(train) gbt_model = gbt.fit(train) #And get their predictions dtc_preds = dtc_model.transform(test) rfc_preds = rfc_model.transform(test) gbt_preds = gbt_model.transform(test) Analyzing the data To evaluate the models, we import MulticlassClassificationEvaluator, which will give us an accuracy metric\nfrom pyspark.ml.evaluation import MulticlassClassificationEvaluator evaluator = MulticlassClassificationEvaluator(metricName=\u0026#39;accuracy\u0026#39;) And we display it\nprint(f\u0026#39;DTC: {evaluator.evaluate(dtc_preds)} \\t Features Importance: {dtc_model.featureImportances}\u0026#39;) print(f\u0026#39;RFC: {evaluator.evaluate(rfc_preds)} Features Importance: {rfc_model.featureImportances}\u0026#39;) print(f\u0026#39;GBT: {evaluator.evaluate(gbt_preds)} \\t Features Importance: {gbt_model.featureImportances}\u0026#39;) DTC: 0.959731543624161 Features Importance: (4,[0,1,2,3],[0.010142105007278246,0.0016897828403142857,0.96352393472086,0.024644177431547582]) RFC: 0.9664429530201343 Features Importance: (4,[0,1,2,3],[0.025437259862519997,0.024627784289245877,0.9287766018080698,0.021158354040164428]) GBT: 0.959731543624161 Features Importance: (4,[0,1,2,3],[0.008071973502739728,0.03292367928105117,0.9086511641861458,0.05035318303006327]) As we can see, all methods have a quite similar (and quite high!) accuracy score, with the three of them coinciding in atttributing the 3rd column (Preservative C) an outsize influence (\u0026gt; 90%) on dog food spoilage.\nThus, we can conclude that it is Preservative C which is the most responsible for Dog Food Spoilage, and we can recommend for it to stop being used.\n","date":"October 14, 2021","hero":"/es/posts/master-en-biolog%C3%ADa-computacional/big-data/predicting-dog-food-spoiling/hero.jpg","permalink":"https://www.pablomarcos.me/es/posts/master-en-biolog%C3%ADa-computacional/big-data/predicting-dog-food-spoiling/","summary":"We have been contracted by a dog food company that uses an additive with 4 different chemicals (A, B, C and D) and a filler to preserve their food. The scientists have detected a problem: some batches of their dog food spoil much faster than expected. Since they haven\u0026rsquo;t updated their machinery, the levels of preservatives can vary a lot, so your job as a consultant is to use Machine Learning to detect which chemical is most responsible for the spoilage.","tags":null,"title":"Predicting Dog Food Spoiling"},{"categories":null,"contents":"Create a Jupyter notebook to execute the following tasks, as part of the Big Data engineerig course:\nStart a simple Spark session !apt-get install openjdk-8-jdk-headless -qq \u0026gt; /dev/null !wget -q https://mirrors.sonic.net/apache/spark/spark-3.1.2/spark-3.1.2-bin-hadoop3.2.tgz !tar xzf spark-3.1.2-bin-hadoop3.2.tgz !pip install -q findspark import os os.environ[\u0026#34;JAVA_HOME\u0026#34;] = \u0026#34;/usr/lib/jvm/java-8-openjdk-amd64\u0026#34; os.environ[\u0026#34;SPARK_HOME\u0026#34;] = \u0026#34;/content/spark-3.1.2-bin-hadoop3.2\u0026#34; import findspark findspark.init() from pyspark.sql import SparkSession spark = SparkSession.builder.master(\u0026#34;local[*]\u0026#34;).getOrCreate() import pandas as pd Load the Walmart Stock CSV file, let Spark infer the data types df = spark.read.options(infersSchema=\u0026#34;true\u0026#34;).csv(\u0026#34;/content/walmart_stock.csv\u0026#34;) df.printSchema() root |-- _c0: string (nullable = true) |-- _c1: string (nullable = true) |-- _c2: string (nullable = true) |-- _c3: string (nullable = true) |-- _c4: string (nullable = true) |-- _c5: string (nullable = true) |-- _c6: string (nullable = true) As we can see, Spark is not really good at infering the Schema. Lets manually coerce it:\ndf = spark.read.options(header=\u0026#34;true\u0026#34;).csv(\u0026#34;/content/walmart_stock.csv\u0026#34;) df.printSchema() root |-- Date: string (nullable = true) |-- Open: string (nullable = true) |-- High: string (nullable = true) |-- Low: string (nullable = true) |-- Close: string (nullable = true) |-- Volume: string (nullable = true) |-- Adj Close: string (nullable = true) Good! Now it works (more or less, see point 4)\nShow the column names df.schema.names ['Date', 'Open', 'High', 'Low', 'Close', 'Volume', 'Adj Close'] What does the Schema look like? df.printSchema() root |-- Date: string (nullable = true) |-- Open: string (nullable = true) |-- High: string (nullable = true) |-- Low: string (nullable = true) |-- Close: string (nullable = true) |-- Volume: string (nullable = true) |-- Adj Close: string (nullable = true) However, as you can see in Point 5, not all columns are strings: some of them are dates, some are numbers\u0026hellip; We will have to fix this later\nPrint out the first 5 rows df.toPandas().head() #If I dont do toPandas(), I only print the first row, and the formatting is worse Date Open High Low Close Volume Adj Close 0 2012-01-03 59.970001 61.060001 59.869999 60.330002 12668800 52.619234999999996 1 2012-01-04 60.209998999999996 60.349998 59.470001 59.709998999999996 9593300 52.078475 2 2012-01-05 59.349998 59.619999 58.369999 59.419998 12768200 51.825539 3 2012-01-06 59.419998 59.450001 58.869999 59.0 8069400 51.45922 4 2012-01-09 59.029999 59.549999 58.919998 59.18 6679300 51.616215000000004 Use describe() to learn about the DataFrame df.toPandas().describe() #If I do toPandas(), the shown info is cuter and more useful Date Open High Low Close Volume Adj Close count 1258 1258 1258 1258 1258 1258 1258 unique 1258 957 956 938 943 1250 1184 top 2014-09-18 74.839996 75.190002 74.510002 73.510002 12653800 69.701339 freq 1 5 5 5 5 2 3 Format the numbers to show only 2 decimal places #Moving it definitely toPandas() to make it easier to manage the df and set datatypes. df2 = df.toPandas().astype({\u0026#39;Date\u0026#39;: \u0026#34;string\u0026#34;, \u0026#39;Open\u0026#39;: float, \u0026#39;High\u0026#39;: float, \u0026#39;Low\u0026#39;: float, \u0026#39;Close\u0026#39;: float, \u0026#39;Volume\u0026#39;: float, \u0026#39;Adj Close\u0026#39;: float}) pd.options.display.float_format = \u0026#39;{:.2f}\u0026#39;.format df2.head() Date Open High Low Close Volume Adj Close 0 2012-01-03 59.97 61.06 59.87 60.33 12668800.00 52.62 1 2012-01-04 60.21 60.35 59.47 59.71 9593300.00 52.08 2 2012-01-05 59.35 59.62 58.37 59.42 12768200.00 51.83 3 2012-01-06 59.42 59.45 58.87 59.00 8069400.00 51.46 4 2012-01-09 59.03 59.55 58.92 59.18 6679300.00 51.62 Create a new DataFrame with a column called \u0026lsquo;HV Ratio\u0026rsquo; that is the ratio of the High Price vs Volume of Stock traded for a day newdf = df2; newdf[\u0026#39;HV Ratio\u0026#39;] = newdf[\u0026#39;High\u0026#39;]/newdf[\u0026#39;Volume\u0026#39;] newdf.head() Date Open High Low Close Volume Adj Close HV Ratio 0 2012-01-03 59.97 61.06 59.87 60.33 12668800.00 52.62 0.00 1 2012-01-04 60.21 60.35 59.47 59.71 9593300.00 52.08 0.00 2 2012-01-05 59.35 59.62 58.37 59.42 12768200.00 51.83 0.00 3 2012-01-06 59.42 59.45 58.87 59.00 8069400.00 51.46 0.00 4 2012-01-09 59.03 59.55 58.92 59.18 6679300.00 51.62 0.00 You may think: It makes no sense! How can the HV Ratio be always 0? Dont worry: its just that we asked pandas to only show the first two decimals. If we ask it to show, for example, 10 decimal places:\npd.options.display.float_format = \u0026#39;{:.8f}\u0026#39;.format newdf.head() Date Open High Low Close Volume Adj Close HV Ratio 0 2012-01-03 59.97000100 61.06000100 59.86999900 60.33000200 12668800.00000000 52.61923500 0.00000482 1 2012-01-04 60.20999900 60.34999800 59.47000100 59.70999900 9593300.00000000 52.07847500 0.00000629 2 2012-01-05 59.34999800 59.61999900 58.36999900 59.41999800 12768200.00000000 51.82553900 0.00000467 3 2012-01-06 59.41999800 59.45000100 58.86999900 59.00000000 8069400.00000000 51.45922000 0.00000737 4 2012-01-09 59.02999900 59.54999900 58.91999800 59.18000000 6679300.00000000 51.61621500 0.00000892 What day had the Peak High in Price? This is easy to do with sql:\ndf.createOrReplaceTempView(\u0026#34;Wallmart\u0026#34;) spark.sql(\u0026#34;SELECT Date FROM Wallmart where High = (SELECT MAX(High) from Wallmart);\u0026#34;).show() +----------+ | Date| +----------+ |2015-01-13| +----------+ What is the mean of the Close column spark.sql(\u0026#34;SELECT AVG(Close) from Wallmart;\u0026#34;).show() +--------------------------+ |avg(CAST(Close AS DOUBLE))| +--------------------------+ | 72.38844998012726| +--------------------------+ What is the max and min of the Volume column? spark.sql(\u0026#34;SELECT MIN(Volume), MAX(Volume) from Wallmart;\u0026#34;).show() +-----------+-----------+ |min(Volume)|max(Volume)| +-----------+-----------+ | 10010500| 9994400| +-----------+-----------+ How many days was the Close lower than 60 dollars? spark.sql(\u0026#34;SELECT COUNT(Date) from Wallmart WHERE Close \u0026lt; 60;\u0026#34;).show() +-----------+ |count(Date)| +-----------+ | 81| +-----------+ What percentage of time was the High greater than 80 dollars? spark.sql(\u0026#34;SELECT COUNT(Date) from Wallmart WHERE High \u0026gt; 80;\u0026#34;).show() +-----------+ |count(Date)| +-----------+ | 106| +-----------+ What is the Pearson correlation between High and Volume? This works better with Pandas:\nnewdf[\u0026#39;High\u0026#39;].corr(newdf[\u0026#39;Volume\u0026#39;]) -0.3384326061737164 What is the max High per year? For this, we need to set the datatype of the date column as datetime, so that python can work with it; then, we can work with the code:\nnewdf[\u0026#39;Date\u0026#39;] = pd.to_datetime(newdf[\u0026#39;Date\u0026#39;], format=\u0026#39;%Y-%m-%d\u0026#39;) newdf.groupby(newdf[\u0026#39;Date\u0026#39;].dt.year)[\u0026#39;High\u0026#39;].max() Date 2012 77.59999800 2013 81.37000300 2014 88.08999600 2015 90.97000100 2016 75.19000200 Name: High, dtype: float64 What is the average Close for each calendar month? newdf.groupby(newdf[\u0026#39;Date\u0026#39;].dt.month)[\u0026#39;Close\u0026#39;].mean() Date 1 71.44801958 2 71.30680444 3 71.77794378 4 72.97361901 5 72.30971689 6 72.49537742 7 74.43971944 8 73.02981855 9 72.18411785 10 71.57854545 11 72.11108931 12 72.84792478 Name: Close, dtype: float64 ","date":"October 14, 2021","hero":"/es/posts/master-en-biolog%C3%ADa-computacional/big-data/walmart-stock-exercise/hero.jpg","permalink":"https://www.pablomarcos.me/es/posts/master-en-biolog%C3%ADa-computacional/big-data/walmart-stock-exercise/","summary":"Create a Jupyter notebook to execute the following tasks, as part of the Big Data engineerig course:\nStart a simple Spark session !apt-get install openjdk-8-jdk-headless -qq \u0026gt; /dev/null !wget -q https://mirrors.sonic.net/apache/spark/spark-3.1.2/spark-3.1.2-bin-hadoop3.2.tgz !tar xzf spark-3.1.2-bin-hadoop3.2.tgz !pip install -q findspark import os os.environ[\u0026#34;JAVA_HOME\u0026#34;] = \u0026#34;/usr/lib/jvm/java-8-openjdk-amd64\u0026#34; os.environ[\u0026#34;SPARK_HOME\u0026#34;] = \u0026#34;/content/spark-3.1.2-bin-hadoop3.2\u0026#34; import findspark findspark.init() from pyspark.sql import SparkSession spark = SparkSession.builder.master(\u0026#34;local[*]\u0026#34;).getOrCreate() import pandas as pd Load the Walmart Stock CSV file, let Spark infer the data types df = spark.","tags":null,"title":"Walmart Stock Exercise"},{"categories":null,"contents":"Crop Production What are the main disciplines that helped to increase crop production in the last century and how did they do so? In the last century, and, specially, after 1960, we have witnessed the unfolding of a green revolution, a huge increase in the yield of crops and in food production that has enabled the human race to reduce hunger even when the population to feed keeps growing. This has been possible through the use of new, higher-yelding varieties, controlled irrigation (i.e. drop irrigation, which has made cultivation in the arid land of Israel possible), mechanisation, agrochemicals such as fertilizers and pesticides, and, as of lately, GMOs. This impact crop production in a variety of ways, all with the final goal of increasing yield: they nake the plants stronger and more productive, more resistant to droughts and protected from pests, and make collecting fresh produce easier and cheaper.\nPlant Breeding Goals What and why are the main goals in plant breeding? Plant breeding is essentially the genetic improvement of plants for the benefit of humans. The objectives pursued are very varied, such as increasing the frequency of favorable alleles (additive effects), increasing the frequency of favourable phenotypes (through dominance and epistatic effects) and increasing adaptation of crops to environments, making yields more stable and predictable and crops better adapted to each region.\nIn short, the aim of plant breeding is to achieve maximum yields of products while minimising impacts and environmental hazard. This, in itself, is a very human objective: maximising yields while minimising effort will lead to more production, and thus enable us to solve the major challenges of our generation, such as hunger or high food prices and food insecurity due to climate change.\nPlant Breeding Problem To improve protein concentration in i27, you do a round of selection, finding that, last year, individuals had 15.5% mean protein after a intensity of selection of 10%, while, the year before, the population had a protein mean of 12.5% and a standard phenotypic deviation of 1.71. If the narrow sense heritability is 0.6, what is the predicted mean of the progeny and genetic gain from this generation?. Show both ways of calculating it. #Since the mean protein concentration increases from 12.5 to 15.5, #the Response can be calculated as follows: Selection_Differential \u0026lt;- 15.5-12.5 Heritability = 0.6 Response \u0026lt;- Heritability^2 * Selection_Differential #This is what is called \u0026#34;genetic gain\u0026#34; print(Response) #Since the average has grown, the new average must be: NewMean \u0026lt;- 12.5 + Response print(NewMean) intensity \u0026lt;- 1.755 #For a selection of 10%, if we look at the normal distribution table, #this is what we get phenotypic_deviation \u0026lt;- 1.71 #What we were provided with Selection_Differential \u0026lt;- intensity*phenotypic_deviation Response \u0026lt;- Heritability^2 * Selection_Differential NewMean \u0026lt;- 12.5 + Response print(NewMean) As we can see, the two methods yield (approximately) the same results.\nBy adding marker assisted selection, you can raise the selection intensity to a value of 4. What would be your expected progeny mean and genetic gain now? With an intensity of selection of i = 4, the normal distribution table tells us that we are selecting 0.00001 % of the population. Unless we have a very big population, this might mean problems for the survival (i.e. population bottleneck).\nintensity \u0026lt;- 4 phenotypic_deviation \u0026lt;- 1.71 #What we were provided with Selection_Differential \u0026lt;- intensity*phenotypic_deviation Response \u0026lt;- Heritability^2 * Selection_Differential NewMean \u0026lt;- 12.5 + Response print(NewMean) As we can see, the difference with the previous case is pretty great, but scientists must take into account that this might cause an increase in genetic-transmitted diseases and susceptibility to pests, both traits that come with low gene diversity.\nWhat is the X~selected~ under this greater selection intensity. As we saw in the previous point, the X~selected~ is 0.00001 % of the original population.\nWheat variety Suppose you have a wheat variety with 2 loci and 2 alleles that affect the height trait, A1A2 and B1B2. Having an allele with a 2, adds one unit of measure to the phenotype. Having 1 doesn\u0026rsquo;t add any unit of measure to the phenotype. Please, indicate what are the possible genotypes and the value of the phenotypes from a two loci and 2 alleles example. Represent the information on a mathematical axis, where in the x axis you indicate the phenotypic values and in the y-axis the counts of the phenotypic values. Given that there are 4 aleles to combinate in groups of 2 (assuming A and B are not mutually exclusive), the combinatory number for 4 elements taken in groups of 2 tells us there are 4 possible combinations: A1B1 A1B2 A2B1 A2B2. For this genotypes, we get 3 possible phenotypes: Height-2, for A2B2; Height-1, for A1B2 and A2B1; and Height-0, for A1B1\nIf we plot a graph, as specified, to show the implications of this 4 genotypes on the phenotype, we get:\npossible_phenotypes \u0026lt;- c(\u0026#34;Height-2\u0026#34;,\u0026#34;Height-1\u0026#34;,\u0026#34;Height-0\u0026#34;) phenotype_frequency \u0026lt;- c(1,2,1) barplot(phenotype_frequency, names.arg=possible_phenotypes, main=\u0026#34;Phenotype frequency graph\u0026#34;, xlab=\u0026#34;Possible Phenotypes\u0026#34;, ylab=\u0026#34;Phenotype Frequency\u0026#34;) Sources used:\nHow to calculate Response to Selection - Nikolay\u0026rsquo;s Genetics Lessons on Youtube Broad sense heritability vs Narrow sense heritability - Nikolay\u0026rsquo;s Genetics Lessons on Youtube How to find a Selection Differential, Genetic Gain and Heritability - Nikolay\u0026rsquo;s Genetics Lessons This interactive normal distribution table Class materials ","date":"October 12, 2021","hero":"/es/posts/master-en-biolog%C3%ADa-computacional/gab/assignment-1/hero.jpg","permalink":"https://www.pablomarcos.me/es/posts/master-en-biolog%C3%ADa-computacional/gab/assignment-1/","summary":"Crop Production What are the main disciplines that helped to increase crop production in the last century and how did they do so? In the last century, and, specially, after 1960, we have witnessed the unfolding of a green revolution, a huge increase in the yield of crops and in food production that has enabled the human race to reduce hunger even when the population to feed keeps growing. This has been possible through the use of new, higher-yelding varieties, controlled irrigation (i.","tags":null,"title":"Assignment 1"},{"categories":null,"contents":"Oscuridad. Nunca más. Nunca más volverme a enamorar, a caer en esta ilusión medieval, en este cuento de hadas embrujado, este puñetero dolor endemoniado del corazón y este arder, este fuego en mi pecho cada vez que te pienso.\nYa basta. Voy a volver a luchar. Voy a volver a ser yo mismo, volver a encontrar esas ganas de vivir, de fluir con mi propia corriente aunque el río, siempre tan arrogante y altivo, intente machacar al insumiso que se atreve a salir.\nYo me bajo aquí. Aquí comienza mi nueva vida. Si tú necesitas seguir, buena suerte, amigo\n📸 : Chris Yang en Unsplash\n#despedida #farewell #poesiaenespañol #estrechosdemiras #poesia #poetry\nInstancia: Compartir Deja tu comentario en ","date":"June 15, 2021","hero":"/es/posts/poesia/despedida/hero.jpg","permalink":"https://www.pablomarcos.me/es/posts/poesia/despedida/","summary":"Oscuridad. Nunca más. Nunca más volverme a enamorar, a caer en esta ilusión medieval, en este cuento de hadas embrujado, este puñetero dolor endemoniado del corazón y este arder, este fuego en mi pecho cada vez que te pienso.\nYa basta. Voy a volver a luchar. Voy a volver a ser yo mismo, volver a encontrar esas ganas de vivir, de fluir con mi propia corriente aunque el río, siempre tan arrogante y altivo, intente machacar al insumiso que se atreve a salir.","tags":null,"title":"Yo me bajo aquí"},{"categories":null,"contents":"Nunca olvidaré cuando te confesé, tomando un café en Torino, que mi vida era una montaña rusa, que había perdido el sentido; que, mareado por los vaivenes del camino, ya no sabía volar, que sólo quería llorar, y que temía de nuevo acabar marginado en las cunetas del destino.\nEntonces - y fue entonces, sin duda, cuando supe que había encontrado un gran amigo - recuerdo que dijistes convencido:\n\u0026ldquo;Pues si tu vida es una montaña rusa, ojalá poderla vivir contigo\u0026rdquo;\nPues espero que esa promesa no caiga en el olvido, y que, cuando el tiempo maldito acaricie por última vez nuestros rostros cetrinos, apagados y marchitos, cuando llegue el último suspiro, cuando nos separe el destino, espero poder siguiendo llamándote amigo.\n📸 : Matt Bowden en Unsplash\n#montañarusa #rollercoaster #poesiaenespañol #estrechosdemiras #poesia #poetry\n","date":"June 6, 2021","hero":"/es/posts/poesia/monta%C3%B1a-rusa/hero.jpg","permalink":"https://www.pablomarcos.me/es/posts/poesia/monta%C3%B1a-rusa/","summary":"Nunca olvidaré cuando te confesé, tomando un café en Torino, que mi vida era una montaña rusa, que había perdido el sentido; que, mareado por los vaivenes del camino, ya no sabía volar, que sólo quería llorar, y que temía de nuevo acabar marginado en las cunetas del destino.\nEntonces - y fue entonces, sin duda, cuando supe que había encontrado un gran amigo - recuerdo que dijistes convencido:\n\u0026ldquo;Pues si tu vida es una montaña rusa, ojalá poderla vivir contigo\u0026rdquo;","tags":null,"title":"Montaña Rusa"},{"categories":null,"contents":"\u0026ldquo;Ya no te quiero\u0026rdquo;, dije en un desliz de guión. \u0026ldquo;Ya no te quiero\u0026rdquo; y mi corazón, ejerciendo como siempre de fiel apuntador, interrumpió a todo trapo en la acción, y me recordo que no hay ni luces, ni camara, ni acción, sin que él, el puñetero director de mi vida, así lo decida.\nVa a costar, como decía la canción, hacer ver que no hay dolor, que todo sigue igual, que no echo de menos tu carita al despertar y tu sonrisa irresistile al hablar. Va a costar fingir que puedo vivir sin ti, olvidar tu nombre y tu apellido, volver a ser sólamente cordiales desconocidos, que es lo que siempre debimos haber sido. Va a costar olvidarme de los juegos, de como creí poder acercarme a tu fuego sin quemarme, de como te conocí para ahora tener que olvidarte, de todos tus matices que te empeñabas en convertir en escala de grises para no destacar ante quienes temes que pudieran atacarte.\nVa a costar, pero te tendré que olvidar. Tendré que convencer a mi corazón de que, efectivamente, ya no te quiero, de que esto no es un hasta luego, sino un adiós definitivo. Va a costar sortear este nuevo abismo, y, sin embargo, lo tendré que lograr.\n📸 : Emiel Molenaar en Unsplash #amor #love #poesiaenespañol #estrechosdemiras #poesia #poetry\nInstancia: Compartir Deja tu comentario en ","date":"June 1, 2021","hero":"/es/posts/poesia/amor/hero.jpg","permalink":"https://www.pablomarcos.me/es/posts/poesia/amor/","summary":"\u0026ldquo;Ya no te quiero\u0026rdquo;, dije en un desliz de guión. \u0026ldquo;Ya no te quiero\u0026rdquo; y mi corazón, ejerciendo como siempre de fiel apuntador, interrumpió a todo trapo en la acción, y me recordo que no hay ni luces, ni camara, ni acción, sin que él, el puñetero director de mi vida, así lo decida.\nVa a costar, como decía la canción, hacer ver que no hay dolor, que todo sigue igual, que no echo de menos tu carita al despertar y tu sonrisa irresistile al hablar.","tags":null,"title":"Ya no te quiero"},{"categories":null,"contents":"No me atrevo ni a quererte ni a decirte que te quiero. No me atrevo ya ni a escribir poemas porque temo, temo estallar y que me abandone la poca salud mental que me queda. No me atrevo a volar, a existir, no quiero salir de casa si no es contigo, no se como romper este hechizo, este aquelarre de locura que barre mi cabeza con la soltura de un vals ruso.\nNo quiero que pase un segundo, un minuto, una hora, sin probar el fruto amargo de este amor que me ahoga, que me lleva en su ola como un tsunami sin control y frente al que yo, solamente he podido rendirme. Yo quise pensar que era imposible, que no sería una hoja más de tu libro de conquistas, que no podrías con mi fría razón y con mi ferreo armazón a prueba de balas\u0026hellip;\nY ahora\u0026hellip; ahora me falta el valor, no se ni como abrirme ante ti el corazón, como atreverme a contarte, mi amor, a decirte lo mucho que te quiero.\n📸 : Ed Leszczynskl en Unsplash\n#valor #courage #poesiaenespañol #estrechosdemiras #poesia #poetry\nInstancia: Compartir Deja tu comentario en ","date":"May 24, 2021","hero":"/es/posts/poesia/valor/hero.jpg","permalink":"https://www.pablomarcos.me/es/posts/poesia/valor/","summary":"No me atrevo ni a quererte ni a decirte que te quiero. No me atrevo ya ni a escribir poemas porque temo, temo estallar y que me abandone la poca salud mental que me queda. No me atrevo a volar, a existir, no quiero salir de casa si no es contigo, no se como romper este hechizo, este aquelarre de locura que barre mi cabeza con la soltura de un vals ruso.","tags":null,"title":"No me atrevo"},{"categories":null,"contents":"Ya basta. Es el momento de romper el hechizo, de conjurar a las brujas para salir del abismo.\nNo tienes poder aquí. No puedes pasar. No volverá a mi corazón la oscuridad que creía haber vencido.\nVolveré a vencer. Volveré a romper todos los cerrojos, machacaré los pedazos de mis sueños rotos, de la sombra del amor y del miedo a seguir solo. Seguiré, arrojaré los pocosb trozos de esta ansiedad que ya no me quita el sueño de este dolor que ya no recuerdo, a los oscuros pozos del olvido.\nYa basta de sueños rotos. Es el momento de amar, y, aunque sea sin ti, no pienso seguir solo\n📸 : Ryan Plomp en Unsplash\n#hechizo #spell #poesiaenespañol #estrechosdemiras #poesia #poetry\nInstancia: Compartir Deja tu comentario en ","date":"May 17, 2021","hero":"/es/posts/poesia/hechizo/hero.jpg","permalink":"https://www.pablomarcos.me/es/posts/poesia/hechizo/","summary":"Ya basta. Es el momento de romper el hechizo, de conjurar a las brujas para salir del abismo.\nNo tienes poder aquí. No puedes pasar. No volverá a mi corazón la oscuridad que creía haber vencido.\nVolveré a vencer. Volveré a romper todos los cerrojos, machacaré los pedazos de mis sueños rotos, de la sombra del amor y del miedo a seguir solo. Seguiré, arrojaré los pocosb trozos de esta ansiedad que ya no me quita el sueño de este dolor que ya no recuerdo, a los oscuros pozos del olvido.","tags":null,"title":"Romper tu embrujo"},{"categories":null,"contents":"No puedo. No puedo continuar sin ese pedacito de mi corazón que me has robado, no puedo pensar, no puedo reir ni llorar, solo puedo recordar los ecos de una noche estrellada bajo el cielo de Bruselas, en el ático de una casa robada con tiempo prestado y donde éramos, atemporalmente, felices.\nNo puedo cerrar los ojos sin ver en lo más profundo de mis párpados reflejarse tu mirada, tu sonrisa, tus palabras, el amor que tú me dabas y que tanto necesito, no puedo olvidar aquellos preciosos momentitos: dormir abrazados para esquivar el frío, reir como tontos en tu casa solitos, jugar a ver quien tumbaba al otro, o cuando me dijiste, secretamente al oído, que por fin habías encontrado, conmigo, un lugar donde desplegar las alas.\nDespliégalas. Abandona el nido. Es tu turno de volar, mientras yo me quedo aquí, perdido, tan cerca de tu amor, de tu mano cogido\u0026hellip; al borde de tus labios, pero siempre fuera.\n📸 : Yo mismo en Instagram, CC-By-Sa 4.0\n#renacer #rebirth #poesiaenespañol #estrechosdemiras #poesia #poetry\nInstancia: Compartir Deja tu comentario en ","date":"May 10, 2021","hero":"/es/posts/poesia/precipicio/hero.jpg","permalink":"https://www.pablomarcos.me/es/posts/poesia/precipicio/","summary":"No puedo. No puedo continuar sin ese pedacito de mi corazón que me has robado, no puedo pensar, no puedo reir ni llorar, solo puedo recordar los ecos de una noche estrellada bajo el cielo de Bruselas, en el ático de una casa robada con tiempo prestado y donde éramos, atemporalmente, felices.\nNo puedo cerrar los ojos sin ver en lo más profundo de mis párpados reflejarse tu mirada, tu sonrisa, tus palabras, el amor que tú me dabas y que tanto necesito, no puedo olvidar aquellos preciosos momentitos: dormir abrazados para esquivar el frío, reir como tontos en tu casa solitos, jugar a ver quien tumbaba al otro, o cuando me dijiste, secretamente al oído, que por fin habías encontrado, conmigo, un lugar donde desplegar las alas.","tags":null,"title":"Al borde de tus labios"},{"categories":null,"contents":"I sometimes feel like a tree, standing right by the cliff, having survived to all of this.\nI sometimes feel like a rock, standing high and standing tall, fighting against all odds.\nI sometimes feel like a mountain, big, old, and sometimes even broken.\nI feel alone most often.\nBut like a rock and like a tree, like a mountain and the sea, this too shall pass. I too shall understand, once the healing breeze of time lets the first grey hairs settle on my forehead, that all of this was not in vain. That all the love I have given and all the love I have received, all the lights, all the hugs and all the kisses in the moonlight, will be what remains of me to remember.\nAnd when the last tree of the forest is cut down, when my last sighs and my last words scratch the air for one last second, I will be able to say that I was full, and this short stay in the world of the living was worth it.\n📸 : JancickaL on Pixabay\n#courage #coraje #poesiaenespañol #poesia #poetry\nInstancia: Compartir Deja tu comentario en ","date":"May 5, 2021","hero":"/es/posts/poesia/courage/hero.png","permalink":"https://www.pablomarcos.me/es/posts/poesia/courage/","summary":"I sometimes feel like a tree, standing right by the cliff, having survived to all of this.\nI sometimes feel like a rock, standing high and standing tall, fighting against all odds.\nI sometimes feel like a mountain, big, old, and sometimes even broken.\nI feel alone most often.\nBut like a rock and like a tree, like a mountain and the sea, this too shall pass. I too shall understand, once the healing breeze of time lets the first grey hairs settle on my forehead, that all of this was not in vain.","tags":null,"title":"Courage"},{"categories":null,"contents":" Yay! Como veréis en lo alto del post, ¡Estrechos de Miras ha vuelto! Voy a ir publicando simultáneamente ahí y aquí: Sin no quereis perderos mis posts, apuntaos a la newsleter! Y, entonces, sucedió. Entonces, cuando sólo quedaba la última carta para completar mi castillo de naipes, cuando por fin culminaba el esfuerzo constante de tres años a la deriva, cuando me creía capaz de vivir la vida bailando a mi propio ritmo, entonces se abrió el cofre prohibido.\nSonó de repente un cañonazo, y, mientras se hacía pedazos mi reino de hielo, empezó a llenar el viento aquella música funesta. Me acordé entonces de esa maldita orquesta, de todos los instrumentos a los que juré prender fuego y que parecen volver de entre los muertos, dispuestos a remover cada grano de arena del desierto, y hasta a parar el tiempo, con tal de encontrarme. Me hechizó de nuevo su concierto macabro, ese que viste los llantos con clave de sol y armonías embotelladas, que droga las venas con cuentos de hadas, y que emborracha con su calor artificial robado al mañana.\nYa sé que juré que no me embaucarían de nuevo las flautas de Hamelín, esas que quise decir que eran Made in China y que ahora me tienen a mi atrapado entre sus sonidos. Ya sé que quemé el violín, y que de un hachazo vil perforé a los tambores: pero, ahora que han vuelto por mí, ahora no habrá forma de que no me enamore.\n📸 : Angelina Yan on Unsplash\n#renacer #rebirth #poesiaenespañol #estrechosdemiras #poesia #poetry\nInstancia: Compartir Deja tu comentario en ","date":"May 1, 2021","hero":"/es/posts/poesia/renacer/hero.jpg","permalink":"https://www.pablomarcos.me/es/posts/poesia/renacer/","summary":"Yay! Como veréis en lo alto del post, ¡Estrechos de Miras ha vuelto! Voy a ir publicando simultáneamente ahí y aquí: Sin no quereis perderos mis posts, apuntaos a la newsleter! Y, entonces, sucedió. Entonces, cuando sólo quedaba la última carta para completar mi castillo de naipes, cuando por fin culminaba el esfuerzo constante de tres años a la deriva, cuando me creía capaz de vivir la vida bailando a mi propio ritmo, entonces se abrió el cofre prohibido.","tags":null,"title":"La orquesta de los muertos"},{"categories":null,"contents":"De todas las teclas de mi teclado, hay cuatro que jamás he confesado: cuatro efigies, cuatro montañas, cuatro pilares y cuatro parcas, cuatro secretos que olvidar de este amor que me mata.\nPara que nadie la encuentre he quemado la primera, que ahora todos los días me falta: ahora ya no puedo confesar mis ojeras, ni el agujero que dejaste en mi alma\nDe tanto escribir tu nombre, la segunda ya solo es un espejismo; se esconde entre las sombras de mi inconsciente, y como un suicida está al borde al abismo.\nEnvidiosa e insolente ha saltado por los aires la tercera, celosa de sus hermanas: no se si no quiso ver, que sin ella no hay verte que valga.\nYo mismo he terminado de arrancar la cuarta, que se negaba a escribirme un \u0026ldquo;sí, quiero\u0026rdquo;; al intentar fundirla en el fuego, me he quemado los dedos.\nDe todas las letras del alfabeto, guardo estas cuatro en secreto; pues con cuatro se escribe su nombre, y temo que, al decirlo, se lo lleve el viento.\n📸 : Thom Milkovic on Unsplash\n#teclado #clavier #keyboard #poesiaenespañol #poesia #poetry\nInstancia: Compartir Deja tu comentario en ","date":"April 28, 2021","hero":"/es/posts/poesia/teclado/hero.jpg","permalink":"https://www.pablomarcos.me/es/posts/poesia/teclado/","summary":"De todas las teclas de mi teclado, hay cuatro que jamás he confesado: cuatro efigies, cuatro montañas, cuatro pilares y cuatro parcas, cuatro secretos que olvidar de este amor que me mata.\nPara que nadie la encuentre he quemado la primera, que ahora todos los días me falta: ahora ya no puedo confesar mis ojeras, ni el agujero que dejaste en mi alma\nDe tanto escribir tu nombre, la segunda ya solo es un espejismo; se esconde entre las sombras de mi inconsciente, y como un suicida está al borde al abismo.","tags":null,"title":"Las letras de tu nombre"},{"categories":null,"contents":"Volverte a ver es como llegar a casa después de un largo viaje, deseando apoyar mi cabeza en la suave almohada de tu pecho y probar los dulces frutos de tus labios. Volverte a ver es como disfrutar de un atardecer en la montaña, de la belleza inefable por no estropearla, remar tranquilo en un mar en calma, y sentir que no me importa lo que guarde el destino si me alarga un minuto este momento. Volverte a ver son todos los estereotipos que miento cuando digo que detesto, pues si hay algo que siento, es que te quiero.\n📸 : Mirko Stödter on Pixabay\n#volver #return #poesiaenespañol #poesia #poetry\nInstancia: Compartir Deja tu comentario en ","date":"April 21, 2021","hero":"/es/posts/poesia/volver/hero.png","permalink":"https://www.pablomarcos.me/es/posts/poesia/volver/","summary":"Volverte a ver es como llegar a casa después de un largo viaje, deseando apoyar mi cabeza en la suave almohada de tu pecho y probar los dulces frutos de tus labios. Volverte a ver es como disfrutar de un atardecer en la montaña, de la belleza inefable por no estropearla, remar tranquilo en un mar en calma, y sentir que no me importa lo que guarde el destino si me alarga un minuto este momento.","tags":null,"title":"Volverte a ver"},{"categories":null,"contents":"Para la Season of KDE 2021, decidí trabajar en el sitio web de Okular. Okular es un programa multifacético que uso casi todos los días para mis necesidades de lectura y anotación de PDF, aunque puede hacer mucho más. Desgraciadamente, su sitio web estaba un poco anticuado y no era apto para móviles. Por lo tanto, me propuse reescribir el sitio web de Okular utilizando el framework HUGO, de forma similar a como se hizo con el sitio web principal de kde.org, y manteniendo la coherencia con otras aplicaciones de KDE como Konsole. Afortunadamente, parte del trabajo ya fue iniciado por Carl Schwan, por lo que sólo tuve que continuar y terminar su trabajo.\nMentor Carl Schwan Enlaces Repositorios\nRepo oficial donde se fusionará todo el trabajo Repo de trabajo Trabajo realizado Enero de 2021 No hice mucho en enero, ya que era el mes de los \u0026ldquo;partiels\u0026rdquo; (exámenes finales) en INSA y estaba muy ocupado estudiando. Pero, de vuelta a España, ¡había mucha nieve! Lo cual es super bonito :p\nFebrero de 2021 Empecé por portar la sección de Anuncios de HTML (RDF) a Markdown, para lo cual escribí un pequeño script. Luego, configuré el proyecto para usar aether-sass (el tema estándar HUGO de KDE) como módulo Go, ya que los submódulos Git eran menos deseables. A continuación, añadí una sección de preguntas frecuentes, lo cual fue fácil ya que la mayoría de las preguntas habían sido eliminadas en una limpieza reciente. Para ello, utilicé las etiquetas HTML \u0026lt;/details\u0026gt; y \u0026lt;/summary\u0026gt;, que no conocía y que me parecen super chulas. Por último, añadí las nuevas páginas /download y /build-it, así como un nuevo index, utilizando una mezcla de markdown de HUGO y plantillas html crudo.\nMarzo de 2021 Una vez añadido el index como una plantilla html sin procesar, ahora necesitaba añadir soporte i18n; esto se consiguió utilizando la plantilla {{i18n}} de HUGO, que también utilicé en la tabla de formatos soportados de /applications. A continuación, añadí la página /contacto, que rediseñé y actualicé para incluir matrix. Modifiqué el índice para incluir una referencia a la próxima compatibilidad con la firma de PDF de Okular (¡qué bien! Puede que finalmente deje de usar Autofirma), limpié los archivos innecesarios que quedaron de la transición, y añadí una funcionalidad de Búsqueda en el Sitio que funciona del lado del cliente y no depende de google, inspirada en (en los mentideros dirán que copiada de) el popular gist de eddieweb.\nAbril de 2021 Todo el trabajo en el proyecto ya está terminado. Sólo quedan algunas modificaciones menores y escribir este informe del proyecto.\nBalance del proyecto Creo que los objetivos fijados al inicio del proyecto (haz clic aquí si no tienes una cuenta de KDE) se han cumplido en su mayor parte: la nueva web presenta la información de forma clara, bonita y amigable para los móviles, gracias al tema aether-sass. Tiene un sistema de búsqueda independiente de Google, aunque no utiliza lunar.js como se sugirió, y las noticias de desarrollo están ahora en su propia sección, aunque los registros de cambios se dejaron fuera de los mensajes individuales por razones de practicidad. Las capturas de pantalla han sido actualizadas, pero la mayoría han sido eliminadas ya que hemos decidido hacer una página de inicio más sencilla que destaque las características más importantes del programa.\nEn definitiva, se han mantenido la mayoría de las características de la antigua página web, pero se ha añadido un rediseño más moderno que aumenta la usabilidad y hace el proyecto más atractivo y coherente con la estética de KDE.\nLo que he aprendido ¡Las etiquetas HTML \u0026lt;/details\u0026gt; y \u0026lt;/summary\u0026gt; existen! Esto ha sido súper útil, pues ya he utilizado ese conocimiento en mi página web personal :p Cómo usar HUGO en general, y HUGO i18n en particular Que, en mi opinión, HUGO es no solo mucho más modular y adaptable que, por ejemplo, WordPress, sino que, para las personas que sabemos usar la terminal de comandos, es incluso más sencillo ¡Que Okular está añadiendo soporte para firmas PDF! Cómo manejar HTML, CSS y JavaScript, conocimientos importantes que ya he aplicado en mi próximo proyecto, la web de la Revista Científica \u0026ldquo;Pensamiento Matemático\u0026rdquo; ¡KDE es tan genial como pensaba! 😏 Blog Posts on KDE Planet Los enlaces de estos posts del blog están en mi sitio personal, no en KDE Planet, pero puedes comprobar que se agregan a Planet pinchando aquí\nPost de enero de 2021 Post de febrero de 2021 Post de marzo de 2021 Post de abril de 2021 Screenshots Aquí puedes encontrar algunos ejemplos del trabajo que he realizado.\nComo puedes ver, la antigua web no tenía soporte para móviles Sin embargo, la segunda versión funciona de maravilla en el móvil He mejorado la sección de Noticias usando list.html y un script de python Y añadí las FAQ usando \u0026lt;detalles\u0026gt; y \u0026lt;resumen\u0026gt;. La sección de descargas te ayuda a encontrar las opciones de descarga disponibles. Y la sección Build It muestra cómo construir el programa desde el código fuente Si quieres contactar con los desarrolladores, hay información en la página de contacto La página de Formatos Soportados lista las extensiones que Okular puede abrir Se puede buscar usando la barra de navegación, y los resultados se resaltan Y, por último, la comparación lado a lado: Aquí está el antiguo sitio web (enlace al Internet Archive): Y aquí está la nueva:\nContacto Si quieres hacer sugerencias para este proyecto, no dudes en ponerte en contacto conmigo.\nKDE Invent :- Pablo Marcos\nMatrix :- @pablitouh:matrix.org\n","date":"April 9, 2021","hero":"/es/posts/concursos/season-of-kde-2021/sok-final-status-report/hero.png","permalink":"https://www.pablomarcos.me/es/posts/concursos/season-of-kde-2021/sok-final-status-report/","summary":"Para la Season of KDE 2021, decidí trabajar en el sitio web de Okular. Okular es un programa multifacético que uso casi todos los días para mis necesidades de lectura y anotación de PDF, aunque puede hacer mucho más. Desgraciadamente, su sitio web estaba un poco anticuado y no era apto para móviles. Por lo tanto, me propuse reescribir el sitio web de Okular utilizando el framework HUGO, de forma similar a como se hizo con el sitio web principal de kde.","tags":null,"title":"SoK 2021 - Informe Final "},{"categories":null,"contents":"Basta. Ya basta de canciones tristes, basta de ciudades grises y de desear imposibles. Es primavera, y ya va tocando ser libres, abrir las ventanas y dejar salir los dramas, que se los lleve el viento para poder recuperar el aliento, pues si algo siento es no sonreir cada momento.\nHe encontrado una casa a la que llamar hogar, y, aunque no se me debe olvidar que todo se desvanecerá cuando llegue agosto, tampoco puedo dejar de gritar, de decir con la boca llena de alegría: ven aquí amigo, entra en mi vida.\n📸 : Lindsey Garcia en Unsplash, Unsplash License\n#basta #enough #poesiaenespañol #poesia #poetry\nInstancia: Compartir Deja tu comentario en ","date":"April 7, 2021","hero":"/es/posts/poesia/basta/hero.jpg","permalink":"https://www.pablomarcos.me/es/posts/poesia/basta/","summary":"Basta. Ya basta de canciones tristes, basta de ciudades grises y de desear imposibles. Es primavera, y ya va tocando ser libres, abrir las ventanas y dejar salir los dramas, que se los lleve el viento para poder recuperar el aliento, pues si algo siento es no sonreir cada momento.\nHe encontrado una casa a la que llamar hogar, y, aunque no se me debe olvidar que todo se desvanecerá cuando llegue agosto, tampoco puedo dejar de gritar, de decir con la boca llena de alegría: ven aquí amigo, entra en mi vida.","tags":null,"title":"Primavera"},{"categories":null,"contents":"Esta noche también me siento solo. Esta noche también resbalan por mi rostro, juguetonas, las lágrimas tristonas del delirio. Y, mientras admiro el equilibrio, tan sencillo y frío, del cosmos - parece tan indiferente en su inmensa soledad - intento poco a poco arrancar las espinas que parezco condenado a clavarme.\nEn el movil suena un cantante, un brujo nigromante que parece que con su arte me cuenta lo que no me atrevo a decir con palabras. Y es que otra vez me siento acabado, me odio como un niño mimado odia su fragilidad, y me pregunto si, en vez de por mi lealtad, debería darme a conocer por mi malicia.\nYo solo quesia besarte, y, cuando dejé de mirarte, vi que me habías empujado al vacío. A ver como salgo vivo de esta.\n📸 : Dejan Zakic en Unsplash, Unsplash License\n#espinas #thorns #poesiaenespañol #poesia #poetry\nInstancia: Compartir Deja tu comentario en ","date":"April 7, 2021","hero":"/es/posts/poesia/espinas/hero.jpg","permalink":"https://www.pablomarcos.me/es/posts/poesia/espinas/","summary":"Esta noche también me siento solo. Esta noche también resbalan por mi rostro, juguetonas, las lágrimas tristonas del delirio. Y, mientras admiro el equilibrio, tan sencillo y frío, del cosmos - parece tan indiferente en su inmensa soledad - intento poco a poco arrancar las espinas que parezco condenado a clavarme.\nEn el movil suena un cantante, un brujo nigromante que parece que con su arte me cuenta lo que no me atrevo a decir con palabras.","tags":null,"title":"Espinas"},{"categories":null,"contents":"Todo el trabajo del proyecto se terminó en marzo, así que sólo quedan algunas modificaciones menores y la redacción del informe del proyecto. Creo que los objetivos fijados al inicio se han cumplido en su mayor parte: la nueva web presenta la información de forma más clara, bonita y amigable para los móviles, gracias al tema aether-sass. Tiene búsqueda independiente de google, aunque no utiliza lunar.js como se sugirió, y las noticias de desarrollo están ahora en su propia sección, aunque los changelogs se dejaron fuera de los posts individuales por razones de practicidad. Las capturas de pantalla han sido actualizadas, pero la mayoría de ellas han sido eliminadas ya que nos hemos decidido por una página de inicio más sencilla que destaque las características más importantes del programa.\nEn definitiva, se han mantenido la mayoría de las características de la antigua página web, pero se ha añadido un rediseño más moderno que aumenta la usabilidad y hace el proyecto más atractivo y coherente con la estética de KDE. Puedes comprobarlo con esta comparación: Aquí está la antigua página web (enlace del Internet Archive): Y aquí está la nueva:\n","date":"April 1, 2021","hero":"/es/posts/concursos/season-of-kde-2021/sok-report-april/hero.jpg","permalink":"https://www.pablomarcos.me/es/posts/concursos/season-of-kde-2021/sok-report-april/","summary":"Todo el trabajo del proyecto se terminó en marzo, así que sólo quedan algunas modificaciones menores y la redacción del informe del proyecto. Creo que los objetivos fijados al inicio se han cumplido en su mayor parte: la nueva web presenta la información de forma más clara, bonita y amigable para los móviles, gracias al tema aether-sass. Tiene búsqueda independiente de google, aunque no utiliza lunar.js como se sugirió, y las noticias de desarrollo están ahora en su propia sección, aunque los changelogs se dejaron fuera de los posts individuales por razones de practicidad.","tags":null,"title":"SoK 2021 - Informe de Abril"},{"categories":null,"contents":"Un abrazo. A veces sólo hace falta un abrazo. Reposar la cabeza suavemente en tu pecho, dejar de dar por hecho que me quieres porque yo te quiero, perder el miedo, y ser sincero.\nAunque sea en silencio. Aunque sólo sea con el torpe danzar de nuestros cuerpos, aunque no tengamos remedio y no superemos el miedo a revelar en voz alta que lo único que queremos es querernos.\nQue tenemos que derretirnos en un abrazo sin pretensiones, un abrazo que no deje corazones rotos, sino tan solo la calma que nos cura poco a poco, la promesa de amistad entre nosotros y el deseo de que esto no toque fondo, mientras cae a plomo la noche y olvidamos a lo loco nuestros nombres, remendando así, con mil parches de colores, la suave tela de seda que visten nuestros corazones.\nY es que al final, no hace falta que os cuente mis reflexiones, que intercambiemos reacciones o que detallemos en confesiones lo que ha pasado: ya lo sé, vosotros también necesitábais un abrazo.\n📸 : Carlo Knell en Unsplash, Unsplash License\n#abrazo #hug #poesiaenespañol #poesia #poetry\nInstancia: Compartir Deja tu comentario en ","date":"March 28, 2021","hero":"/es/posts/poesia/abrazo/hero.jpg","permalink":"https://www.pablomarcos.me/es/posts/poesia/abrazo/","summary":"Un abrazo. A veces sólo hace falta un abrazo. Reposar la cabeza suavemente en tu pecho, dejar de dar por hecho que me quieres porque yo te quiero, perder el miedo, y ser sincero.\nAunque sea en silencio. Aunque sólo sea con el torpe danzar de nuestros cuerpos, aunque no tengamos remedio y no superemos el miedo a revelar en voz alta que lo único que queremos es querernos.\nQue tenemos que derretirnos en un abrazo sin pretensiones, un abrazo que no deje corazones rotos, sino tan solo la calma que nos cura poco a poco, la promesa de amistad entre nosotros y el deseo de que esto no toque fondo, mientras cae a plomo la noche y olvidamos a lo loco nuestros nombres, remendando así, con mil parches de colores, la suave tela de seda que visten nuestros corazones.","tags":null,"title":"Un abrazo"},{"categories":null,"contents":"En el post de febrero, expliqué como he reordenado el material que había en el MVP de Carl Schwan para el index, que ahora explica las principales características de Okular. Una vez hecho esto, me dediqué a añadir soporte para i18n, ya que puede ser difícil para los traductores trabajar con una plantilla de html crudo. Para ello, he usado la plantilla {{i18n}} de HUGO, que especifica las partes traducibles usando {{ i18n \u0026quot;Section.variable\u0026quot; }} en cada una de las cadenas a traducir, y luego mapeando los valores en un archivo yaml:\n/i18n/es.yaml #Nombre de la sección traducible. Section.variable: other: \u0026#34;Traducción al inglés de \u0026#39;variable\u0026#39;\u0026#34; Section.othervariable:: other: \u0026#34;Traducción al inglés para \u0026#39;othervariable\u0026#39;\u0026#34; Section.yetanothervariable:: other: \u0026#34;Traducción al inglés para \u0026#39;yetanothervariable\u0026#39;\u0026#34; También he utilizado esto para traducir la tabla de formatos soportados en /applications. El resultado final se puede ver aquí:\nLa página de inicio, con soporte para i18n Y la lista de formatos de archivo que Okular puede abrir A continuación, añadí la página de /contacto, que rediseñé y actualicé para incluir la posibilidad de usar matrix. También modifiqué el index para incluir una referencia a la próxima funcionalidad de firma de PDFs de Okular (¡UwU! A ver si me sirve de reemplazo para Autofirma), y limpié los archivos innecesarios que quedaban de la transición desde la web anterior.\nSi quieres contactar con los desarrolladores, hay información en la página de contacto También puedes firmar tus propios pdfs así como ver y verificar otras firmas. También quería añadir un sistema de búsqueda del lado del cliente, ya que el que existía era realmente un botón que redirigía la consulta una búsqueda de Google. Me inspiré en (en los mentideros dirán que me copié de) el popular gist de eddieweb, que no sólo no requiere instalar paquetes adicionales, sino que además resalta las coincidencias usando JSON y Fuse.js. Ha sido fácil de añadir, ¡pero sigo estando orgulloso! 🤩 Es bonito ayudar a hacer la web menos dependiente de google 😊\nPuedes ver cómo queda aquí:\nPuedes buscar usando la barra de navegación, y los resultados están resaltados\nSí, ha sido un mes con mucho trabajo, pero, ¡por fin está todo listo! 🥳\n","date":"March 26, 2021","hero":"/es/posts/concursos/season-of-kde-2021/sok-report-march/hero.jpg","permalink":"https://www.pablomarcos.me/es/posts/concursos/season-of-kde-2021/sok-report-march/","summary":"En el post de febrero, expliqué como he reordenado el material que había en el MVP de Carl Schwan para el index, que ahora explica las principales características de Okular. Una vez hecho esto, me dediqué a añadir soporte para i18n, ya que puede ser difícil para los traductores trabajar con una plantilla de html crudo. Para ello, he usado la plantilla {{i18n}} de HUGO, que especifica las partes traducibles usando {{ i18n \u0026quot;Section.","tags":null,"title":"SoK 2021 - Informe de Marzo"},{"categories":null,"contents":"Te arrancaré como se arranca el hiero de una herida, dejaré a mi sangre correr por la hierba mullida del olvido, juraré que nunca te he conocido, te negaré tres veces antes de que cante el gallo, y, cuando despierte de este esperpéntico desmayo, olvidaré que estuve enamorado.\nHoy, en vez de un poema largo como suelo hacer, me ha llamado la atención el #2MinuteSketch \u0026ldquo;challenge\u0026rdquo; de @Curator, y he pensado que podría escribir algo por el estilo. No se que tal me ha quedado 🙈🙈\n📸 : Ritah Nyakato en Pixabay, Pixabay License\n#cerezo #cherry #poesiaenespañol #poesia #poetry\nInstancia: Compartir Deja tu comentario en ","date":"March 23, 2021","hero":"/es/posts/poesia/cerezo/hero.png","permalink":"https://www.pablomarcos.me/es/posts/poesia/cerezo/","summary":"Te arrancaré como se arranca el hiero de una herida, dejaré a mi sangre correr por la hierba mullida del olvido, juraré que nunca te he conocido, te negaré tres veces antes de que cante el gallo, y, cuando despierte de este esperpéntico desmayo, olvidaré que estuve enamorado.\nHoy, en vez de un poema largo como suelo hacer, me ha llamado la atención el #2MinuteSketch \u0026ldquo;challenge\u0026rdquo; de @Curator, y he pensado que podría escribir algo por el estilo.","tags":null,"title":"Te arrancaré"},{"categories":null,"contents":"A veces sólo me apetece dormir. A veces solo quiero reposar, soñar, construir con un grano un castillo de arena, vivir el cuento de la lechera, ver florecer de una vez a la puta primavera; a veces, hasta yo me canso de que me corten las flores, de que me pidan glorificar los dolores y de los achaques, o de la gente que fantasea con que todo sea como era antes.\nA veces creo escuchar, jugetona, en mi cabeza, una elegante pieza, una proeza sinfónica y descontrolada, que me dice: \u0026ldquo;No, para, deja de perder el aliento, siéntate junto a mí y dejemos pasar el tiempo, veamos a nuestra risa desvanecerse en el viento, cortemos las crueles amarras del pensamiento, unámonos en este juego se sábanas en el que pasamos media vida.\u0026rdquo;\nY entonces, acepto: aunque me cueste diez horas de intentos, y el insomnio se oponga a todos mis esfuerzos, al final siempre llega el momento en que me desvanezco.\n📸 : Caspar David Friedrich, El caminante sobre el mar de nubes, Public domain, via Wikimedia Commons\n#sueño #soñar #dream #poesiaenespañol #poesia #poetry\nInstancia: Compartir Deja tu comentario en ","date":"March 16, 2021","hero":"/es/posts/poesia/sue%C3%B1o/hero.jpg","permalink":"https://www.pablomarcos.me/es/posts/poesia/sue%C3%B1o/","summary":"A veces sólo me apetece dormir. A veces solo quiero reposar, soñar, construir con un grano un castillo de arena, vivir el cuento de la lechera, ver florecer de una vez a la puta primavera; a veces, hasta yo me canso de que me corten las flores, de que me pidan glorificar los dolores y de los achaques, o de la gente que fantasea con que todo sea como era antes.","tags":null,"title":"Soñar"},{"categories":null,"contents":"Eres como un helado de guayaba, un sabor misterioso que antes sólo contemplaba, curioso, mientras pasaba a mi lado. Y, sin embargo, ahora que me he fijado en tí, ahora que te veo reir con la gracia de una musa griega, ahora te quiero sentir, quiero tenerte a mi vera. Quiero empaparme de tu olor, jugar a esconder en las notas dulces y sutiles de tu perfume nuestro amor, a veces regalarte una flor y a veces dibujarte las estrellas. Quiero verte como a un ruiseñor, volando libre pero a mi lado, pues esa risa me ha cautivado, y necesito conocerte mejor.\nQuiero sentirte sin dolor, sin rabia, sin tristeza, con la ligereza que confiere el presente, y ya me dirás, cuando llegue septiembre, si mereció la pena este alocado verano. Quiero amarte como nunca he amado, con la intensidad de lo contidiano y con el día mutuamente acordado de una fecha de caducidad. Quiero recordarte con bondad, no como otra herida que no cierra; por eso te lo digo, amor: quiero comerte entera.\n📸 : Dovile Ramoskaite on Unsplash\n#guayaba #poesiaenespañol #poesia #poetry\n","date":"March 10, 2021","hero":"/es/posts/poesia/guayaba/hero.jpg","permalink":"https://www.pablomarcos.me/es/posts/poesia/guayaba/","summary":"Eres como un helado de guayaba, un sabor misterioso que antes sólo contemplaba, curioso, mientras pasaba a mi lado. Y, sin embargo, ahora que me he fijado en tí, ahora que te veo reir con la gracia de una musa griega, ahora te quiero sentir, quiero tenerte a mi vera. Quiero empaparme de tu olor, jugar a esconder en las notas dulces y sutiles de tu perfume nuestro amor, a veces regalarte una flor y a veces dibujarte las estrellas.","tags":null,"title":"Guayaba"},{"categories":null,"contents":"Empecé el mes portando la sección de Anuncios de HTML a Markdown, para lo cual escribí un pequeño script en python que analiza todo el contenido de este archivo news.rdf, heredado del antiguo sitio, y crea varios archivos de posts en markdown que serán procesados por las plantillas \u0026rsquo;list.html\u0026rsquo; y \u0026lsquo;single.html\u0026rsquo; del tema aether-sass para convertirlas en la nueva sección.\nPara ver el script de python al completo, pincha aquí import re text_file = open(\u0026#34;./news.rdf\u0026#34;, \u0026#34;r\u0026#34;) data = text_file.read() titles = re.findall(\u0026#39;\u0026lt;title\u0026gt;(.*)\u0026lt;/title\u0026gt;\u0026#39;, data) dates = re.findall(\u0026#39;\u0026lt;date\u0026gt;(.*)\u0026lt;/date\u0026gt;\u0026#39;, data) fullstories = re.findall(\u0026#39;\u0026lt;fullstory\u0026gt;(.*)\u0026lt;/fullstory\u0026gt;\u0026#39;, data) i=0; mes=0; dia =0 for i in range(len(dates)): año = dates[i].split(\u0026#39; \u0026#39;)[2] day = dates[i].split(\u0026#39; \u0026#39;)[1].replace(\u0026#39;,\u0026#39;,\u0026#39;\u0026#39;) if day == \u0026#39;1\u0026#39;: dia = \u0026#39;01\u0026#39; elif day == \u0026#39;2\u0026#39;: dia = \u0026#39;02\u0026#39; elif day == \u0026#39;3\u0026#39;: dia = \u0026#39;03\u0026#39; elif day == \u0026#39;4\u0026#39;: dia = \u0026#39;04\u0026#39; elif day == \u0026#39;5\u0026#39;: dia = \u0026#39;05\u0026#39; elif day == \u0026#39;6\u0026#39;: dia = \u0026#39;06\u0026#39; elif day == \u0026#39;7\u0026#39;: dia = \u0026#39;07\u0026#39; elif day == \u0026#39;8\u0026#39;: dia = \u0026#39;08\u0026#39; elif day == \u0026#39;9\u0026#39;: dia = \u0026#39;09\u0026#39; else: dia = day month = dates[i].split(\u0026#39; \u0026#39;)[0] if month == \u0026#39;January\u0026#39;: mes = \u0026#39;01\u0026#39; elif month == \u0026#39;February\u0026#39;: mes = \u0026#39;02\u0026#39; elif month == \u0026#39;March\u0026#39;: mes = \u0026#39;03\u0026#39; elif month == \u0026#39;April\u0026#39;: mes = \u0026#39;04\u0026#39; elif month == \u0026#39;May\u0026#39;: mes = \u0026#39;05\u0026#39; elif month == \u0026#39;June\u0026#39;: mes = \u0026#39;06\u0026#39; elif month == \u0026#39;July\u0026#39;: mes = \u0026#39;07\u0026#39; elif month == \u0026#39;August\u0026#39;: mes = \u0026#39;08\u0026#39; elif month == \u0026#39;September\u0026#39;: mes = \u0026#39;09\u0026#39; elif month == \u0026#39;October\u0026#39;: mes = \u0026#39;10\u0026#39; elif month == \u0026#39;November\u0026#39;: mes = \u0026#39;11\u0026#39; elif month == \u0026#39;December\u0026#39;: mes = \u0026#39;12\u0026#39; elif month == \u0026#39;December\u0026#39;: mes = \u0026#39;12\u0026#39; elif month == \u0026#39;Jul\u0026#39;: mes = \u0026#39;07\u0026#39; elif month == \u0026#39;Apr\u0026#39;: mes = \u0026#39;04\u0026#39; elif month == \u0026#39;Jan\u0026#39;: mes = \u0026#39;01\u0026#39; elif month == \u0026#39;Nov\u0026#39;: mes = \u0026#39;11\u0026#39; elif month == \u0026#39;Aug\u0026#39;: mes = \u0026#39;08\u0026#39; fecha = str(año)+\u0026#39;-\u0026#39;+str(mes)+\u0026#39;-\u0026#39;+str(dia) filetoopen = str(titles[i].replace(\u0026#39;,\u0026#39;,\u0026#39;\u0026#39;).replace(\u0026#39; \u0026#39;,\u0026#39;_\u0026#39;)+\u0026#39;.md\u0026#39;) f = open(filetoopen, \u0026#34;a\u0026#34;) f.write(\u0026#39;---\\ndate: \u0026#39;+fecha+\u0026#39;\\ntitle: \u0026#39;+titles[i]+\u0026#39;\\n---\\n\u0026#39;+fullstories[i]) #f.close() Luego, configuré el proyecto para usar el ya mencionado aether-sass (el tema estándar de KDE para HUGO) como módulo Go, ya que los submódulos Git son menos deseables. A continuación, añadí una sección de preguntas frecuentes, lo cual fue fácil ya que la mayoría de las preguntas habían sido eliminadas en una limpieza reciente. Para ello, utilicé las etiquetas HTML \u0026lt;/details\u0026gt; y \u0026lt;/summary\u0026gt;, que no conocía y que me parecen super chulas. ¡Esta fue, también, la primera vez que experimenté cómo funciona una plantilla de HUGO! ¡Yay!\nHe mejorado la sección de Noticias usando list.html, de HUGO, y un script de python Y he añadido las preguntas frecuentes usando \u0026lt;detalles\u0026gt; y \u0026lt;resumen\u0026gt;. Por último, añadí las nuevas páginas /download y /build-it, así como un nuevo index, utilizando una mezcla de markdown de HUGO y plantillas de html. Puedes ver algunas capturas de pantalla a continuación:\nLa sección de descargas te ayuda a encontrar las opciones de descarga disponibles. Y la sección Build It muestra cómo construir el programa desde el código fuente ","date":"February 25, 2021","hero":"/es/posts/concursos/season-of-kde-2021/sok-report-february/hero.jpg","permalink":"https://www.pablomarcos.me/es/posts/concursos/season-of-kde-2021/sok-report-february/","summary":"Empecé el mes portando la sección de Anuncios de HTML a Markdown, para lo cual escribí un pequeño script en python que analiza todo el contenido de este archivo news.rdf, heredado del antiguo sitio, y crea varios archivos de posts en markdown que serán procesados por las plantillas \u0026rsquo;list.html\u0026rsquo; y \u0026lsquo;single.html\u0026rsquo; del tema aether-sass para convertirlas en la nueva sección.\nPara ver el script de python al completo, pincha aquí import re text_file = open(\u0026#34;.","tags":null,"title":"SoK 2021 - Informe de Febrero"},{"categories":null,"contents":"Sigo buscando el reflejo de tus ojos verdes, de esas dos esmeraldas de luz que siempre me miraron altivas como copa de pino, sigo luchando por seguir vivo, como una planta que escala desesperada las grietas de una cumbre borrascosa.\nSigo escuchando que el verde es el color de la esperanza, el color que siempre falta en mi arcoiris, el que me acompaña cuando huyo buscando escapar de otra crisis, corriendo al cesped siempre mas verde de mi vecino, persiguiendo tu luz tenue e ignorando el sol más fuerte que late en mi pecho.\nDicho y hecho. Voy a dejar de escuchar. Por una vez, será mi turno de hablar, o, mejor aún, de gritar, de que me escuchen encender las luces, de declararle a guerra a quienes dicen que es mejor que no sea yo mismo: voy a saltar al abismo, y se que, una vez abajo, le habré perdido el miedo al precipicio.\n📸 : Laura Lee on Unsplash\n#verde #green #poesiaenespañol #poesia #poetry\n","date":"February 22, 2021","hero":"/es/posts/poesia/verde/hero.jpg","permalink":"https://www.pablomarcos.me/es/posts/poesia/verde/","summary":"Sigo buscando el reflejo de tus ojos verdes, de esas dos esmeraldas de luz que siempre me miraron altivas como copa de pino, sigo luchando por seguir vivo, como una planta que escala desesperada las grietas de una cumbre borrascosa.\nSigo escuchando que el verde es el color de la esperanza, el color que siempre falta en mi arcoiris, el que me acompaña cuando huyo buscando escapar de otra crisis, corriendo al cesped siempre mas verde de mi vecino, persiguiendo tu luz tenue e ignorando el sol más fuerte que late en mi pecho.","tags":null,"title":"Verde"},{"categories":null,"contents":"No he hecho mucho este mes de enero, ya que ha sido el mes de los \u0026ldquo;parciales\u0026rdquo; (exámenes finales) en el INSA, y he estado muy liado estudiando. Sin embargo, encontré tiempo para escribir la solicitud, que he adjuntado a continuación como referencia, y pasé el poco tiempo que pude dedicar a la SoK aprendiendo a utilizar correctamente HUGO y qué plantillas tenía que editar para obtener un resultado determinado.\nSolicitud: Nuevo sitio web para Okular Okular es un programa multifacético que uso casi a diario para mis necesidades de lectura y anotación de PDFs, aunque puede hacer mucho más. Lamentablemente, su página web está un poco anticuada y no es mobile-friendly. Por lo tanto, propongo reescribir el sitio web de Okular utilizando HUGO, de forma similar a como se hizo con el sitio web principal de kde.org, y manteniendo la coherencia con otras aplicaciones de KDE como Konsole. Afortunadamente, parte del trabajo ya fue iniciado por Carl Schwan, por lo que sólo tendría que continuar y terminar su trabajo.\nObjetivos del proyecto La página web actualizada de Okular debe ser bonita, presentar la información de forma clara y ser fácil de usar. Algunas características deseadas son:\nFacilidad de uso en móviles Búsqueda en la página independiente de Google, por ejemplo, utilizando Lunar Search Actualización de la sección de capturas de pantalla (las capturas de pantalla me parecen anticuadas, pero tal vez es por que se tomaron en una plataforma diferente, como windows) Una interfaz más parecida a un blog para las noticias de desarrollo, tal vez manteniendo los registros de cambios dentro de la página web de Okular. Además, las características ya presentes en la página web actual, como el soporte multilingüe, deben ser preservadas.\nImplementación Como se ha dicho anteriormente, el trabajo ya fue iniciado por Carl Schwan. El proyecto utiliza el framework HUGO y algunos scripts de python para la i18n. A lo largo del proceso se recogerán los comentarios de los desarrolladores de Okular para garantizar que el nuevo sitio web se adapte a sus necesidades. En cuanto a la documentación, creo que una descripción básica de cómo hacer funcionar el sitio sería suficiente, ya que HUGO está produciendo, en última instancia, HTML, CSS y JS crudo.\nCalendario 13 Ene: Inicio del trabajo según la web de KDE 15 Ene - 24 Ene: Familiarización con la plataforma y aprendizaje de su funcionamiento interno. Dejo mucho tiempo aquí por si esto acaba siendo más difícil de lo que creo, pero posiblemente se pueda hacer en menos tiempo 24 de enero - 31 de enero: Última semana del semestre en INSA, probablemente tendré presentaciones y trabajo que hacer 1 de febrero - 12 de febrero: Tiempo de programar 13 de febrero - 21 de febrero: Vacaciones en Francia, si es posible debido al coronavirus podría ir de vacaciones, por lo que sería difícil programar ahora 22 de febrero - 28 de marzo: Tiempo de programar 29 de marzo - 4 de abril: El trabajo ya debería estar hecho, es hora de escribir un informe sobre cómo ha ido todo 5 de abril - 9 de abril: Última semana antes de las vacaciones en Francia, así que es posible que tenga trabajo que hacer en la uni 9 de abril: Fin del trabajo según la página web de KDE Sobre mí Soy un estudiante de biotecnología computacional de la UPMadrid que actualmente está haciendo un ERASMUS en el INSA de Lyon. Me encanta el código abierto, y uso Kubuntu -por lo tanto, KDE- como mi sistema operativo de elección, y me gustaría devolver a la comunidad aportando algo a cambio. Aunque trabajo principalmente con python (especialmente el paquete BioPython), he utilizado anteriormente HUGO para hacer mi sitio web personal, pablomarcos.me, donde se puede encontrar más información personal y mi CV completo. También he trabajado en otro proyecto con una UI orientada al usuario: Who\u0026rsquo;s that Function, un juego interactivo que enseña a los alumnos de primer grado las propiedades de las funciones, como parte de una beca de colaboración entre profesores y alumnos; aunque no está súper relacionado.\nMe siento completamente cómodo trabajando a distancia, como ya lo hice para el juego antes mencionado debido a las restricciones de COVID-19, y, aunque el inglés no es mi lengua materna, puedo escribirlo y hablarlo con fluidez. También puedo hablar español, mi lengua materna, y francés.\nP.D.: Además, en España, ¡había mucha nieve! Lo cual es super bonito :p aunque algunas personas (mi madre incluida) acabaron atrapadas en sus trabajos. Por si tenéis curiosidad, aquí tenéis una foto de la calle Alcalá de Madrid, cubierta de nieve por el temporal Borrasca Filomena. Javier Pérez Montes, CC BY-SA 4.0, vía Wikimedia Commons\n","date":"January 30, 2021","hero":"/es/posts/concursos/season-of-kde-2021/sok-report-january/hero.jpg","permalink":"https://www.pablomarcos.me/es/posts/concursos/season-of-kde-2021/sok-report-january/","summary":"No he hecho mucho este mes de enero, ya que ha sido el mes de los \u0026ldquo;parciales\u0026rdquo; (exámenes finales) en el INSA, y he estado muy liado estudiando. Sin embargo, encontré tiempo para escribir la solicitud, que he adjuntado a continuación como referencia, y pasé el poco tiempo que pude dedicar a la SoK aprendiendo a utilizar correctamente HUGO y qué plantillas tenía que editar para obtener un resultado determinado.","tags":null,"title":"SoK 2021 - Informe de Enero"},{"categories":null,"contents":"He participado en el I CONCURSO DE CARTELES del Centro de Lenguas de la UPM, cuyo tema ha sido: “Las lenguas extranjeras y tú: cómo han influido en tu vida”. Aunque no he ganado, me parece que me ha quedado un cartel mono y original, que es lo que quería; para otro año, revisaré mejor Unsplash antes de tomar decisiones 🤓\nHe aquí mi cartel:\n","date":"December 17, 2020","hero":"/es/posts/concursos/centrodelenguas/hero.jpg","permalink":"https://www.pablomarcos.me/es/posts/concursos/centrodelenguas/","summary":"He participado en el I CONCURSO DE CARTELES del Centro de Lenguas de la UPM, cuyo tema ha sido: “Las lenguas extranjeras y tú: cómo han influido en tu vida”. Aunque no he ganado, me parece que me ha quedado un cartel mono y original, que es lo que quería; para otro año, revisaré mejor Unsplash antes de tomar decisiones 🤓\nHe aquí mi cartel:","tags":null,"title":"Concurso de Carteles para el Centro de Lenguas UPM"},{"categories":null,"contents":"He sido finalista en el Concurso de Diseño de Etiquetas de Vino y Aceite de 2020 de la ETSIAAB, en el que participé con la ayuda de mi primo, Daniel Marcos, que me ayudó a diseñar la etiqueta de vino, y Marta Pérez, que me ayudó con la de aceite.\n","date":"December 8, 2020","hero":"/es/posts/concursos/vinoyaceite2020/hero.png","permalink":"https://www.pablomarcos.me/es/posts/concursos/vinoyaceite2020/","summary":"He sido finalista en el Concurso de Diseño de Etiquetas de Vino y Aceite de 2020 de la ETSIAAB, en el que participé con la ayuda de mi primo, Daniel Marcos, que me ayudó a diseñar la etiqueta de vino, y Marta Pérez, que me ayudó con la de aceite.","tags":null,"title":"Concurso de Etiquetas de Vino y Aceite de 2020 en la ETSIAAB"},{"categories":null,"contents":"Escúchame bien compañero del alma, que voy con prisa y mi tren sale al alba.\nNo eres mas débil por soñar, ni pesas menos si al andar, notas tus pies poco a poco despegar del suelo; es lo que tiene volar, que a veces te sientes como un polluelo.\nSé que te han hecho llorar, pero reirás el último si acortas el duelo: los que intentaron perforar tu piel desnuda con la daga fría de la duda te verán saltar y se quedarán de hielo.\nAlgún día abrazará tu corazón el tacto sedoso de una mañana inundada de sol, y cuando, por fin, mires hacia abajo sin temor, como no te des prisa, mi amor, yo no estaré ahí para quererte.\n📸 : CC-By-Sa 4.0 Daniel Marcos\n#pluma #feather #poesiaenespañol #poesia #poetry\nYou might have noticed que el domingo pasado se me olvidó publicar\u0026hellip; mala cosa, pero estaba contribuyendo al Mes de Asia en #Wikipedia!! Mi primo Daniel me pidió un texto para su publicación de Instagram, y, aunque al final le gustó otro que ya había publicado, le escribí este.\n","date":"December 6, 2020","hero":"/es/posts/poesia/pluma/hero.png","permalink":"https://www.pablomarcos.me/es/posts/poesia/pluma/","summary":"Escúchame bien compañero del alma, que voy con prisa y mi tren sale al alba.\nNo eres mas débil por soñar, ni pesas menos si al andar, notas tus pies poco a poco despegar del suelo; es lo que tiene volar, que a veces te sientes como un polluelo.\nSé que te han hecho llorar, pero reirás el último si acortas el duelo: los que intentaron perforar tu piel desnuda con la daga fría de la duda te verán saltar y se quedarán de hielo.","tags":null,"title":"Plumas"},{"categories":null,"contents":"Bueno bueno bueno\u0026hellip; Después de meses preocupado por los exámenes\u0026hellip; I\u0026rsquo;m back, bitches!! 🥳🥳 Como era eso? Miércoles un post de Estrechos de Miras 🤓 y Domingo uno recién horneado 🥐? Vamos allá!!\nBueno, ya se que hoy es miércoles, pero esta semana va a ser un poco (y por un poco, digamos más bien COMPLETAMENTE) diferente. Ya que lo he cogido con ganas, publico hoy el original y el domingo el refritillo 🍟\nOtra vez vuelvo a mancillar tu frío rostro con mis manos cansadas. Otra vez me siento arder en el fondo. Otra vez vuelves a ser el soporte de mis sentimientos, la tierra mágica que sale en los cuentos, la estrella que más brilla en el reino donde todo brilla.\nTe amo porque eres el corazón de mil poetas, el sonido alegre de una banda de cornetas, el éxito en ventas y los sueños de un loco. Pero eres también cristales rotos, aliado de dictadores asesinos y pedantes cansinos, una sentencia de muerte o la partida que da la vida.\nEres un ser omnipotente, y también un imperio decadente, sucio y limpio al mismo tiempo, una forma esmerada que flota en el viento, el mapa secreto de un pirata y la burocracia que nos atrapa. El papel nuestro de cada día.\n📸 : Annie Spratt on Unsplash\n#papel #paper #poesiaenespañol #poesia #poetry\n","date":"November 18, 2020","hero":"/es/posts/poesia/papel/hero.jpg","permalink":"https://www.pablomarcos.me/es/posts/poesia/papel/","summary":"Bueno bueno bueno\u0026hellip; Después de meses preocupado por los exámenes\u0026hellip; I\u0026rsquo;m back, bitches!! 🥳🥳 Como era eso? Miércoles un post de Estrechos de Miras 🤓 y Domingo uno recién horneado 🥐? Vamos allá!!\nBueno, ya se que hoy es miércoles, pero esta semana va a ser un poco (y por un poco, digamos más bien COMPLETAMENTE) diferente. Ya que lo he cogido con ganas, publico hoy el original y el domingo el refritillo 🍟","tags":null,"title":"Papel"},{"categories":null,"contents":"A veces me levanto y no sé muy bien qué hacer. Me encuentro perdido, como en un cuadro cubista, jugando a encajar en un círculo y a evitar cortarme con las aristas; intentando seguir tu pista entre el azul de la tinta y el rojo de mi corazón. Y así, busca buscando voy embarrando mi vista, me voy perdiendo en un cuadro vanguardista y monocromo, flotando en tanto rosa que me ahogo.\nCamino sonámbulo, apostando por una cama la bolsa y la vida, sabiendo siempre demasiado deprisa qué viene después; adicto, como el fantasma que deja tras de sí la heroína, a las páginas del calendario.\nY así, veo pasar poco a poco los años, agotado en esta carrera de fondo que sólo consigue ponerme histriónico; desgastándome en la lucha contra fobos, luchando por todos los que merezcan la pena; buscando los ecos de la chispa de emoción que prenda de nuevo este viejo motor de ignición que está a diez kilómetros de quedarse sin gasolina\n📸 : Anónimo en PXFuel, Free for Commercial Use en teoría; si alguien es el dueño del copyright de esta foto y pxfuel la está usando sin su permiso, por favor, contácteme.\n#simulacion #simulation #matrix #poesiaenespañol #poesia #poetry #estrechosdemiras\n","date":"May 14, 2020","hero":"/es/posts/poesia/simulaci%C3%B3n/hero.jpg","permalink":"https://www.pablomarcos.me/es/posts/poesia/simulaci%C3%B3n/","summary":"A veces me levanto y no sé muy bien qué hacer. Me encuentro perdido, como en un cuadro cubista, jugando a encajar en un círculo y a evitar cortarme con las aristas; intentando seguir tu pista entre el azul de la tinta y el rojo de mi corazón. Y así, busca buscando voy embarrando mi vista, me voy perdiendo en un cuadro vanguardista y monocromo, flotando en tanto rosa que me ahogo.","tags":null,"title":"A veces me levanto"},{"categories":null,"contents":"Soy egoista cuando sólo sueño con volver a verte, cuando otros sueñan con salvar su vida. Soy egoista cuando no me importa la pandemia, cuando quiero salir y cuando me harta la tragedia.\nSoy egoista cuando quiero robar de tus ojos una intensa mirada, aunque si con la mia se cruza, acabo siempre mal parada; y soy egoísta cuando sueño en secreto tocarte, romper el hechizo con mis ganas de follarte; amarte, aunque sea, de lejos y sin molestarte.\nSoy egoista cuando sueño con diamantes y otros no tienen para comer; y cuando como, y cuando odio, y cuando te echo de menos también; todos somos egoistas a veces: qué le vamos a hacer.\n📸 : La muerte del artista: su último amigo - Por Zygmunt Andrychiewicz (1861-1943) en Wikimedia Commons. Museo Nacional de Varsovia.\n#egoismo #selfishness #poesia #poetry\n","date":"May 3, 2020","hero":"/es/posts/poesia/egoismo/hero.jpg","permalink":"https://www.pablomarcos.me/es/posts/poesia/egoismo/","summary":"Soy egoista cuando sólo sueño con volver a verte, cuando otros sueñan con salvar su vida. Soy egoista cuando no me importa la pandemia, cuando quiero salir y cuando me harta la tragedia.\nSoy egoista cuando quiero robar de tus ojos una intensa mirada, aunque si con la mia se cruza, acabo siempre mal parada; y soy egoísta cuando sueño en secreto tocarte, romper el hechizo con mis ganas de follarte; amarte, aunque sea, de lejos y sin molestarte.","tags":null,"title":"Egoísmo"},{"categories":null,"contents":"If ants rose up, they would take over your room. They would march like silent sentinels, through their hidden passages and their obscure tunnels, down into the deepest realms of yourself.\nIf ants rose up, there\u0026rsquo;d be no windows or doors to shut; no chains, no pains, no frosts and no repellents; there\u0026rsquo;d be grains, and seeds, and those who opposed them would be at their knees.\nThis post was published disconnected on mastodon\nThe ants that queue playfully in your room, the ones that you find just searching for food; the queens, and the workers, and the males too: they are all secretely plotting your doom.\nIf ants rose up they\u0026rsquo;d be no way to stop them; they\u0026rsquo;d go unencumbered from the coach to the kitchen, and they\u0026rsquo;d finally have a way to fend for themselves. But hear me my friend, because there is something you must hear: there is a tiny little ant beating inside your chest.\n📸 : MD_JERRY on Unsplash\n#hormigas #ants #poesia #poetry\n","date":"April 26, 2020","hero":"/es/posts/poesia/if-ants-rose-up/hero.jpg","permalink":"https://www.pablomarcos.me/es/posts/poesia/if-ants-rose-up/","summary":"If ants rose up, they would take over your room. They would march like silent sentinels, through their hidden passages and their obscure tunnels, down into the deepest realms of yourself.\nIf ants rose up, there\u0026rsquo;d be no windows or doors to shut; no chains, no pains, no frosts and no repellents; there\u0026rsquo;d be grains, and seeds, and those who opposed them would be at their knees.\nThis post was published disconnected on mastodon","tags":null,"title":"If ants rose up"},{"categories":null,"contents":"¿Inspiración ardiente, artísticos deseos? ¡Lo siento, señora, yo no tengo nada de estro! ¡Que yo escribo como puedo, por inercia y por canguelo; porque quiero desahogarme, y gritar me da miedo!\nY al fin pensará la gente, que todos tienen su dolor; que me suba la bragueta y me ponga a mirar el televisor; que no por ser poeta, tienen que aguantar mi sermón; pero yo me escribo una cuarteta, que queda mucho más molón\nPues ya apenas late mi corazón arrítmico, ya no me queda tiempo para ponerme metafísico; ahora que he evitado el final trágico, sólo quiero sentirme único.\n📸 : ABC Television, Public domain, via Wikimedia Commons\n#estro #poesiaenespañol #poesia #poetry #estrechosdemiras\n","date":"April 20, 2020","hero":"/es/posts/poesia/estro/hero.jpg","permalink":"https://www.pablomarcos.me/es/posts/poesia/estro/","summary":"¿Inspiración ardiente, artísticos deseos? ¡Lo siento, señora, yo no tengo nada de estro! ¡Que yo escribo como puedo, por inercia y por canguelo; porque quiero desahogarme, y gritar me da miedo!\nY al fin pensará la gente, que todos tienen su dolor; que me suba la bragueta y me ponga a mirar el televisor; que no por ser poeta, tienen que aguantar mi sermón; pero yo me escribo una cuarteta, que queda mucho más molón","tags":null,"title":"Perdón, ¿qué es estro?"},{"categories":null,"contents":"El peligro de las cosas hermosas es clavarte las espinas contemplando las rosas. El peligro de llamarte tesoro es dormirme, como un dragón, en tus cabellos de oro, y no volverme a despertar. El peligro es ahogarme en el mar de tus jos de hechizante aguamarina, o en la curva misteriosamente fina que dibujan tu boca cuando te sonrojas. Una estatua de un ángel con cuerpo femenino, sentada en un cementerio con tonos otroñales\nEl peligro es que me atrapes en tus alas de marmol, que me que me encierren tus brazos como las ramas de un árbol; que me olvide de que eres más frío que el hielo, y que la belleza no te cura lo soberbio; que eres hermoso cual Adonis, pero no te aguanta ni Eris\nYo quiero volar con mis alas de cera, pero se derriten cuando voy, como alma en pena, imaginándote al claroscuro de una vela; claro cuando me llenas el alma entera, pero oscuro, negro como si no fuera de este mundo, cuando recuerdo tu interior más profundo.\n📸 : Veit Hammer on Unsplash\n#poesia #poetry #peligro #danger #poesiaenespañol\n","date":"April 19, 2020","hero":"/es/posts/poesia/peligro/hero.jpg","permalink":"https://www.pablomarcos.me/es/posts/poesia/peligro/","summary":"El peligro de las cosas hermosas es clavarte las espinas contemplando las rosas. El peligro de llamarte tesoro es dormirme, como un dragón, en tus cabellos de oro, y no volverme a despertar. El peligro es ahogarme en el mar de tus jos de hechizante aguamarina, o en la curva misteriosamente fina que dibujan tu boca cuando te sonrojas. Una estatua de un ángel con cuerpo femenino, sentada en un cementerio con tonos otroñales","tags":null,"title":"Colisión Inminente"},{"categories":null,"contents":"No he durado ni un año viviendo pleno de mí mismo, susurrando en secreto y descuidado mi deseo de recuperar la locura del pasado; ha vuelto el tornado que siempre gira, que me enseña que este corazón que creía quemado aún guarda en su interior una cosecha tal vez mayor que la del mejor abril. Y yo, preso del hechizo del inconformismo, me consumo por volver al embrujo de la música disco, por soñar con los ojos abiertos con la tranquilidad de que el mundo sigue en paz y nada, absolutamente nada, ha cambiado.\nNo he durado ni un año y ya siento cómo se rompen las barricadas, cómo todas mis defensas se ahogan en esta marea que no perdona nada, cómo se desbocan las emociones y se diluyen las precauciones en esta tormenta que sólo se sirve a sí misma.\nOjalá, encontrarle sentido a mi ingenuo organismo. Ojalá romper las cadenas que me atan a mí mismo. Ojalá aprender a no ignorar las señales del abismo. Ojalá desviar el curso, y no lanzarme de nuevo a los brazos del espejismo.\n📸 : Foto de Daniel Park en Unsplash\n#ojalá #hopefully #poesiaenespañol #poesia #poetry #estrechosdemiras\n","date":"April 3, 2020","hero":"/es/posts/poesia/ojala/hero.jpg","permalink":"https://www.pablomarcos.me/es/posts/poesia/ojala/","summary":"No he durado ni un año viviendo pleno de mí mismo, susurrando en secreto y descuidado mi deseo de recuperar la locura del pasado; ha vuelto el tornado que siempre gira, que me enseña que este corazón que creía quemado aún guarda en su interior una cosecha tal vez mayor que la del mejor abril. Y yo, preso del hechizo del inconformismo, me consumo por volver al embrujo de la música disco, por soñar con los ojos abiertos con la tranquilidad de que el mundo sigue en paz y nada, absolutamente nada, ha cambiado.","tags":null,"title":"En círculos"},{"categories":null,"contents":"Contexto: el 22 de marzo de 2020, publiqué:\nBueno, debido a la #cuarentena por el #coronavirus, he decidido cambiar mi meta de publicar contenido original dos veces al mes a una a la semana (🎉 🎉). Así, los miércoles subiré contenido de Estrechos de Miras, como he venido haciendo para generar un poco de \u0026ldquo;fondo de armario\u0026rdquo;, y, los domingos, algo más recientito 🥐\nA ver que tal se me da 😆\n#writing #poetry #poemas #poesia #poesiaenespañol\nAsí, el 29 de marzo publicaba que:\nBueno, pues primer domingo, primera en la frente jeje. No creo que me de tiempo a publicar un poema original hoy (😅 ) pero, a cambio, os dejo esta página de Wikipedia súper interesante en la que he estado trabajando.\nhttps://es.wikipedia.org/wiki/Centinelas_Silenciosas\nSon las Centinelas Silenciosas, un grupo de valientes mujeres que lucharon por sus derechos contra viento y marea. Una historia hermosa e inspirante de coraje contra la opresión.\n📸 : Harris \u0026amp; Ewing en W Commons\n#feminismo #feminism #queseriadenosotras\n","date":"March 29, 2020","hero":"/es/posts/poesia/sufragettes/hero.jpg","permalink":"https://www.pablomarcos.me/es/posts/poesia/sufragettes/","summary":"Contexto: el 22 de marzo de 2020, publiqué:\nBueno, debido a la #cuarentena por el #coronavirus, he decidido cambiar mi meta de publicar contenido original dos veces al mes a una a la semana (🎉 🎉). Así, los miércoles subiré contenido de Estrechos de Miras, como he venido haciendo para generar un poco de \u0026ldquo;fondo de armario\u0026rdquo;, y, los domingos, algo más recientito 🥐\nA ver que tal se me da 😆","tags":null,"title":"Sufragettes"},{"categories":null,"contents":"Hace unos años me asustaba el otoño; ahora, soy invierno. Hace unos años, temía quedarme sólo; ahora, canto el himno sonoro de la libertad. Con un clavo te ha desclavado Pablito, yo que estaba loquito por tus huesos; y ya no son tus besos, las nubes de cristal roto con que me corto. He cogido el espejo y he reconstruido poco a poco la portada de un nuevo capítulo de este loco libro de aventuras en que ya no me ahogo.\n\u0026ldquo;No me mires con tus ojos, que son luz, y yo, que soy sombra de tu sombra, me desvanezco\u0026rdquo;, decía; pero luego, temeroso y hambriento, me puse las gafas violetas para entender lo que siento, y ya no hay eclipse que no se me quede pequeño ni luz que brille más que la que llevo dentro.\nMe he cambiado de provedor de sueños, y ahora por fin entiendo, que el amor no se vive sufriendo y que la vida, poco a poco se va viviendo.\nInspirado por un verso de Mario Benedetti\n📸 : Andre Benz on Unsplash\n#poesia #poetry #invierno #winter\n","date":"March 22, 2020","hero":"/es/posts/poesia/invierno/hero.jpg","permalink":"https://www.pablomarcos.me/es/posts/poesia/invierno/","summary":"Hace unos años me asustaba el otoño; ahora, soy invierno. Hace unos años, temía quedarme sólo; ahora, canto el himno sonoro de la libertad. Con un clavo te ha desclavado Pablito, yo que estaba loquito por tus huesos; y ya no son tus besos, las nubes de cristal roto con que me corto. He cogido el espejo y he reconstruido poco a poco la portada de un nuevo capítulo de este loco libro de aventuras en que ya no me ahogo.","tags":null,"title":"Ahora soy Invierno"},{"categories":null,"contents":"Ok, so: There\u0026rsquo;s nothing cuter than @davidrevoy \u0026rsquo;s cat generator! You should definetely give it a try! You can see mine below hehe.\nhttps://www.peppercarrot.com/extras/html/2016_cat-generator/index.php?seed=seedname\nFound it via @mmu_man\n#art #cuteness #cats #CatsOfMastodon\n","date":"March 17, 2020","hero":"/es/posts/poesia/david-revoy/hero.png","permalink":"https://www.pablomarcos.me/es/posts/poesia/david-revoy/","summary":"Ok, so: There\u0026rsquo;s nothing cuter than @davidrevoy \u0026rsquo;s cat generator! You should definetely give it a try! You can see mine below hehe.\nhttps://www.peppercarrot.com/extras/html/2016_cat-generator/index.php?seed=seedname\nFound it via @mmu_man\n#art #cuteness #cats #CatsOfMastodon","tags":null,"title":"David Revoy's Cats"},{"categories":null,"contents":"¡Que empiece la metamorfosis! Dice el hombre trajeado, y con pulsar un botón empiezan a tocar al son más de cien violonchelistas; es el jefe el amo de la pista, y recorta con tesón las caras largas; se edulcoran las amargas, y comienza la reconversión para hacer que la explotación parezca ahora lo mejor.\n¡Ahora toca celebrar! Dicen con aire triunfal; suben las ventas, crecen las cuitas, recortan en gastos de personal; total, como a nadie le importa, subsiste sólo el capital. Pero si no se puede cerrar la boca, si no se para de deforestar, no habrá papel para hacer billetes, ni dinero que ganar.\n📸 : Diego Delso @ delso.photo, CC-By-Sa 4.0 en Wikimedia Commons.\n#metamorfosis #metamorphosis #poesiaenespañol #poesia #poetry #estrechosdemiras\nImagen completa:\n","date":"March 16, 2020","hero":"/es/posts/poesia/metamorfosis/hero.jpg","permalink":"https://www.pablomarcos.me/es/posts/poesia/metamorfosis/","summary":"¡Que empiece la metamorfosis! Dice el hombre trajeado, y con pulsar un botón empiezan a tocar al son más de cien violonchelistas; es el jefe el amo de la pista, y recorta con tesón las caras largas; se edulcoran las amargas, y comienza la reconversión para hacer que la explotación parezca ahora lo mejor.\n¡Ahora toca celebrar! Dicen con aire triunfal; suben las ventas, crecen las cuitas, recortan en gastos de personal; total, como a nadie le importa, subsiste sólo el capital.","tags":null,"title":"¡Que empiece la metamorfosis!"},{"categories":null,"contents":"Fugaz, por un momento, pasó por su mente el extraño pensamiento. Un remanso de paz, un lugar para descansar, un eternamente despreciado paraíso calmo y pleno de felicidad.\nComo el soma escurriendo por sus venas notó por fin cómo se soltaban las cadenas; cómo la jeringuilla que entró con un pinchazo arrancaba el veneno de serpiente de sus brazos; cómo sus párpados se cerraban, saludando al trance y al olvido de otra brevísima escapada; cómo sanaban las cicatrices de las alas arrancadas; y cómo, por fin, podría dormir sin que nadie lo molestara\n📸 : 0x010C, CC-By-Sa 4.0 en Wikimedia Commons.\n#fugaz #fleeting #poesiaenespañol #poesia #poetry #estrechosdemiras\n","date":"February 26, 2020","hero":"/es/posts/poesia/fugaz/hero.jpg","permalink":"https://www.pablomarcos.me/es/posts/poesia/fugaz/","summary":"Fugaz, por un momento, pasó por su mente el extraño pensamiento. Un remanso de paz, un lugar para descansar, un eternamente despreciado paraíso calmo y pleno de felicidad.\nComo el soma escurriendo por sus venas notó por fin cómo se soltaban las cadenas; cómo la jeringuilla que entró con un pinchazo arrancaba el veneno de serpiente de sus brazos; cómo sus párpados se cerraban, saludando al trance y al olvido de otra brevísima escapada; cómo sanaban las cicatrices de las alas arrancadas; y cómo, por fin, podría dormir sin que nadie lo molestara","tags":null,"title":"Locus amoenus"},{"categories":null,"contents":"Algunos las esconden como un cervatillo tímido en mitad del bosque; otros las llevamos expuestas, como trofeos de caza arrebatados mano a mano a la sociedad; la mayoría intentan limarlas, o cortarlas sin mirar atrás.\nEs doloroso ser diferente, aunque no lo entiendan los hombres grises; incluso entre ellos, unos son más iguales que otros, unos lanzan y otros atrapan toneladas de suave propaganda, una nube cálida de humo que te abraza en su sueño de consumismo y perfección.\nPero el ser humano es bizco o tuerto, loco y descolorido; y el sexo es sucio, y la piel llena de granos y de pelos. Pues no somos perfectos, pero somos auténticos.\n📸 : Foto de AxelHH, Public Domain en Wikimedia Commons, sobre una estatua de Ulrike Enders (Uy, que no tiene página en Wikipedia en Español\u0026hellip; tendré que crearla yo 😏😏) en Hannover\n#rarezas #imperfections #poesiaenespañol #poesia #poetry #estrechosdemiras\n","date":"February 14, 2020","hero":"/es/posts/poesia/rarezas/hero.jpg","permalink":"https://www.pablomarcos.me/es/posts/poesia/rarezas/","summary":"Algunos las esconden como un cervatillo tímido en mitad del bosque; otros las llevamos expuestas, como trofeos de caza arrebatados mano a mano a la sociedad; la mayoría intentan limarlas, o cortarlas sin mirar atrás.\nEs doloroso ser diferente, aunque no lo entiendan los hombres grises; incluso entre ellos, unos son más iguales que otros, unos lanzan y otros atrapan toneladas de suave propaganda, una nube cálida de humo que te abraza en su sueño de consumismo y perfección.","tags":null,"title":"Imperfectos"},{"categories":null,"contents":"Tampoco sé muy bien de qué escribir. Me pasa siempre, llámalo síndrome de la página en blanco o llámalo pereza, pero sigo recurriendo a los mismos tópicos y las mismas experiencias dolorosas que echo de menos como un masoquista. Y es que siempre estoy buscando una pista, un clavo ardiendo al que aferrarme para, a pesar de quemarme, sentir que estoy vivo\nPorque ahora soy libre y tengo síndrome de Estocolmo; porque sólo escribo bien cuando estoy hasta el coño; porque una parte de mí quiere escapar de este remanso de paz en que la otra le tiene secuestrado. Porque quiero volver a arder, quiero echarlo todo a perder, desvanecerme entre las sombras; y, sin embargo, sé que sigo mejor en el sofá.\n📸 : Franz Reichelt, 1912, Anónimo en Wikimedia Commons\n#incertidumbre #uncertainty #poesiaenespañol #poesia #poetry #estrechosdemiras\n","date":"January 31, 2020","hero":"/es/posts/poesia/incertidumbre/hero.png","permalink":"https://www.pablomarcos.me/es/posts/poesia/incertidumbre/","summary":"Tampoco sé muy bien de qué escribir. Me pasa siempre, llámalo síndrome de la página en blanco o llámalo pereza, pero sigo recurriendo a los mismos tópicos y las mismas experiencias dolorosas que echo de menos como un masoquista. Y es que siempre estoy buscando una pista, un clavo ardiendo al que aferrarme para, a pesar de quemarme, sentir que estoy vivo\nPorque ahora soy libre y tengo síndrome de Estocolmo; porque sólo escribo bien cuando estoy hasta el coño; porque una parte de mí quiere escapar de este remanso de paz en que la otra le tiene secuestrado.","tags":null,"title":"A punto de escapar"},{"categories":null,"contents":"Llevaba años encerrado en una cárcel sin color, donde tu estancia y tu ausencia ejercían de blanco y negro; una cárcel que llené de mi inocencia como lágrimas de plata y que tú conquistaste, una jaula flotante que acaba de poner los pies en el suelo.\nSiempre quise arrancarte como se arranca el hierro de una herida, pero las cadenas no me dejaban luchar; pero, ahora que mis ojos se acostumbran a la oscuridad y me atrevo a sacar una cerilla para protegerme del frío; ahora, por fin, he aprendido a volar.\nAhora sí, he dejado atrás todo; ahora sí, me he comido a mis miedos, fuegos que parecían consumirme y que al final cabían en una cáscara de nuez.\nEs tiempo de despedida. Este texto es un adiós, y no el suicidio estúpido de dos amantes con problemas de comunicación, sino la muerte tranquila de un rey legendario que da paso a una nueva era. Porque paso del drama. Porque en este nuevo horizonte, sólo me queda abrir las alas.\n📸 : Photo by Samuel Zeller on Unsplash\n#horizontes #horizons #poesiaenespañol #poesia #poetry #estrechosdemiras\n","date":"January 15, 2020","hero":"/es/posts/poesia/horizontes/hero.jpg","permalink":"https://www.pablomarcos.me/es/posts/poesia/horizontes/","summary":"Llevaba años encerrado en una cárcel sin color, donde tu estancia y tu ausencia ejercían de blanco y negro; una cárcel que llené de mi inocencia como lágrimas de plata y que tú conquistaste, una jaula flotante que acaba de poner los pies en el suelo.\nSiempre quise arrancarte como se arranca el hierro de una herida, pero las cadenas no me dejaban luchar; pero, ahora que mis ojos se acostumbran a la oscuridad y me atrevo a sacar una cerilla para protegerme del frío; ahora, por fin, he aprendido a volar.","tags":null,"title":"Tiempo de despedida"},{"categories":null,"contents":"Mi último paseo terminó huyendo de nuevo de tu fantasma, sintiendo la llamada de la destrucción y notando el cosquilleo de la adrenalina que me suplica esconderme. No existen para mí los paseos tranquilos, las noches de luna o el aire puro, pues vivo atormentado por puñaladas abiertas que los demás sabrían coserse mientras yo no puedo ni hacerme un torniquete\nY, así, como el cuento de hadas se ha tornado pesadilla, ahora las encinas se convierten en espinos, cortándome el paso y clavándose poco a poco; los ríos son “meaos” como el Manzanares, el rojo de las nubes al atardecer solo me evoca recuerdos de Vietnam, y me muero por volver a esconder mi cabeza entre las sábanas como un niño asustado\n📸 : Samuel Zeller on Unsplash\n#gingko #poesiaenespañol #poesia #poetry #estrechosdemiras\n","date":"December 18, 2019","hero":"/es/posts/poesia/ginko/hero.jpg","permalink":"https://www.pablomarcos.me/es/posts/poesia/ginko/","summary":"Mi último paseo terminó huyendo de nuevo de tu fantasma, sintiendo la llamada de la destrucción y notando el cosquilleo de la adrenalina que me suplica esconderme. No existen para mí los paseos tranquilos, las noches de luna o el aire puro, pues vivo atormentado por puñaladas abiertas que los demás sabrían coserse mientras yo no puedo ni hacerme un torniquete\nY, así, como el cuento de hadas se ha tornado pesadilla, ahora las encinas se convierten en espinos, cortándome el paso y clavándose poco a poco; los ríos son “meaos” como el Manzanares, el rojo de las nubes al atardecer solo me evoca recuerdos de Vietnam, y me muero por volver a esconder mi cabeza entre las sábanas como un niño asustado","tags":null,"title":"Camino a la desesperación"},{"categories":null,"contents":"Cada día al levantarme veo junto a mi cama un narciso; un ramo de bellas flores mientras yo, solo e indeciso, me contento con ser la sombra triste de mí mismo. Me veo en el espejo y me enamora mi reflejo afable y juguetón, lleno de inocencia; pero huyo en cuanto empiezan a aparecer, en los bordes, las tres furias de mi consciencia, salvándome con su dolor de arrojarme en el furor de tus brazos y olvidarme de mi nombre.\nY así, dispuesto a encontrar un clavo con el que sacarte, me hechizo con aromas asfixiantes y salgo a buscar una estrella rutilante con la que compartir mi fuego.\n📸 : El dormitorio en Arlés (1ª versión) Licencia: Dominio Público Autor: Vincent Willem van Gogh\n#narciso #daffodil #narcissus #poesiaenespañol #poesia #poetry #estrechosdemiras\n","date":"November 29, 2019","hero":"/es/posts/poesia/narciso/hero.jpg","permalink":"https://www.pablomarcos.me/es/posts/poesia/narciso/","summary":"Cada día al levantarme veo junto a mi cama un narciso; un ramo de bellas flores mientras yo, solo e indeciso, me contento con ser la sombra triste de mí mismo. Me veo en el espejo y me enamora mi reflejo afable y juguetón, lleno de inocencia; pero huyo en cuanto empiezan a aparecer, en los bordes, las tres furias de mi consciencia, salvándome con su dolor de arrojarme en el furor de tus brazos y olvidarme de mi nombre.","tags":null,"title":"Narciso, me bajo a bailar"},{"categories":null,"contents":"No me arrepiento de cada mordisco que le pegué a la maldita manzana, aún cuando descubro en tus labios de escarlata la encarnación del mismo fruto del pecado. Como una urraca deseo mirarte a los ojos, brillantes como piedras iridiscentes recién vomitadas por este volcán de lujuria en que me conviertes, loco por volver a verte. Me tientas por cosas que no puedo decir, aunque facebook no las censure por no ser de mujer; me tienta tu carisma, tu sonrisa, quiero perderme en una conversación contigo y no encuentro el valor para empezar a hablar; me siento presa de un hechizo de silencio y temo convertirme en espuma de mar\n📸 : ‘David’ by Michelangelo Licencia: CC-By-Sa 3.0 Unported Autor: Jörg Bittner Unna\n#tentación #temptation #poesiaenespañol #poesia #poetry #estrechosdemiras\n","date":"October 14, 2019","hero":"/es/posts/poesia/tentacion/hero.jpg","permalink":"https://www.pablomarcos.me/es/posts/poesia/tentacion/","summary":"No me arrepiento de cada mordisco que le pegué a la maldita manzana, aún cuando descubro en tus labios de escarlata la encarnación del mismo fruto del pecado. Como una urraca deseo mirarte a los ojos, brillantes como piedras iridiscentes recién vomitadas por este volcán de lujuria en que me conviertes, loco por volver a verte. Me tientas por cosas que no puedo decir, aunque facebook no las censure por no ser de mujer; me tienta tu carisma, tu sonrisa, quiero perderme en una conversación contigo y no encuentro el valor para empezar a hablar; me siento presa de un hechizo de silencio y temo convertirme en espuma de mar","tags":null,"title":"No me arrepiento"},{"categories":null,"contents":"Es fácil llamar hipócrita a quien quiere que cambien las cosas, señalar que vivimos en un mundo de tinieblas mientras soñamos con la luz, recordar que los que defiendes no se pueden permitir esa relaxing cup of café con leche que disfrutas en una terraza; pero más difícil es luchar por ellos.\nLa hipocresía se usa como arma, y te atacan más quienes menos creen en tu causa, pues es más fácil quejarse cuando no tienes nada que defender. Vivimos todas en un palacio de cristal, confiriendo las llaves de nuestra jaula dorada al fantasma de la indiferencia, y quien despierta del sueño es señalado para volverlo a dormir.\n📸 : The modern devil, his play between the false and the good Licencia: Dominio Público Autor: Isaiah Mench\n#hipocresía #hypocrisy #poesiaenespañol #poesia #poetry #estrechosdemiras\nImagen completa:\n","date":"September 18, 2019","hero":"/es/posts/poesia/hipocresia/hero.png","permalink":"https://www.pablomarcos.me/es/posts/poesia/hipocresia/","summary":"Es fácil llamar hipócrita a quien quiere que cambien las cosas, señalar que vivimos en un mundo de tinieblas mientras soñamos con la luz, recordar que los que defiendes no se pueden permitir esa relaxing cup of café con leche que disfrutas en una terraza; pero más difícil es luchar por ellos.\nLa hipocresía se usa como arma, y te atacan más quienes menos creen en tu causa, pues es más fácil quejarse cuando no tienes nada que defender.","tags":null,"title":"Total, si luego compras en el Zara…"},{"categories":null,"contents":" Para este poema, en Estrechos de Miras nos propusieron tema libre, o, \u0026ldquo;miscelánea\u0026rdquo;: el que quisiéramos. Yo la palabra que elegí fue \u0026ldquo;pájaro\u0026rdquo; Ayer se coló un pájaro en mi balcón, y, perdido en mi caja de cristal, sólo quería escapar. Aleteaba rápido como una rapaz, elegante como un colibrí; buscaba encontrar al sol, y no podía quedarse aquí.\nAyer se coló un pájaro en mi balcón, y ni mis palabras ni mis mimos le hacían entrar en razón. Tenía miedo de mí, de mis aires de adulto y mi mente de niño; Tenía miedo de mí, que también quiero volar y no supero el precipicio.\nAyer se coló un pájaro en mi balcón; le abrí las ventanas y salió. Ayer se coló un pájaro en mi balcón, y ese pájaro era yo.\n📸 : Green Violetear by Sas Cuyvers en Wikimedia Commons\n#pájaro #bird #poesiaenespañol #poesia #poetry #estrechosdemiras\n","date":"September 3, 2019","hero":"/es/posts/poesia/pajaro/hero.jpg","permalink":"https://www.pablomarcos.me/es/posts/poesia/pajaro/","summary":"Para este poema, en Estrechos de Miras nos propusieron tema libre, o, \u0026ldquo;miscelánea\u0026rdquo;: el que quisiéramos. Yo la palabra que elegí fue \u0026ldquo;pájaro\u0026rdquo; Ayer se coló un pájaro en mi balcón, y, perdido en mi caja de cristal, sólo quería escapar. Aleteaba rápido como una rapaz, elegante como un colibrí; buscaba encontrar al sol, y no podía quedarse aquí.\nAyer se coló un pájaro en mi balcón, y ni mis palabras ni mis mimos le hacían entrar en razón.","tags":null,"title":"Un pájaro en mi balcón"},{"categories":null,"contents":"Esta primavera me noto renacer, salvaje como un minotauro, fuerte como la insignificante semilla de mostaza, duro como un diente de león que crece donde puede, pero ligero como un vilano al desplegar las alas. Noto la sangre desbocarse en mis venas como los caballos de Nemea, henchido de poder como Sauron con su anillo, libre para ejercer mi voluntad.\nMe he hipotecado en mil años de tiempo prestado, te he llamado musa y he sido museo, y, ciego como Tiresias, sólo escuchaba tu voz en un lago de amnesia. Pero ya se acabó. Por fin le he devuelto el fuego a Afrodita, pues no lo necesito; ahora, gracias a Prometeo, me puedo calentar solo, y aguardo con deseo las doce pruebas que se me pongan por delante.\nTema: Mitología\n📸 : Apollo and Daphne (Bernini) Licencia: CC-By-Sa 4.0 Autor: Architas en Wikimedia Commons Texto de la licencia: https://creativecommons.org/licenses/by-sa/4.0/deed.en\n#mitologia #mythology #poesiaenespañol #poesia #poetry #estrechosdemiras\n","date":"August 21, 2019","hero":"/es/posts/poesia/mitologia/hero.jpg","permalink":"https://www.pablomarcos.me/es/posts/poesia/mitologia/","summary":"Esta primavera me noto renacer, salvaje como un minotauro, fuerte como la insignificante semilla de mostaza, duro como un diente de león que crece donde puede, pero ligero como un vilano al desplegar las alas. Noto la sangre desbocarse en mis venas como los caballos de Nemea, henchido de poder como Sauron con su anillo, libre para ejercer mi voluntad.\nMe he hipotecado en mil años de tiempo prestado, te he llamado musa y he sido museo, y, ciego como Tiresias, sólo escuchaba tu voz en un lago de amnesia.","tags":null,"title":"Por fin, la Primavera"},{"categories":null,"contents":"Simetría, que bonito cuando no te falta un pecho y estás tuerto de un ojo. Qué fácil es esculpir el mármol a capricho del corazón, y qué dura es la piel, repleta de negros puntos y aparte escritos con tinta imborrable. No son simétricos los ríos, ni el viento, ni la chispa clamorosa de un fósforo que ilumina donde la esperanza se desvanece.\nSimetría es la muerte, el silencio y el vacío, libres de imperfecciones y de locura, libres de sueños resbalosos como agua jabonosa. Simétrico es el espejo en que me miro y que no me atrevo a cruzar por miedo de cortarme, y simétrico es el cincel con el que intento esculpir en mi máscara una sonrisa, aun a costa de las impurezas que me dan la vida.\nTema: Simetría\n📸 : Front views of the Venus de Milo, Cc-By 4.0 by Livioandronico2013, Wikimedia Commons Que por cierto, ¡ha sido baneado de commons! I am kinda shocked hehe, sobre todo porque me he enterado buscanodo el original para máxima calidad jeje.\n#simetria #poesiaenespañol #poesia #poetry #estrechosdemiras\n","date":"August 10, 2019","hero":"/es/posts/poesia/simetria/hero.jpg","permalink":"https://www.pablomarcos.me/es/posts/poesia/simetria/","summary":"Simetría, que bonito cuando no te falta un pecho y estás tuerto de un ojo. Qué fácil es esculpir el mármol a capricho del corazón, y qué dura es la piel, repleta de negros puntos y aparte escritos con tinta imborrable. No son simétricos los ríos, ni el viento, ni la chispa clamorosa de un fósforo que ilumina donde la esperanza se desvanece.\nSimetría es la muerte, el silencio y el vacío, libres de imperfecciones y de locura, libres de sueños resbalosos como agua jabonosa.","tags":null,"title":"Asimétrico"},{"categories":null,"contents":"Es de noche y no hay luna, y si no hay luna no hay luz; ni luz ni antorcha ilumina el cuerpo que esconden en su ataúd.\n¡En el día del eclipse, en la noche de las brujas! Ahí se citan los tres para llenar de oro las alforjas Son un cura y un panadero, y los sigue un timador; Vienen a buscar dinero para comprar algo de calor.\nEl uno escala la tapia, el otro vigila el jardín; les abre la puerta el tercero, que ha conquistado al edil. Van rondando la casa, buscando llenar su saco; sólo encuentran muebles de ikea, un poäng y un sollerön.\nCuando llegan al despacho, donde guarda lo de valor el edil está esperando: ha descubierto al traidor. Entre todos le dan muerte, por no dejar relator; si les preguntan quién lo hizo, ¡Fuenteovejuna, señor!\nTema: Eclipse\nTítulo: La noche de las Brujas\n📸 : Black hole - Messier 87 crop max res by the European Southern Observatory (ESO) on Wikimedia Commons - CC-By-Sa 4.0\n#eclipse #poesiaenespañol #poesia #poetry #estrechosdemiras\n","date":"July 23, 2019","hero":"/es/posts/poesia/eclipse/hero.png","permalink":"https://www.pablomarcos.me/es/posts/poesia/eclipse/","summary":"Es de noche y no hay luna, y si no hay luna no hay luz; ni luz ni antorcha ilumina el cuerpo que esconden en su ataúd.\n¡En el día del eclipse, en la noche de las brujas! Ahí se citan los tres para llenar de oro las alforjas Son un cura y un panadero, y los sigue un timador; Vienen a buscar dinero para comprar algo de calor.\nEl uno escala la tapia, el otro vigila el jardín; les abre la puerta el tercero, que ha conquistado al edil.","tags":null,"title":"La noche de las Brujas"},{"categories":null,"contents":"¿Qué somos sino una volátil brisa que se enrolla, bailarina, entre las ramas espinadas del olvido?\n¿Qué es la vida, sino un fastuoso juego de la oca, donde al que más cambie más le toca, y quien no evoluciona muere o abandona?\n¿Qué soy yo sino una catedral de cristal, que tiembla cada vez que siente llegar el último quejumbroso susurro de un corazón roto, un templo de paredes desnudas e interior marchito cuyos muros de carga no quieren sino yacer en ruinas?\n¿Qué es eso de la coherencia, si tú también cambiarías tus cimientos por veinte monedas de plata? ¿Qué significa la estabilidad si este verso se convierte en espuma de mar en cuanto sale de tus labios?\nTema: Efímero\n📸 : James Eades on Unsplash\n#efimero #ephemeral #poesiaenespañol #poesia #poetry #estrechosdemiras\n","date":"July 22, 2019","hero":"/es/posts/poesia/efimero/hero.jpg","permalink":"https://www.pablomarcos.me/es/posts/poesia/efimero/","summary":"¿Qué somos sino una volátil brisa que se enrolla, bailarina, entre las ramas espinadas del olvido?\n¿Qué es la vida, sino un fastuoso juego de la oca, donde al que más cambie más le toca, y quien no evoluciona muere o abandona?\n¿Qué soy yo sino una catedral de cristal, que tiembla cada vez que siente llegar el último quejumbroso susurro de un corazón roto, un templo de paredes desnudas e interior marchito cuyos muros de carga no quieren sino yacer en ruinas?","tags":null,"title":"Efímero"},{"categories":null,"contents":"Ya llega, ya noto a la primavera galopar, ya noto su asta taladrándome las venas y evaporando mi sangre. Ya me noto loco, perdido de nuevo, deseoso de ahuyar, aunque sea a la luna; y deseoso de amar, aunque sea a mis demonios.\nEn estos días de primavera oigo voces en mi corazón, resoplidos a punto de estallar cuando me baño con las sales perfumadas del ayer porque me faltan tus brazos.\nPorque mi pobre corazón está loco y mi razón, cegada por el vaho en los cristales empañados del invierno, sólo hace que seguirlo, mientras los dos, atrapado yo en esta prisión, me buscan la ruina.\nTítulo: Ya llega\nTema: Cuando los ciegos guían a los locos\n📸 : Paolo Nicolello on Unsplash\n#crazy #loco #poesiaenespañol #poesia #poetry #estrechosdemiras\nAs a sidenote, he de decir que ciertamente el título resulta un poco capacitista, pero yo no lo elegí y, de algún modo, es parte de mi historia\n","date":"June 27, 2019","hero":"/es/posts/poesia/locos/hero.png","permalink":"https://www.pablomarcos.me/es/posts/poesia/locos/","summary":"Ya llega, ya noto a la primavera galopar, ya noto su asta taladrándome las venas y evaporando mi sangre. Ya me noto loco, perdido de nuevo, deseoso de ahuyar, aunque sea a la luna; y deseoso de amar, aunque sea a mis demonios.\nEn estos días de primavera oigo voces en mi corazón, resoplidos a punto de estallar cuando me baño con las sales perfumadas del ayer porque me faltan tus brazos.","tags":null,"title":"Cuando los Ciegos guían a los Locos"},{"categories":null,"contents":"Cuando escribimos este poema para Estrechos de Miras, la idea era competir por servir de soporte narrativo a este video de youtube:\nLa imagen (📸 : Avinash Kumar on Unsplash) es en este caso un cutre juego de palabras, pero así lo hice también en el original 😂 😂\nHe aquí el (mini) texto:\nSilencio. Explotan poco a poco las granadas a mi alrededor, como cristales de sal en este erial sin sentimiento. Son las doce en el reloj, y, en vez de campanadas, suenan los cañones de la octava compañía. Una granada, que antes guardaba las lluvias de abril para los meses de estío, sirve sólo ahora de juguete antiestrés. Sólo rezo porque no me encuentren. Porque no caiga la siguiente bomba. Sólo pido piedad. Solo grito: no a la guerra\n#granada #grenade #poesiaenespañol #estrechosdemiras\n","date":"June 12, 2019","hero":"/es/posts/poesia/granada/hero.jpg","permalink":"https://www.pablomarcos.me/es/posts/poesia/granada/","summary":"Cuando escribimos este poema para Estrechos de Miras, la idea era competir por servir de soporte narrativo a este video de youtube:\nLa imagen (📸 : Avinash Kumar on Unsplash) es en este caso un cutre juego de palabras, pero así lo hice también en el original 😂 😂\nHe aquí el (mini) texto:\nSilencio. Explotan poco a poco las granadas a mi alrededor, como cristales de sal en este erial sin sentimiento.","tags":null,"title":"Granada"},{"categories":null,"contents":"Son las doce en el viejo reloj de oro. Vestida de gala y ornato, se pavonea entre bailes la alta sociedad. Purpurina, lentejuelas, plumas; Oh la lá!, si levantan las piernas las chicas del cabaret. Risas, champán, amor, qué bien, mon dieu. La noche es joven, y nosotros, también.\nSon las seis en el viejo reloj de oro. Vómitos, resaca y mal olor; los que pueden irse a su casa se van, y los que no, emporrados en el sofá. Se escucha un inocente grito:\n-¡No lo volveré a hacer más!\n📸 : Amy Shamblen on Unsplash\n","date":"May 30, 2019","hero":"/es/posts/poesia/reloj/hero.png","permalink":"https://www.pablomarcos.me/es/posts/poesia/reloj/","summary":"Son las doce en el viejo reloj de oro. Vestida de gala y ornato, se pavonea entre bailes la alta sociedad. Purpurina, lentejuelas, plumas; Oh la lá!, si levantan las piernas las chicas del cabaret. Risas, champán, amor, qué bien, mon dieu. La noche es joven, y nosotros, también.\nSon las seis en el viejo reloj de oro. Vómitos, resaca y mal olor; los que pueden irse a su casa se van, y los que no, emporrados en el sofá.","tags":null,"title":"Reloj"},{"categories":null,"contents":"Si me tendieses de nuevo una trampa, volvería a caer. Me tiraría de cabeza, como un jabalí herido, con el corazón por delante como un corredor de carreras intoxicado de colacao.\nQuiero volver a ti, a deshojar, a rubato, los pétalos de la flor que no cambia y a ver brillar el color que más cambia cuando te miro a los ojos.\nSé que, si me tendieses una trampa, de nuevo volvería a caer. Pero justo antes de arrojarme al vacío, pensando que tú me darías alas para no despeñarme otra vez, mi cabeza retomaría el control, y mis manos, aún sin quererlo, se agarrarían al borde para salvarme la vida.\nPorque ya estoy harto de tropezar, y va siendo hora de levantarse. Porque quiero volver a flotar, no como un muerto a la deriva en una bolsa de sal, sino como una nube de algodón, irreal, intangible hasta para el sol, vanidosa e incomprendida.\n📸 : Andrey Zvyagintsev on Unsplash\n#music #rubato #estoyhastaelcoño #poesia #poetry #estrechosdemiras\n","date":"May 20, 2019","hero":"/es/posts/poesia/rubato/hero.png","permalink":"https://www.pablomarcos.me/es/posts/poesia/rubato/","summary":"Si me tendieses de nuevo una trampa, volvería a caer. Me tiraría de cabeza, como un jabalí herido, con el corazón por delante como un corredor de carreras intoxicado de colacao.\nQuiero volver a ti, a deshojar, a rubato, los pétalos de la flor que no cambia y a ver brillar el color que más cambia cuando te miro a los ojos.\nSé que, si me tendieses una trampa, de nuevo volvería a caer.","tags":null,"title":"Rubato"},{"categories":null,"contents":"¡Vamos, hermanos, a luchar por la revolución! ¡A las barricadas! ¡A salvar a la nación! ¡Vivan los gritos, las soflamas, el pundonor!; ¡Que todos sepan que dulce et decorum!\nPero mientras unos juegan con las vidas como piezas de ajedrez, otros, ni con pócimas de druidas van a dejar de perder.\nLas palomas entre el fuego no pueden llevar la paz; nosotros lo vemos desde lejos porque nos da igual.\n📸 : Jean-Pierre Houël\n#hermano #revolución #poesia #poetry\nEs mi primer poema que cabe en el límite de 500 caracteres de mastodon 😆 😆 (aunque es republicado de #estrechosdemiras)\n","date":"April 12, 2019","hero":"/es/posts/poesia/hermanos/hero.jpg","permalink":"https://www.pablomarcos.me/es/posts/poesia/hermanos/","summary":"¡Vamos, hermanos, a luchar por la revolución! ¡A las barricadas! ¡A salvar a la nación! ¡Vivan los gritos, las soflamas, el pundonor!; ¡Que todos sepan que dulce et decorum!\nPero mientras unos juegan con las vidas como piezas de ajedrez, otros, ni con pócimas de druidas van a dejar de perder.\nLas palomas entre el fuego no pueden llevar la paz; nosotros lo vemos desde lejos porque nos da igual.","tags":null,"title":"La Revolución"},{"categories":null,"contents":"Se resbalan por el minutero los segundos en el reloj, chorreando como sangre cada instante de mi vida. Me mareo, me desmayo y chorreo yo también, lágrimas de bilis amarga y miedo. No quiero estar sólo, aunque estuviera peor contigo. El silencio está lleno de voces afónicas que me gritan que me calle, que me comporte. Vivo en una prisión donde el viento se niega a transmitir mis palabras.\nCuento los días como una penitencia mientras espero al príncipe azul que me saque de mi torre, y, como no baje yo solito, voy a acabar convertido en espuma de mar.\n📸 : Edvard Munch, El Grito, Wikimedia Commons\n#poesia #poetry #soledad #solitude #loneliness #poesiaenespañol #estrechosdemiras\n","date":"April 5, 2019","hero":"/es/posts/poesia/soledad/hero.jpg","permalink":"https://www.pablomarcos.me/es/posts/poesia/soledad/","summary":"Se resbalan por el minutero los segundos en el reloj, chorreando como sangre cada instante de mi vida. Me mareo, me desmayo y chorreo yo también, lágrimas de bilis amarga y miedo. No quiero estar sólo, aunque estuviera peor contigo. El silencio está lleno de voces afónicas que me gritan que me calle, que me comporte. Vivo en una prisión donde el viento se niega a transmitir mis palabras.\nCuento los días como una penitencia mientras espero al príncipe azul que me saque de mi torre, y, como no baje yo solito, voy a acabar convertido en espuma de mar.","tags":null,"title":"Soledad"},{"categories":null,"contents":"Qué fácil es ver el fuego y decir que arde, pero a mí me hiela las venas ver cómo la gente se consume buscando una luz, una chispa en forma de incendio, un fantasma de locura que devora el mundo.\nEl mismo fuego que iluminaba las cuevas para dibujar bisontes, que robó Prometeo e inundó el corazón de los hombres, nos condena ahora a estar por siempre en guerra, presos de sed de sangre, pecado y lujuria, de ese fuego de las pasiones que no apaga la razón, de este baile de máscaras de felicidad, detrás de las cuales sólo hay marionetas vacías.\n📸 : Sheldon Nunes on Unsplash\n#fuego #fire #poesia #poetry\n","date":"March 19, 2019","hero":"/es/posts/poesia/fuego/hero.png","permalink":"https://www.pablomarcos.me/es/posts/poesia/fuego/","summary":"Qué fácil es ver el fuego y decir que arde, pero a mí me hiela las venas ver cómo la gente se consume buscando una luz, una chispa en forma de incendio, un fantasma de locura que devora el mundo.\nEl mismo fuego que iluminaba las cuevas para dibujar bisontes, que robó Prometeo e inundó el corazón de los hombres, nos condena ahora a estar por siempre en guerra, presos de sed de sangre, pecado y lujuria, de ese fuego de las pasiones que no apaga la razón, de este baile de máscaras de felicidad, detrás de las cuales sólo hay marionetas vacías.","tags":null,"title":"Fuego 🔥 🔥"},{"categories":null,"contents":"Bueno, bueno, bueno\u0026hellip; Pues\u0026hellip; después de bastante agobio por la uni\u0026hellip; I\u0026rsquo;m back! Y el nuevo tema es\u0026hellip; Reloj ⌚ ⌚ ⌚\nMira, estoy hasta los cojones de no verte nunca cuando llego, de que hayas cogido ya el bus o que haya caído ya el ocaso, de llegar siempre tarde, como el conejo de Alicia temeroso de que me hagas perder la cabeza.\nA veces no tengo tiempo ni de pensarte, perdido entre transbordos y trenes infinitos, donde más que buscar colores en el viento me limito a intentar aguantar el hedor en el aire de un John Smith que no se ha duchado en dos semanas.\nYa tarde y cansado me he dado cuenta de que esa tontería del amor sólo existe en los cuentos de hadas; de que me enrollé buscando el sol cuando lo que necesitaba eran unas gafas violetas; de que, ahora que las sombras me inundan, es cuando más claro veo mi reflejo.\n📸 : Diego Duarte Cereceda on Unsplash\n#poesia #poetry #reloj #clock #estrechosdemiras #poesiaenespañol\n","date":"February 25, 2019","hero":"/es/posts/poesia/tarde/hero.png","permalink":"https://www.pablomarcos.me/es/posts/poesia/tarde/","summary":"Bueno, bueno, bueno\u0026hellip; Pues\u0026hellip; después de bastante agobio por la uni\u0026hellip; I\u0026rsquo;m back! Y el nuevo tema es\u0026hellip; Reloj ⌚ ⌚ ⌚\nMira, estoy hasta los cojones de no verte nunca cuando llego, de que hayas cogido ya el bus o que haya caído ya el ocaso, de llegar siempre tarde, como el conejo de Alicia temeroso de que me hagas perder la cabeza.\nA veces no tengo tiempo ni de pensarte, perdido entre transbordos y trenes infinitos, donde más que buscar colores en el viento me limito a intentar aguantar el hedor en el aire de un John Smith que no se ha duchado en dos semanas.","tags":null,"title":"Tarde"},{"categories":null,"contents":"Tengo alergia al reloj, que carcome con sus manecillas de plata cada segundo que me queda.\nEscucho el traqueteo del tren, rodando incapaz de llegar a otra estación, como la bola de cristal de mi bolígrafo, que sólo apunta al pasado.\nHuelo el veneno en el aire, el smog, suave como el susurro de la muerte invadiendo mis pulmones.\nMe miro reflejado en un charco de ácido y alquitrán, su compañía lo único sincero que me queda en este baile de máscaras.\nVeo poetas y puritanos, sodoma y gomorra revisitados, una guerra sin tregua donde sólo se dispara una cosa: palabras\n📸 : Paulo Silva on Unsplash\n#poesia #poetry #ciudad #city\n","date":"February 1, 2019","hero":"/es/posts/poesia/ciudad/hero.jpg","permalink":"https://www.pablomarcos.me/es/posts/poesia/ciudad/","summary":"Tengo alergia al reloj, que carcome con sus manecillas de plata cada segundo que me queda.\nEscucho el traqueteo del tren, rodando incapaz de llegar a otra estación, como la bola de cristal de mi bolígrafo, que sólo apunta al pasado.\nHuelo el veneno en el aire, el smog, suave como el susurro de la muerte invadiendo mis pulmones.\nMe miro reflejado en un charco de ácido y alquitrán, su compañía lo único sincero que me queda en este baile de máscaras.","tags":null,"title":"Ciudad"},{"categories":null,"contents":"Este poema es uno de los que más me gustan. Lo escribí en su momento pensando en un amigo que no se atrevía a dar el paso y salir del armario, pero vale como grito por el orgullo de todxs\nNo sabes lo que me ha costado recoger los pedazos de mis sueños rotos, que se escapaban entre mis manos como la arena de un desierto sin fin. Juntarlos, grano a grano, idea a idea, y refundirlos en este espejo de papel en que ahora me contemplo.\nA volar he aprendido, y a ser libre; a volar, huyendo del dolor de arrancarme la pluma e intentar fingir que no te conocía. A soñar, a viajar más lejos, más alto, mientras tú, presa del miedo, no te podías ni mover. Soy libre porque he roto mis cadenas. Solo tú puedes matar a los demonios de tu pasado. Solo tú puedes enfrentarte a tu mayor miedo: tu propia sombra.\nApaga las luces. Respira. Estás sólo en esta vida, ¿a quién temes? Cruza el puente del arcoíris, envuélvete en 7 sombras de Technicolor. El cielo no se va a romper aunque lo asaltes.\n📸 : Sara Rampazzo on Unsplash\n#poesia #poetry #gay #pasodecebra #crosswalk\n","date":"January 24, 2019","hero":"/es/posts/poesia/paso-de-cebra/hero.png","permalink":"https://www.pablomarcos.me/es/posts/poesia/paso-de-cebra/","summary":"Este poema es uno de los que más me gustan. Lo escribí en su momento pensando en un amigo que no se atrevía a dar el paso y salir del armario, pero vale como grito por el orgullo de todxs\nNo sabes lo que me ha costado recoger los pedazos de mis sueños rotos, que se escapaban entre mis manos como la arena de un desierto sin fin. Juntarlos, grano a grano, idea a idea, y refundirlos en este espejo de papel en que ahora me contemplo.","tags":null,"title":"Atrévete a Cruzar"},{"categories":null,"contents":"El efecto pigmalión, ¿sabes que si crees en ti mismo todo lo puedes? ¿Y que si no, eres un ser despreciable, un amargado innombrable que no merece compasión?\n¿No ves que en África están peor y se matan? ¿Qué más te da compartir tu cama con ratas? Si la vida te da limones, ¡haz limonada! Pero, ¿y si en vez de limones te tira granadas? Granadas bomba, y tus sueños rotos, como ruinas antiguas de tiempos ignotos.\nPero ahora vives el sueño americano, ¿por qué ibas a despertar? Mejor sigue callado, dormido, olvidado; poniendo capa sobre capa sobre capa de maquillaje para que no se te borre nunca esa sonrisa de Joker.\n📸 : Marc Schäfer on Unsplash\n#poesia #poetry #poesiaenespañol #efectopigmalion #pygmalioneffect #rosenthaleffect\n","date":"November 30, 2018","hero":"/es/posts/poesia/efecto-pigmalion/hero.jpg","permalink":"https://www.pablomarcos.me/es/posts/poesia/efecto-pigmalion/","summary":"El efecto pigmalión, ¿sabes que si crees en ti mismo todo lo puedes? ¿Y que si no, eres un ser despreciable, un amargado innombrable que no merece compasión?\n¿No ves que en África están peor y se matan? ¿Qué más te da compartir tu cama con ratas? Si la vida te da limones, ¡haz limonada! Pero, ¿y si en vez de limones te tira granadas? Granadas bomba, y tus sueños rotos, como ruinas antiguas de tiempos ignotos.","tags":null,"title":"Efecto Pigmalión"},{"categories":null,"contents":"Es como la espina de una rosa fría y vanidosa, que se clava en tus entrañas; como una serpiente huidiza que te acecha siempre tras la cortina.\nEs un río de sangre, dudas y adrenalina; un escalofrío que te espera en Samarra; una mano negra que se posa sobre ti, suave como un traje de noche entretejido de tus sueños.\nEs como una piel de cordero que esconde a un dementor con corbata, que te domina con unas esposas de tiempo que destruyen todo intento de huida. Es el arma con que te mantienen sumiso, mientras poco a poco se van apagando las luces.\n📸 : Hasan Almasi on Unsplash\n#poesia #poetry #poesiaenespañol #miedo #fear\n","date":"November 12, 2018","hero":"/es/posts/poesia/miedo/hero.jpg","permalink":"https://www.pablomarcos.me/es/posts/poesia/miedo/","summary":"Es como la espina de una rosa fría y vanidosa, que se clava en tus entrañas; como una serpiente huidiza que te acecha siempre tras la cortina.\nEs un río de sangre, dudas y adrenalina; un escalofrío que te espera en Samarra; una mano negra que se posa sobre ti, suave como un traje de noche entretejido de tus sueños.\nEs como una piel de cordero que esconde a un dementor con corbata, que te domina con unas esposas de tiempo que destruyen todo intento de huida.","tags":null,"title":"Miedo"},{"categories":null,"contents":"‘Philanthropy’, they say, they who killed my family without an afterthought; ‘philanthropy’, those who destroy the forests and crush dreams. ‘Philanthropy’, and a grin, while they mercilessly unleash the rage of the gods.\nSome open the Gates, others offer you the Golden Apple of Iðunn; both wash their hands in the water they’ve stolen. But their hands are stained with blood, and the more they wash, the more they poison, the ghost of christmas future hanging over their heads. That’s why they give us back the leftovers of their glory – because they know how they gained it, and they’d hate to live this world with such a debt.\n📸 : Annie Spratt on Unsplash\n#poesia #poetry #philanthropy #filantropia\n","date":"October 30, 2018","hero":"/es/posts/poesia/philanthropy/hero.png","permalink":"https://www.pablomarcos.me/es/posts/poesia/philanthropy/","summary":"‘Philanthropy’, they say, they who killed my family without an afterthought; ‘philanthropy’, those who destroy the forests and crush dreams. ‘Philanthropy’, and a grin, while they mercilessly unleash the rage of the gods.\nSome open the Gates, others offer you the Golden Apple of Iðunn; both wash their hands in the water they’ve stolen. But their hands are stained with blood, and the more they wash, the more they poison, the ghost of christmas future hanging over their heads.","tags":null,"title":"Philantropy, they say"},{"categories":null,"contents":"Otra vez Venus me niega su luz mientras se cierne sobre mí la noche. Me acuerdo de ti, que encendiste los fuegos de Marte en mi corazón, mientras busco la forma de no perderme en mi propio mundo.\nMe ha costado romper los anillos de Saturno, he castrado a Júpiter y casi me ahogo en la soledad de Neptuno por pararme un segundo a contemplar el firmamento envenenado de Urano. Y ahora estoy en Plutón, a años luz de ti, sólo, frío, olvidado. Puede que arda en las ascuas del recuerdo, pero espero que me pille de fiesta en el infierno.\n📸 : Tom Harnasz en Unsplash\n#poesia #poetry #poesiaenespañol #plutón #pluto\n","date":"October 6, 2018","hero":"/es/posts/poesia/pluton/hero.jpg","permalink":"https://www.pablomarcos.me/es/posts/poesia/pluton/","summary":"Otra vez Venus me niega su luz mientras se cierne sobre mí la noche. Me acuerdo de ti, que encendiste los fuegos de Marte en mi corazón, mientras busco la forma de no perderme en mi propio mundo.\nMe ha costado romper los anillos de Saturno, he castrado a Júpiter y casi me ahogo en la soledad de Neptuno por pararme un segundo a contemplar el firmamento envenenado de Urano. Y ahora estoy en Plutón, a años luz de ti, sólo, frío, olvidado.","tags":null,"title":"Plutón"},{"categories":null,"contents":"Coincidencia es que siempre sea el príncipe (alto, fuerte y guapo) quien se lleve a la princesa; y que yo, ni lo uno ni lo otro, me enamore de un dragón que me encerró en su castillo\nCoincidencia era la música que martilleaba mi cabeza cuando veía tus fotos en un carrete ahora oxidado; coincidencia es que, ahora que el fuego en que me consumías se apaga, según se desvanece su danza y se esfuman tus juegos fatuos, por fin puedo ver las estrellas. Que me siento libre, por primera vez en mucho tiempo y que, preso del síndrome de Estocolmo, no sé qué hacer.\n📸 : Simon Migaj on Unsplash\n#poesia #poetry #poesiaenespañol #coincidencia #coincidence\n","date":"October 5, 2018","hero":"/es/posts/poesia/coincidencia/hero.png","permalink":"https://www.pablomarcos.me/es/posts/poesia/coincidencia/","summary":"Coincidencia es que siempre sea el príncipe (alto, fuerte y guapo) quien se lleve a la princesa; y que yo, ni lo uno ni lo otro, me enamore de un dragón que me encerró en su castillo\nCoincidencia era la música que martilleaba mi cabeza cuando veía tus fotos en un carrete ahora oxidado; coincidencia es que, ahora que el fuego en que me consumías se apaga, según se desvanece su danza y se esfuman tus juegos fatuos, por fin puedo ver las estrellas.","tags":null,"title":"Coincidencia"},{"categories":null,"contents":"Cuando el viento siente la cabeza cerraré yo mi boca. Cuando no quede ya luz lucharé por encender una chispa. Como un tren desbocado destinado a caerse por un precipicio; como un loco, un gallito, un idiota, un simple monito.\nPegajoso como el azúcar y amargo como el cianuro, tinta azul que corre por mis venas; arcoíris a veces, tan gris siempre. Fugaz como las estrellas, constante si me lo pides; cansado, despierto, hiperactivo, vago. Como un cactus que arrancó sus espinas, como una rosa que perdió a su Principito; así soy yo.\n#poesia #poetry #poesiaenespañol #introductions\n","date":"September 28, 2018","hero":"/es/posts/poesia/introduccion/hero.jpg","permalink":"https://www.pablomarcos.me/es/posts/poesia/introduccion/","summary":"Cuando el viento siente la cabeza cerraré yo mi boca. Cuando no quede ya luz lucharé por encender una chispa. Como un tren desbocado destinado a caerse por un precipicio; como un loco, un gallito, un idiota, un simple monito.\nPegajoso como el azúcar y amargo como el cianuro, tinta azul que corre por mis venas; arcoíris a veces, tan gris siempre. Fugaz como las estrellas, constante si me lo pides; cansado, despierto, hiperactivo, vago.","tags":null,"title":"Introducción"},{"categories":null,"contents":"Este texto es un clásico y uno de mis favoritos de los que he escrito nunca, tanto que me decidí cuando lo publiqué en Estrechos de Miras por publicarlo como CC-By-NC-ND 4.0. Aunque se les olvidó ponerlo y pusieron By-Sa, mantengo mi deseo de hacerlo NC-ND, aunque me cueste decidirme al ser esta una licencia no libre.\nHe puesto como fecha noviembre de 2016, pues es entorno a entonces cuando lo concebí, en un taller de escritura en casa de un profesor y buen amigo.\nLa música, según Darwin, existió en el género humano antes que la capacidad del habla; es por eso que nos afecta tan profundamente, que clava su daga tan hondo en nuestro corazón.\nLa música es el dulce ronroneo de tu voz, el sentimiento que le sobra al corazón y se me escapa por las manos; es un llanto desesperado, un canto a un beso que nunca llega, un sueño de una noche de verano.\nY ahora, solo, triste, olvidado y eternamente muerto, la música se ha parado, y el silencio ha llegado a mi alma. Y me despierto a las cinco de la mañana, febril, enfermo de ti, gritando ¡TENGO HAMBRE! Hambre de música, de color en una vida en blanco y negro, salpicada por los borrones de tinta que deja el bolígrafo al escribir, los borrones que me convierten en un esclavo de la pena y del llanto, y que parecen las notas de una partitura, del requiem en que has convertido mi vida.\n¿De qué nos sirve seguir así, manteniendo a base de resoplidos de un fuelle ya gastado la sintonía de un órgano decimonónico, cuando los órganos hace tiempo que dejaron de estar de moda y nuestra relación se cae a pedazos? ¿Por qué mantenemos una vida artificial, haciéndonos el boca a boca, fingiendo ser felices mientras nos asamos lentamente en el fuego del olvido?\nQuememos mejor los violines y las violas, destrocemos a hachazos los violonchelos, silenciemos los tambores, las flautas de Hamelín con las que me embaucaste y que en realidad eran made in China; fundamos las trompetas y las tubas, acabemos de una vez con esta orquesta sin sentido; toda buena melodía necesita un silencio de vez en cuando si no quiere convertirse en ruido.\n📸 : Fundación Juan March\n#music #sinfonia #symphony #música #poesia #poetry #estrechosdemiras\n","date":"November 11, 2016","hero":"/es/posts/poesia/orquesta/hero.png","permalink":"https://www.pablomarcos.me/es/posts/poesia/orquesta/","summary":"Este texto es un clásico y uno de mis favoritos de los que he escrito nunca, tanto que me decidí cuando lo publiqué en Estrechos de Miras por publicarlo como CC-By-NC-ND 4.0. Aunque se les olvidó ponerlo y pusieron By-Sa, mantengo mi deseo de hacerlo NC-ND, aunque me cueste decidirme al ser esta una licencia no libre.\nHe puesto como fecha noviembre de 2016, pues es entorno a entonces cuando lo concebí, en un taller de escritura en casa de un profesor y buen amigo.","tags":null,"title":"Orquesta 🎻 🎺"},{"categories":null,"contents":"Este texto, o al menos uno muy parecido, lo escribí cerca de noviembre de 2015, por eso he puesto esta fecha; sin embargo, la foto es muy posterior, de Villalba en 2020. Curiosamente, lo redacté para prepararme el First Certificate in English, aunque a mi profe no le gustó mucho que fuera tan dramatic 😅😅. Pero no pasa nada, ya estoy mejor :D\nLara saw something unusual in the grass and went back to take a closer look. No, it was nothing. Her sight had treasoned her another time. It was all like that since it all began. First laughter, then tears. First happiness, then a world of fear. She didnt know how everything had happened, but it had.\nIt all started two years ago, with a boy more beautiful than the world had ever known. With two eyes like stars and a hair like gold, not because of its colour but because of the way it shone. She fell in love, but never knew the reason why; he never loved her, but who cares? She was in the sea, and love was an island there.\nAnd after hundreds and hundreds of stares at the stars; after suffering of love, after suffering so bad, she used a silver knife and got out of the dark.\n📸 : Photo by\u0026hellip; Hum! Actually, photo by me ! Title: La casa de Lucifer, CC-By-Sa 4.0 Pablo Marcos\n#éraseunavez #story #poesiaenespañol #poesia #poetry #estrechosdemiras\n","date":"October 15, 2015","hero":"/es/posts/poesia/erase/hero.jpg","permalink":"https://www.pablomarcos.me/es/posts/poesia/erase/","summary":"Este texto, o al menos uno muy parecido, lo escribí cerca de noviembre de 2015, por eso he puesto esta fecha; sin embargo, la foto es muy posterior, de Villalba en 2020. Curiosamente, lo redacté para prepararme el First Certificate in English, aunque a mi profe no le gustó mucho que fuera tan dramatic 😅😅. Pero no pasa nada, ya estoy mejor :D\nLara saw something unusual in the grass and went back to take a closer look.","tags":null,"title":"Escape"}]