<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Big Data Engineering on Pablo Marcos' Blog</title><link>https://www.pablomarcos.me/es/posts/master-en-biolog%C3%ADa-computacional/big-data/</link><description>Recent content in Big Data Engineering on Pablo Marcos' Blog</description><generator>Hugo -- gohugo.io</generator><language>en-us</language><lastBuildDate>Fri, 11 Feb 2022 00:00:00 +0000</lastBuildDate><atom:link href="https://www.pablomarcos.me/es/posts/master-en-biolog%C3%ADa-computacional/big-data/index.xml" rel="self" type="application/rss+xml"/><item><title>Annex I - Code</title><link>https://www.pablomarcos.me/es/posts/master-en-biolog%C3%ADa-computacional/big-data/final-project/annex-i-code/</link><pubDate>Fri, 11 Feb 2022 00:00:00 +0000</pubDate><guid>https://www.pablomarcos.me/es/posts/master-en-biolog%C3%ADa-computacional/big-data/final-project/annex-i-code/</guid><description>This jupyter notebook shall serve as accompanying material to this repositories&amp;rsquo; README, a report for the “Big Data Engineering” subject at UPM’s Master in Computational Biology. It is thus only intended as a recopilation of used code; for the full discussion, please refer to the README.
Dependencies Installation # First, we install jdk8 !apt-get install openjdk-8-jdk-headless -qq &amp;gt; /dev/null import os # And set the environment variable &amp;#39;JAVA_HOME&amp;#39;. os.environ[&amp;#34;JAVA_HOME&amp;#34;] = &amp;#34;/usr/lib/jvm/java-8-openjdk-amd64&amp;#34; !</description></item><item><title>Pneumonia identification from X-Ray Imagery</title><link>https://www.pablomarcos.me/es/posts/master-en-biolog%C3%ADa-computacional/big-data/final-project/paper/</link><pubDate>Fri, 11 Feb 2022 00:00:00 +0000</pubDate><guid>https://www.pablomarcos.me/es/posts/master-en-biolog%C3%ADa-computacional/big-data/final-project/paper/</guid><description>by Yaiza ARNAIZ ALCACER and Pablo Ignacio MARCOS LOPEZ
Abstract Pneumonia is a serious illness characterised by a severe cough with phlegm, fever, chills and shortness of breath, which is caused by inflammation of the alveoli in one or both lungs. Despite advances in the diagnosis and treatment of lung infections, pneumonia is the sixth leading cause of death in adults in the United States, with more than six million cases of acute pneumonia each year, one million of which require hospitalisation.</description></item><item><title>Predicting Churn Risk</title><link>https://www.pablomarcos.me/es/posts/master-en-biolog%C3%ADa-computacional/big-data/predicting-churn-risk/</link><pubDate>Mon, 01 Nov 2021 00:00:00 +0000</pubDate><guid>https://www.pablomarcos.me/es/posts/master-en-biolog%C3%ADa-computacional/big-data/predicting-churn-risk/</guid><description>Your goal is to create a model that can predict whether a customer will churn (0 or 1) based on the features in this dataset. See the slides for more information.
Initial Steps First, we need to create the Spark Session
!apt-get install openjdk-8-jdk-headless -qq &amp;gt; /dev/null !wget -q https://mirrors.sonic.net/apache/spark/spark-3.1.2/spark-3.1.2-bin-hadoop3.2.tgz !tar xzf spark-3.1.2-bin-hadoop3.2.tgz !pip install -q findspark import os os.environ[&amp;#34;JAVA_HOME&amp;#34;] = &amp;#34;/usr/lib/jvm/java-8-openjdk-amd64&amp;#34; os.environ[&amp;#34;SPARK_HOME&amp;#34;] = &amp;#34;/content/spark-3.1.2-bin-hadoop3.2&amp;#34; import findspark findspark.init() from pyspark.sql import SparkSession spark = SparkSession.</description></item><item><title>Predicting Crew Members</title><link>https://www.pablomarcos.me/es/posts/master-en-biolog%C3%ADa-computacional/big-data/predicting-crew-members/</link><pubDate>Tue, 26 Oct 2021 00:00:00 +0000</pubDate><guid>https://www.pablomarcos.me/es/posts/master-en-biolog%C3%ADa-computacional/big-data/predicting-crew-members/</guid><description>Your job is to create a regression model that will help predict how many crew members will be needed for future ships. In other words, use the features you think will be useful to predict the value in the Crew column.
Create the Spark Session !apt-get install openjdk-8-jdk-headless -qq &amp;gt; /dev/null !wget -q https://mirrors.sonic.net/apache/spark/spark-3.1.2/spark-3.1.2-bin-hadoop3.2.tgz !tar xzf spark-3.1.2-bin-hadoop3.2.tgz !pip install -q findspark import os os.environ[&amp;#34;JAVA_HOME&amp;#34;] = &amp;#34;/usr/lib/jvm/java-8-openjdk-amd64&amp;#34; os.environ[&amp;#34;SPARK_HOME&amp;#34;] = &amp;#34;/content/spark-3.1.2-bin-hadoop3.2&amp;#34; import findspark findspark.</description></item><item><title>Extra Point: Auto Insurance</title><link>https://www.pablomarcos.me/es/posts/master-en-biolog%C3%ADa-computacional/big-data/dog-food-day/</link><pubDate>Thu, 14 Oct 2021 00:00:00 +0000</pubDate><guid>https://www.pablomarcos.me/es/posts/master-en-biolog%C3%ADa-computacional/big-data/dog-food-day/</guid><description>Nothing ruins the thrill of buying a brand new car more quickly than seeing your new insurance bill. The sting’s even more painful when you know you’re a good driver. It doesn’t seem fair that you have to pay so much if you’ve been cautious on the road for years.
Inaccuracies in car insurance company’s claim predictions raise the cost of insurance for good drivers and reduce the price for bad ones.</description></item><item><title>Hack Data</title><link>https://www.pablomarcos.me/es/posts/master-en-biolog%C3%ADa-computacional/big-data/hack-data/</link><pubDate>Thu, 14 Oct 2021 00:00:00 +0000</pubDate><guid>https://www.pablomarcos.me/es/posts/master-en-biolog%C3%ADa-computacional/big-data/hack-data/</guid><description>A technology start-up in California has recently been hacked, and their forensic engineers have grabbed valuable information, including information like session time,locations, wpm typing speed, etc, to identify the hackers. Your goal is to use SparkML to do this.
Initial Steps First, we need to create the Spark Session
#In collab, we need to install everything: !apt-get install openjdk-8-jdk-headless -qq &amp;gt; /dev/null !wget -q https://mirrors.sonic.net/apache/spark/spark-3.1.2/spark-3.1.2-bin-hadoop3.2.tgz !tar xzf spark-3.1.2-bin-hadoop3.2.tgz !pip install -q findspark import os os.</description></item><item><title>Predicting Dog Food Spoiling</title><link>https://www.pablomarcos.me/es/posts/master-en-biolog%C3%ADa-computacional/big-data/predicting-dog-food-spoiling/</link><pubDate>Thu, 14 Oct 2021 00:00:00 +0000</pubDate><guid>https://www.pablomarcos.me/es/posts/master-en-biolog%C3%ADa-computacional/big-data/predicting-dog-food-spoiling/</guid><description>We have been contracted by a dog food company that uses an additive with 4 different chemicals (A, B, C and D) and a filler to preserve their food. The scientists have detected a problem: some batches of their dog food spoil much faster than expected. Since they haven&amp;rsquo;t updated their machinery, the levels of preservatives can vary a lot, so your job as a consultant is to use Machine Learning to detect which chemical is most responsible for the spoilage.</description></item><item><title>Walmart Stock Exercise</title><link>https://www.pablomarcos.me/es/posts/master-en-biolog%C3%ADa-computacional/big-data/walmart-stock-exercise/</link><pubDate>Thu, 14 Oct 2021 00:00:00 +0000</pubDate><guid>https://www.pablomarcos.me/es/posts/master-en-biolog%C3%ADa-computacional/big-data/walmart-stock-exercise/</guid><description>Create a Jupyter notebook to execute the following tasks, as part of the Big Data engineerig course:
Start a simple Spark session !apt-get install openjdk-8-jdk-headless -qq &amp;gt; /dev/null !wget -q https://mirrors.sonic.net/apache/spark/spark-3.1.2/spark-3.1.2-bin-hadoop3.2.tgz !tar xzf spark-3.1.2-bin-hadoop3.2.tgz !pip install -q findspark import os os.environ[&amp;#34;JAVA_HOME&amp;#34;] = &amp;#34;/usr/lib/jvm/java-8-openjdk-amd64&amp;#34; os.environ[&amp;#34;SPARK_HOME&amp;#34;] = &amp;#34;/content/spark-3.1.2-bin-hadoop3.2&amp;#34; import findspark findspark.init() from pyspark.sql import SparkSession spark = SparkSession.builder.master(&amp;#34;local[*]&amp;#34;).getOrCreate() import pandas as pd Load the Walmart Stock CSV file, let Spark infer the data types df = spark.</description></item></channel></rss>