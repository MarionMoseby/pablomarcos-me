{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Consulting Project: Hack Data.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9UzwUQzLjkJo"
      },
      "source": [
        "A technology start-up in California needs your help! Theyâ€™ve been recently hacked and need your help finding out about the hackers!\n",
        "\n",
        "Luckily their forensic engineers have grabbed valuable data about the hacks, including information like session time,locations, wpm typing speed, etc. Use your machine-learning skills predict how many hackers (two or three) took part in the attacks, and bust them!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H9O49lUEhFpp"
      },
      "source": [
        "First, we need to create the Spark Session"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B950rmcJhIHx"
      },
      "source": [
        "#In collab, we need to install everything:\n",
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
        "!wget -q https://mirrors.sonic.net/apache/spark/spark-3.1.2/spark-3.1.2-bin-hadoop3.2.tgz\n",
        "!tar xzf spark-3.1.2-bin-hadoop3.2.tgz\n",
        "!pip install -q findspark\n",
        "\n",
        "\n",
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-3.1.2-bin-hadoop3.2\"\n",
        "\n",
        "\n",
        "import findspark\n",
        "findspark.init()\n",
        "from pyspark.sql import SparkSession\n",
        "spark = SparkSession.builder.master(\"local[*]\").getOrCreate()\n",
        "\n",
        "#In a native Jupyter notebook, we would simply do:\n",
        "#from pyspark.sql import SparkSession\n",
        "#spark = SparkSession.builder.appName('seedfinder').getOrCreate()"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B0p6xJ0WkGCe"
      },
      "source": [
        "Afterwards, we can read the file and inspect it"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "o61gqNjOkFoF",
        "outputId": "075ae1dd-448b-4e62-8610-2750966e39cb"
      },
      "source": [
        "#Please drop the file in the environments 'Files' panel\n",
        "df = spark.read.options(header=\"true\", inferSchema=\"true\").csv(\"/content/hack_data.csv\")\n",
        "df.describe().toPandas()"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>summary</th>\n",
              "      <th>Session_Connection_Time</th>\n",
              "      <th>Bytes Transferred</th>\n",
              "      <th>Kali_Trace_Used</th>\n",
              "      <th>Servers_Corrupted</th>\n",
              "      <th>Pages_Corrupted</th>\n",
              "      <th>Location</th>\n",
              "      <th>WPM_Typing_Speed</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>count</td>\n",
              "      <td>334</td>\n",
              "      <td>334</td>\n",
              "      <td>334</td>\n",
              "      <td>334</td>\n",
              "      <td>334</td>\n",
              "      <td>334</td>\n",
              "      <td>334</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>mean</td>\n",
              "      <td>30.008982035928145</td>\n",
              "      <td>607.2452694610777</td>\n",
              "      <td>0.5119760479041916</td>\n",
              "      <td>5.258502994011977</td>\n",
              "      <td>10.838323353293413</td>\n",
              "      <td>None</td>\n",
              "      <td>57.342395209580864</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>stddev</td>\n",
              "      <td>14.088200614636158</td>\n",
              "      <td>286.33593163576757</td>\n",
              "      <td>0.5006065264451406</td>\n",
              "      <td>2.30190693339697</td>\n",
              "      <td>3.06352633036022</td>\n",
              "      <td>None</td>\n",
              "      <td>13.41106336843464</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>min</td>\n",
              "      <td>1.0</td>\n",
              "      <td>10.0</td>\n",
              "      <td>0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>6.0</td>\n",
              "      <td>Afghanistan</td>\n",
              "      <td>40.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>max</td>\n",
              "      <td>60.0</td>\n",
              "      <td>1330.5</td>\n",
              "      <td>1</td>\n",
              "      <td>10.0</td>\n",
              "      <td>15.0</td>\n",
              "      <td>Zimbabwe</td>\n",
              "      <td>75.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "  summary Session_Connection_Time  ...     Location    WPM_Typing_Speed\n",
              "0   count                     334  ...          334                 334\n",
              "1    mean      30.008982035928145  ...         None  57.342395209580864\n",
              "2  stddev      14.088200614636158  ...         None   13.41106336843464\n",
              "3     min                     1.0  ...  Afghanistan                40.0\n",
              "4     max                    60.0  ...     Zimbabwe                75.0\n",
              "\n",
              "[5 rows x 8 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1W9GQT4ykcu6"
      },
      "source": [
        "The idea for this assignment is to use clustering methods to see if we can find which attacks belong to which hacker: essentially, we want to create n number of groups of attacks, where n is the number of involved hackers. We are also told that hackers like to equally divide work; so, for example, if we have (as we do) 335 attacks and 3 hackers, each would do 110 attacks; if, however, we only had 2 hackers, each would only do 165 attacks, and so on. I will use K-means clustering, which is not surprising, given its the only one we have been told how to use ðŸ˜‹."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lyoth-F2mOK6"
      },
      "source": [
        "We were told the \"Location\" feature is not really important due to VPN use, but I think including it might be interesting nonetheless, if not for the final result, just to learn a bit! So, here I use the StringIndexer to convert it to string format:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "7P6962Hwl8rd",
        "outputId": "86f86122-81ea-4e9e-9c81-73866a2c013c"
      },
      "source": [
        "from pyspark.ml import Pipeline\n",
        "from pyspark.ml.feature import StringIndexer\n",
        "\n",
        "#Bonus! Change this code to index multiple columns at once!\n",
        "indexers = [StringIndexer(inputCol=column, outputCol=column+\"_index\").fit(df) for column in list([\"Location\"]) ]\n",
        "\n",
        "\n",
        "pipeline = Pipeline(stages=indexers)\n",
        "df_indexed = pipeline.fit(df).transform(df)\n",
        "df_indexed.describe().toPandas()"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>summary</th>\n",
              "      <th>Session_Connection_Time</th>\n",
              "      <th>Bytes Transferred</th>\n",
              "      <th>Kali_Trace_Used</th>\n",
              "      <th>Servers_Corrupted</th>\n",
              "      <th>Pages_Corrupted</th>\n",
              "      <th>Location</th>\n",
              "      <th>WPM_Typing_Speed</th>\n",
              "      <th>Location_index</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>count</td>\n",
              "      <td>334</td>\n",
              "      <td>334</td>\n",
              "      <td>334</td>\n",
              "      <td>334</td>\n",
              "      <td>334</td>\n",
              "      <td>334</td>\n",
              "      <td>334</td>\n",
              "      <td>334</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>mean</td>\n",
              "      <td>30.008982035928145</td>\n",
              "      <td>607.2452694610777</td>\n",
              "      <td>0.5119760479041916</td>\n",
              "      <td>5.258502994011977</td>\n",
              "      <td>10.838323353293413</td>\n",
              "      <td>None</td>\n",
              "      <td>57.342395209580864</td>\n",
              "      <td>64.99700598802396</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>stddev</td>\n",
              "      <td>14.088200614636158</td>\n",
              "      <td>286.33593163576757</td>\n",
              "      <td>0.5006065264451406</td>\n",
              "      <td>2.30190693339697</td>\n",
              "      <td>3.06352633036022</td>\n",
              "      <td>None</td>\n",
              "      <td>13.41106336843464</td>\n",
              "      <td>50.98975334284259</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>min</td>\n",
              "      <td>1.0</td>\n",
              "      <td>10.0</td>\n",
              "      <td>0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>6.0</td>\n",
              "      <td>Afghanistan</td>\n",
              "      <td>40.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>max</td>\n",
              "      <td>60.0</td>\n",
              "      <td>1330.5</td>\n",
              "      <td>1</td>\n",
              "      <td>10.0</td>\n",
              "      <td>15.0</td>\n",
              "      <td>Zimbabwe</td>\n",
              "      <td>75.0</td>\n",
              "      <td>180.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "  summary Session_Connection_Time  ...    WPM_Typing_Speed     Location_index\n",
              "0   count                     334  ...                 334                334\n",
              "1    mean      30.008982035928145  ...  57.342395209580864  64.99700598802396\n",
              "2  stddev      14.088200614636158  ...   13.41106336843464  50.98975334284259\n",
              "3     min                     1.0  ...                40.0                0.0\n",
              "4     max                    60.0  ...                75.0              180.0\n",
              "\n",
              "[5 rows x 9 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "woF9b03emw5n"
      },
      "source": [
        "Now, we can use the VectorAssembler to define our \"features\" column:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XCy5I4oKl8j8"
      },
      "source": [
        "from pyspark.ml.feature import VectorAssembler\n",
        "#By using a list comprehension we can define inputcols as the exclusion of some columns from df_indexed\n",
        "assembler = VectorAssembler(inputCols= [e for e in df_indexed.columns if e not in ('Location')]  , outputCol='features', \n",
        "                            handleInvalid='skip')\n",
        "output = assembler.transform(df_indexed)"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cr_YZblynRwe"
      },
      "source": [
        "Another interesting thing to do is \"standarization\". In essence, this adjusts all values to follow a \"common scale\", so that they are easier to compare and process. Thus:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AGmkthGrl8dK",
        "outputId": "b2cf2191-2b42-48aa-c597-452dffdf3c98"
      },
      "source": [
        "from pyspark.ml.feature import StandardScaler\n",
        "scaler = StandardScaler(inputCol='features', outputCol='scaled_features')\n",
        "scalar_model = scaler.fit(output)\n",
        "scaled_data = scalar_model.transform(output)\n",
        "scaled_data.select('scaled_features').head()"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Row(scaled_features=DenseVector([0.5679, 1.3658, 1.9976, 1.2859, 2.2849, 5.3963, 1.7258]))"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "amqgck_loQ67"
      },
      "source": [
        "Now starts the difficult, more think-about-it part! We already know that we might have 2 OR 3 [harkers](https://www.youtube.com/watch?v=H3edGTP7GVY), so, we are going to try to do it first with 3, then with 2, and compare which gets the best clustering score!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lVJXMK3mqncU"
      },
      "source": [
        "from pyspark.ml.clustering import KMeans #Import the module"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uu0rXRlLrfMF"
      },
      "source": [
        "First, we define and apply the models:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VHwO8N6Gl8R7"
      },
      "source": [
        "kmeans3 = KMeans(featuresCol='scaled_features', k=3)\n",
        "model3 = kmeans3.fit(scaled_data)\n",
        "kmeans2 = KMeans(featuresCol='scaled_features', k=2)\n",
        "model2 = kmeans2.fit(scaled_data)"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4TIjEbu8rjlf"
      },
      "source": [
        "And then, we get the results:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BQGub8jqqzMn"
      },
      "source": [
        "results3 = model3.transform(scaled_data)\n",
        "results2 = model2.transform(scaled_data)"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ERtcNdFgroxV"
      },
      "source": [
        "We could also visualize the results; this has been abbreviated for efficiency"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q55DRgXyq3Sf"
      },
      "source": [
        "#results3.select('prediction').show()\n",
        "#results2.select('prediction').show()"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L20G6bOerqFh"
      },
      "source": [
        "Thats it! We have the results! Now, lets see how good the classification is: if all the attacks classify neatly in two groups, then, the two-hacker-theory would be validated; else, if we need a group more to explain all the attacks better, the three-hacker-thesis would be king! Lets see how this works:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vpw7WhzMsAy8"
      },
      "source": [
        "from pyspark.ml.evaluation import ClusteringEvaluator"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QwPHioTKsG2v"
      },
      "source": [
        "Lets generate the evaluations:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HUznEbyzsF9A",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a3070f1c-8c7c-47cf-8966-9ebc12825aec"
      },
      "source": [
        "ClusteringEvaluator().evaluate(results2)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.6555369436993117"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7yH6OXbRsK1q",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e5597504-4b84-453c-e3e5-1de2d8c07bed"
      },
      "source": [
        "ClusteringEvaluator().evaluate(results3)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.3008773897853434"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OWDJ9wLdsoU6"
      },
      "source": [
        "As we can see, Â¡the two-hacker-theorem gets way, way better evaluation scores! This means that the underlying patterns in the data fit two-group classification way, way better than three-group classification! (ClusteringEvaluator uses the [silhouette method](https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.ml.evaluation.ClusteringEvaluator.html), for which closeness to 1 signals closeness between the clusters and the clustering center. This centers can be shown using model.clusterCenters() )\n",
        "\n",
        "To sum up: there are, definetely, **two and only two hackers here**\n",
        "\n",
        "And interesting to-do would be to show the characteristic's % on each cluster, to see if we can unmask the criminals."
      ]
    }
  ]
}